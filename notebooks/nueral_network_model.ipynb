{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:11:40.567641Z",
     "start_time": "2025-05-22T17:11:17.133080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score, balanced_accuracy_score\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE"
   ],
   "id": "715ca0a2f781ed09",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:24:01.136219Z",
     "start_time": "2025-05-22T17:24:01.097029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_draft_data = pd.read_csv('../data/2025/all_draft_data.csv')\n",
    "all_draft_data.head()"
   ],
   "id": "7719b80c3dabc02d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   pick_no  round  scoring_type  qb_need  rb_need  wr_need  te_need  k_need  \\\n",
       "0        1      1           0.0        1        2        2        1       1   \n",
       "1        2      1           0.0        1        2        2        1       1   \n",
       "2        3      1           0.0        1        2        2        1       1   \n",
       "3        4      1           0.0        1        2        2        1       1   \n",
       "4        5      1           0.0        1        2        2        1       1   \n",
       "\n",
       "   dst_need  flex_need  ...  dst_available  flex_available  qb_vor  rb_vor  \\\n",
       "0         1          2  ...             32             151    69.0   164.0   \n",
       "1         1          2  ...             32             150    69.0   164.0   \n",
       "2         1          2  ...             32             149    69.0   164.0   \n",
       "3         1          2  ...             32             148    69.0   154.0   \n",
       "4         1          2  ...             32             147    69.0   154.0   \n",
       "\n",
       "   wr_vor  te_vor  k_vor  flex_vor  position_drafted             draft_id  \n",
       "0   125.0    68.0   17.0     164.0                WR  1224007842931937280  \n",
       "1   116.0    68.0   17.0     164.0                WR  1224007842931937280  \n",
       "2   116.0    68.0   17.0     164.0                RB  1224007842931937280  \n",
       "3   116.0    68.0   17.0     154.0                RB  1224007842931937280  \n",
       "4   116.0    68.0   17.0     154.0                WR  1224007842931937280  \n",
       "\n",
       "[5 rows x 32 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pick_no</th>\n",
       "      <th>round</th>\n",
       "      <th>scoring_type</th>\n",
       "      <th>qb_need</th>\n",
       "      <th>rb_need</th>\n",
       "      <th>wr_need</th>\n",
       "      <th>te_need</th>\n",
       "      <th>k_need</th>\n",
       "      <th>dst_need</th>\n",
       "      <th>flex_need</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_available</th>\n",
       "      <th>flex_available</th>\n",
       "      <th>qb_vor</th>\n",
       "      <th>rb_vor</th>\n",
       "      <th>wr_vor</th>\n",
       "      <th>te_vor</th>\n",
       "      <th>k_vor</th>\n",
       "      <th>flex_vor</th>\n",
       "      <th>position_drafted</th>\n",
       "      <th>draft_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>151</td>\n",
       "      <td>69.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>WR</td>\n",
       "      <td>1224007842931937280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>150</td>\n",
       "      <td>69.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>WR</td>\n",
       "      <td>1224007842931937280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>149</td>\n",
       "      <td>69.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>RB</td>\n",
       "      <td>1224007842931937280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>148</td>\n",
       "      <td>69.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>RB</td>\n",
       "      <td>1224007842931937280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>147</td>\n",
       "      <td>69.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>WR</td>\n",
       "      <td>1224007842931937280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:32:34.227571Z",
     "start_time": "2025-05-22T17:32:34.195936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "draft_data = all_draft_data[~all_draft_data['round'].isin([14, 15])].copy()\n",
    "\n",
    "draft_data.drop(columns=['k_need','k_available','k_vor','other_k_need','dst_available','dst_need','other_dst_need'            ], inplace=True)\n",
    "draft_data.drop(columns=['draft_id'], inplace=True)\n",
    "#draft_data.drop(columns=['pick_no'], inplace=True)\n",
    "#draft_data.drop(columns=['qb_need', 'other_qb_need', 'qb_vor', 'qb_available'], inplace=True)\n",
    "#draft_data.drop(columns=['te_need', 'other_te_need', 'te_vor', 'te_available'], inplace=True)\n",
    "#draft_data.drop(columns=['other_qb_need', 'other_rb_need', 'other_wr_need', 'other_te_need', 'other_flex_need'], inplace=True)\n",
    "#draft_data.drop(columns=['qb_available', 'rb_available', 'wr_available', 'te_available', 'flex_available'], inplace=True)\n",
    "draft_data.drop(columns=['qb_vor', 'rb_vor', 'wr_vor', 'te_vor', 'flex_vor'], inplace=True)\n",
    "#draft_data = draft_data[~draft_data['position_drafted'].isin(['QB', 'TE'])].copy()\n",
    "draft_data.drop(columns=['scoring_type'], inplace=True)\n",
    "\n",
    "bounded_cols = [\n",
    "    'pick_no','round',\n",
    "    'qb_need', 'rb_need', 'wr_need', 'te_need', 'flex_need',\n",
    "    'other_qb_need', 'other_rb_need', 'other_wr_need', 'other_te_need', 'other_flex_need',\n",
    "    'qb_available', 'rb_available', 'wr_available', 'te_available', 'flex_available'\n",
    "]\n",
    "vor_cols = ['qb_vor', 'rb_vor', 'wr_vor', 'te_vor', 'flex_vor']\n",
    "\n",
    "#draft_data[bounded_cols + vor_cols] = draft_data[bounded_cols + vor_cols].astype(float)\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "draft_data[bounded_cols] = minmax_scaler.fit_transform(draft_data[bounded_cols])\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "#draft_data[vor_cols] = std_scaler.fit_transform(draft_data[vor_cols])\n",
    "\n",
    "#draft_data.drop(columns=['flex_vor', 'other_flex_need', 'flex_available','flex_need'], inplace=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "draft_data['position_drafted_encoded'] = le.fit_transform(draft_data['position_drafted'])"
   ],
   "id": "ecbeed41ca3a33ce",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:32:34.285188Z",
     "start_time": "2025-05-22T17:32:34.276164Z"
    }
   },
   "cell_type": "code",
   "source": "draft_data.head()",
   "id": "a85ea3f461e239de",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    pick_no  round  qb_need  rb_need  wr_need  te_need  flex_need  \\\n",
       "0  0.000000    0.0      1.0      1.0      1.0      1.0        1.0   \n",
       "1  0.007752    0.0      1.0      1.0      1.0      1.0        1.0   \n",
       "2  0.015504    0.0      1.0      1.0      1.0      1.0        1.0   \n",
       "3  0.023256    0.0      1.0      1.0      1.0      1.0        1.0   \n",
       "4  0.031008    0.0      1.0      1.0      1.0      1.0        1.0   \n",
       "\n",
       "   other_qb_need  other_rb_need  other_wr_need  other_te_need  \\\n",
       "0            1.0       1.000000       1.000000            1.0   \n",
       "1            1.0       1.000000       0.944444            1.0   \n",
       "2            1.0       1.000000       0.888889            1.0   \n",
       "3            1.0       0.944444       0.888889            1.0   \n",
       "4            1.0       0.888889       0.888889            1.0   \n",
       "\n",
       "   other_flex_need  qb_available  rb_available  wr_available  te_available  \\\n",
       "0              1.0           1.0      1.000000      1.000000           1.0   \n",
       "1              1.0           1.0      1.000000      0.979167           1.0   \n",
       "2              1.0           1.0      1.000000      0.958333           1.0   \n",
       "3              1.0           1.0      0.973684      0.958333           1.0   \n",
       "4              1.0           1.0      0.947368      0.958333           1.0   \n",
       "\n",
       "   flex_available position_drafted  position_drafted_encoded  \n",
       "0        1.000000               WR                         3  \n",
       "1        0.989899               WR                         3  \n",
       "2        0.979798               RB                         1  \n",
       "3        0.969697               RB                         1  \n",
       "4        0.959596               WR                         3  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pick_no</th>\n",
       "      <th>round</th>\n",
       "      <th>qb_need</th>\n",
       "      <th>rb_need</th>\n",
       "      <th>wr_need</th>\n",
       "      <th>te_need</th>\n",
       "      <th>flex_need</th>\n",
       "      <th>other_qb_need</th>\n",
       "      <th>other_rb_need</th>\n",
       "      <th>other_wr_need</th>\n",
       "      <th>other_te_need</th>\n",
       "      <th>other_flex_need</th>\n",
       "      <th>qb_available</th>\n",
       "      <th>rb_available</th>\n",
       "      <th>wr_available</th>\n",
       "      <th>te_available</th>\n",
       "      <th>flex_available</th>\n",
       "      <th>position_drafted</th>\n",
       "      <th>position_drafted_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>WR</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>WR</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>RB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>RB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>WR</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:32:34.358049Z",
     "start_time": "2025-05-22T17:32:34.346076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_val, test = train_test_split(draft_data, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train_val, test_size=0.25, random_state=42)\n",
    "X_train = train.drop(columns=[ 'position_drafted','position_drafted_encoded'])\n",
    "y_train = train['position_drafted_encoded']\n",
    "X_val = val.drop(columns=['position_drafted','position_drafted_encoded'])\n",
    "y_val = val['position_drafted_encoded']\n",
    "X_test = test.drop(columns=[ 'position_drafted','position_drafted_encoded'])\n",
    "y_test = test['position_drafted_encoded']\n"
   ],
   "id": "1c58a19f598f2be5",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:32:34.440216Z",
     "start_time": "2025-05-22T17:32:34.407039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_train = le.transform(train['position_drafted'])\n",
    "y_val = le.transform(val['position_drafted'])\n",
    "y_test = le.transform(test['position_drafted'])\n",
    "position_mapping = dict(enumerate(le.classes_))\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "print(position_mapping)"
   ],
   "id": "c603e29cb885402b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'QB', 1: 'RB', 2: 'TE', 3: 'WR'}\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:32:34.499450Z",
     "start_time": "2025-05-22T17:32:34.496505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"X Train Shape: {X_train.shape}\")\n",
    "print(f\"X Test Shape: {X_test.shape}\")\n",
    "print(f\"X Val Shape: {X_val.shape}\")\n",
    "print(f\"y Train Shape: {y_train.shape}\")\n",
    "print(f\"y Test Shape: {y_test.shape}\")\n",
    "print(f\"y Val Shape: {y_val.shape}\")"
   ],
   "id": "ead1e80059161f3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train Shape: (3892, 17)\n",
      "X Test Shape: (780, 17)\n",
      "X Val Shape: (780, 17)\n",
      "y Train Shape: (3892,)\n",
      "y Test Shape: (780,)\n",
      "y Val Shape: (780,)\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:32:34.537323Z",
     "start_time": "2025-05-22T17:32:34.533869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_counts = pd.Series(y_train).value_counts()\n",
    "print(class_counts)"
   ],
   "id": "4841a14376beab09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    973\n",
      "2    973\n",
      "3    973\n",
      "0    973\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:44:04.834429Z",
     "start_time": "2025-05-22T17:44:04.630220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "x1 = layers.Dense(1028)(inputs)\n",
    "x1 = layers.LeakyReLU()(x1)\n",
    "x1 = layers.LayerNormalization()(x1)\n",
    "x1 = layers.Dropout(0.2)(x1)\n",
    "\n",
    "x2 = layers.Dense(512)(x1)\n",
    "x2 = layers.LeakyReLU()(x2)\n",
    "x2 = layers.LayerNormalization()(x2)\n",
    "x2 = layers.Dropout(0.2)(x2)\n",
    "\n",
    "x3 = layers.Dense(256)(x2)\n",
    "x3 = layers.LeakyReLU()(x3)\n",
    "x3 = layers.LayerNormalization()(x3)\n",
    "x3 = layers.Dropout(0.2)(x3)\n",
    "\n",
    "x4 = layers.Dense(128)(x3)\n",
    "x4 = layers.LeakyReLU()(x4)\n",
    "x4 = layers.LayerNormalization()(x4)\n",
    "x4 = layers.Dropout(0.2)(x4)\n",
    "\n",
    "concat = layers.Concatenate()([inputs, x1, x2, x3, x4])\n",
    "\n",
    "outputs = layers.Dense(len(position_mapping), activation='softmax')(concat)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ],
   "id": "7d515ab928188b39",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:58:24.026808Z",
     "start_time": "2025-05-22T17:44:04.903332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='../models/best_model.weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=1000,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=32,\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ],
   "id": "c2f3ab069809b0b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 1.5341 - accuracy: 0.4052\n",
      "Epoch 1: val_loss improved from inf to 1.27394, saving model to best_model.weights.h5\n",
      "122/122 [==============================] - 2s 8ms/step - loss: 1.5276 - accuracy: 0.4052 - val_loss: 1.2739 - val_accuracy: 0.4962\n",
      "Epoch 2/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 1.2598 - accuracy: 0.4258\n",
      "Epoch 2: val_loss did not improve from 1.27394\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.2588 - accuracy: 0.4268 - val_loss: 1.4139 - val_accuracy: 0.3423\n",
      "Epoch 3/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 1.2017 - accuracy: 0.4600\n",
      "Epoch 3: val_loss improved from 1.27394 to 1.06375, saving model to best_model.weights.h5\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.2013 - accuracy: 0.4599 - val_loss: 1.0637 - val_accuracy: 0.4782\n",
      "Epoch 4/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 1.2118 - accuracy: 0.4526\n",
      "Epoch 4: val_loss did not improve from 1.06375\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.2091 - accuracy: 0.4538 - val_loss: 1.4133 - val_accuracy: 0.3808\n",
      "Epoch 5/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 1.1666 - accuracy: 0.4721\n",
      "Epoch 5: val_loss did not improve from 1.06375\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.1651 - accuracy: 0.4723 - val_loss: 1.3379 - val_accuracy: 0.3410\n",
      "Epoch 6/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 1.1095 - accuracy: 0.4899\n",
      "Epoch 6: val_loss did not improve from 1.06375\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.1091 - accuracy: 0.4923 - val_loss: 1.1620 - val_accuracy: 0.4282\n",
      "Epoch 7/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 1.1078 - accuracy: 0.4853\n",
      "Epoch 7: val_loss did not improve from 1.06375\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.1083 - accuracy: 0.4861 - val_loss: 1.1650 - val_accuracy: 0.4654\n",
      "Epoch 8/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 1.0849 - accuracy: 0.5139\n",
      "Epoch 8: val_loss did not improve from 1.06375\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.0849 - accuracy: 0.5139 - val_loss: 1.0787 - val_accuracy: 0.5051\n",
      "Epoch 9/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 1.0770 - accuracy: 0.5108\n",
      "Epoch 9: val_loss did not improve from 1.06375\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.0772 - accuracy: 0.5092 - val_loss: 1.2559 - val_accuracy: 0.4000\n",
      "Epoch 10/1000\n",
      "113/122 [==========================>...] - ETA: 0s - loss: 1.0593 - accuracy: 0.5102\n",
      "Epoch 10: val_loss improved from 1.06375 to 1.02478, saving model to best_model.weights.h5\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.0600 - accuracy: 0.5100 - val_loss: 1.0248 - val_accuracy: 0.4821\n",
      "Epoch 11/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 1.0537 - accuracy: 0.5080\n",
      "Epoch 11: val_loss did not improve from 1.02478\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 1.0517 - accuracy: 0.5095 - val_loss: 1.1217 - val_accuracy: 0.4526\n",
      "Epoch 12/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 1.0080 - accuracy: 0.5413\n",
      "Epoch 12: val_loss did not improve from 1.02478\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.0127 - accuracy: 0.5391 - val_loss: 1.0871 - val_accuracy: 0.4513\n",
      "Epoch 13/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 1.0243 - accuracy: 0.5171\n",
      "Epoch 13: val_loss did not improve from 1.02478\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 1.0274 - accuracy: 0.5167 - val_loss: 1.0639 - val_accuracy: 0.4885\n",
      "Epoch 14/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 1.0079 - accuracy: 0.5356\n",
      "Epoch 14: val_loss improved from 1.02478 to 1.01057, saving model to best_model.weights.h5\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.0081 - accuracy: 0.5367 - val_loss: 1.0106 - val_accuracy: 0.5013\n",
      "Epoch 15/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 1.0117 - accuracy: 0.5307\n",
      "Epoch 15: val_loss did not improve from 1.01057\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 1.0115 - accuracy: 0.5311 - val_loss: 1.0312 - val_accuracy: 0.5218\n",
      "Epoch 16/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.9837 - accuracy: 0.5470\n",
      "Epoch 16: val_loss improved from 1.01057 to 1.00816, saving model to best_model.weights.h5\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.9837 - accuracy: 0.5470 - val_loss: 1.0082 - val_accuracy: 0.5077\n",
      "Epoch 17/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.9925 - accuracy: 0.5438\n",
      "Epoch 17: val_loss did not improve from 1.00816\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9963 - accuracy: 0.5437 - val_loss: 1.0627 - val_accuracy: 0.4667\n",
      "Epoch 18/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 1.0028 - accuracy: 0.5361\n",
      "Epoch 18: val_loss did not improve from 1.00816\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.0005 - accuracy: 0.5380 - val_loss: 1.0254 - val_accuracy: 0.5051\n",
      "Epoch 19/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.9933 - accuracy: 0.5355\n",
      "Epoch 19: val_loss did not improve from 1.00816\n",
      "122/122 [==============================] - 1s 12ms/step - loss: 0.9917 - accuracy: 0.5357 - val_loss: 1.0475 - val_accuracy: 0.4821\n",
      "Epoch 20/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.9663 - accuracy: 0.5546\n",
      "Epoch 20: val_loss did not improve from 1.00816\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9679 - accuracy: 0.5532 - val_loss: 1.0382 - val_accuracy: 0.5192\n",
      "Epoch 21/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.9854 - accuracy: 0.5462\n",
      "Epoch 21: val_loss did not improve from 1.00816\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9858 - accuracy: 0.5452 - val_loss: 1.0590 - val_accuracy: 0.5000\n",
      "Epoch 22/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.9758 - accuracy: 0.5546\n",
      "Epoch 22: val_loss did not improve from 1.00816\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9761 - accuracy: 0.5552 - val_loss: 1.0425 - val_accuracy: 0.5051\n",
      "Epoch 23/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.9844 - accuracy: 0.5435\n",
      "Epoch 23: val_loss did not improve from 1.00816\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9866 - accuracy: 0.5419 - val_loss: 1.1359 - val_accuracy: 0.4603\n",
      "Epoch 24/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.9723 - accuracy: 0.5517\n",
      "Epoch 24: val_loss did not improve from 1.00816\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9716 - accuracy: 0.5524 - val_loss: 1.0471 - val_accuracy: 0.4949\n",
      "Epoch 25/1000\n",
      "113/122 [==========================>...] - ETA: 0s - loss: 0.9562 - accuracy: 0.5694\n",
      "Epoch 25: val_loss did not improve from 1.00816\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9510 - accuracy: 0.5725 - val_loss: 1.0147 - val_accuracy: 0.5013\n",
      "Epoch 26/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.9543 - accuracy: 0.5584\n",
      "Epoch 26: val_loss did not improve from 1.00816\n",
      "122/122 [==============================] - 1s 11ms/step - loss: 0.9551 - accuracy: 0.5576 - val_loss: 1.0316 - val_accuracy: 0.4936\n",
      "Epoch 27/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.9621 - accuracy: 0.5658\n",
      "Epoch 27: val_loss did not improve from 1.00816\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.9635 - accuracy: 0.5632 - val_loss: 1.0462 - val_accuracy: 0.5000\n",
      "Epoch 28/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.9485 - accuracy: 0.5543\n",
      "Epoch 28: val_loss improved from 1.00816 to 0.97707, saving model to best_model.weights.h5\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9498 - accuracy: 0.5534 - val_loss: 0.9771 - val_accuracy: 0.5269\n",
      "Epoch 29/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.9486 - accuracy: 0.5647\n",
      "Epoch 29: val_loss did not improve from 0.97707\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.9486 - accuracy: 0.5647 - val_loss: 0.9882 - val_accuracy: 0.5372\n",
      "Epoch 30/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.9371 - accuracy: 0.5768\n",
      "Epoch 30: val_loss did not improve from 0.97707\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9364 - accuracy: 0.5773 - val_loss: 1.1475 - val_accuracy: 0.4513\n",
      "Epoch 31/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.9430 - accuracy: 0.5712\n",
      "Epoch 31: val_loss did not improve from 0.97707\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9420 - accuracy: 0.5730 - val_loss: 1.2867 - val_accuracy: 0.3923\n",
      "Epoch 32/1000\n",
      "113/122 [==========================>...] - ETA: 0s - loss: 0.9325 - accuracy: 0.5697\n",
      "Epoch 32: val_loss did not improve from 0.97707\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9350 - accuracy: 0.5699 - val_loss: 1.0253 - val_accuracy: 0.5192\n",
      "Epoch 33/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.9339 - accuracy: 0.5733\n",
      "Epoch 33: val_loss did not improve from 0.97707\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9332 - accuracy: 0.5748 - val_loss: 1.0283 - val_accuracy: 0.4936\n",
      "Epoch 34/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.9440 - accuracy: 0.5564\n",
      "Epoch 34: val_loss did not improve from 0.97707\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9406 - accuracy: 0.5588 - val_loss: 1.0397 - val_accuracy: 0.4936\n",
      "Epoch 35/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.9406 - accuracy: 0.5693\n",
      "Epoch 35: val_loss improved from 0.97707 to 0.95548, saving model to best_model.weights.h5\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9424 - accuracy: 0.5671 - val_loss: 0.9555 - val_accuracy: 0.5372\n",
      "Epoch 36/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.9266 - accuracy: 0.5801\n",
      "Epoch 36: val_loss did not improve from 0.95548\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9266 - accuracy: 0.5804 - val_loss: 1.0713 - val_accuracy: 0.4808\n",
      "Epoch 37/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.9407 - accuracy: 0.5671\n",
      "Epoch 37: val_loss did not improve from 0.95548\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.9407 - accuracy: 0.5671 - val_loss: 1.0890 - val_accuracy: 0.4731\n",
      "Epoch 38/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.9376 - accuracy: 0.5723\n",
      "Epoch 38: val_loss did not improve from 0.95548\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.9368 - accuracy: 0.5709 - val_loss: 0.9933 - val_accuracy: 0.5244\n",
      "Epoch 39/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.9219 - accuracy: 0.5818\n",
      "Epoch 39: val_loss did not improve from 0.95548\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9262 - accuracy: 0.5779 - val_loss: 0.9818 - val_accuracy: 0.5179\n",
      "Epoch 40/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.9280 - accuracy: 0.5768\n",
      "Epoch 40: val_loss did not improve from 0.95548\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.9269 - accuracy: 0.5768 - val_loss: 1.0143 - val_accuracy: 0.5077\n",
      "Epoch 41/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.9248 - accuracy: 0.5732\n",
      "Epoch 41: val_loss did not improve from 0.95548\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.9252 - accuracy: 0.5717 - val_loss: 0.9855 - val_accuracy: 0.5282\n",
      "Epoch 42/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.9055 - accuracy: 0.5969\n",
      "Epoch 42: val_loss did not improve from 0.95548\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.9055 - accuracy: 0.5969 - val_loss: 1.1577 - val_accuracy: 0.4423\n",
      "Epoch 43/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.9183 - accuracy: 0.5776\n",
      "Epoch 43: val_loss did not improve from 0.95548\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9174 - accuracy: 0.5779 - val_loss: 1.0251 - val_accuracy: 0.5321\n",
      "Epoch 44/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.9022 - accuracy: 0.5844\n",
      "Epoch 44: val_loss did not improve from 0.95548\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9048 - accuracy: 0.5830 - val_loss: 1.0117 - val_accuracy: 0.5359\n",
      "Epoch 45/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.9203 - accuracy: 0.5844\n",
      "Epoch 45: val_loss did not improve from 0.95548\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.9195 - accuracy: 0.5848 - val_loss: 0.9742 - val_accuracy: 0.5244\n",
      "Epoch 46/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.8998 - accuracy: 0.5943\n",
      "Epoch 46: val_loss did not improve from 0.95548\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8998 - accuracy: 0.5943 - val_loss: 1.0318 - val_accuracy: 0.5436\n",
      "Epoch 47/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.9127 - accuracy: 0.5906\n",
      "Epoch 47: val_loss did not improve from 0.95548\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.9102 - accuracy: 0.5912 - val_loss: 1.0007 - val_accuracy: 0.5372\n",
      "Epoch 48/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.9138 - accuracy: 0.5897\n",
      "Epoch 48: val_loss improved from 0.95548 to 0.95041, saving model to best_model.weights.h5\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9135 - accuracy: 0.5915 - val_loss: 0.9504 - val_accuracy: 0.5385\n",
      "Epoch 49/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.8912 - accuracy: 0.5938\n",
      "Epoch 49: val_loss improved from 0.95041 to 0.94758, saving model to best_model.weights.h5\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8931 - accuracy: 0.5920 - val_loss: 0.9476 - val_accuracy: 0.5474\n",
      "Epoch 50/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.9126 - accuracy: 0.5816\n",
      "Epoch 50: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.9096 - accuracy: 0.5825 - val_loss: 1.0312 - val_accuracy: 0.5038\n",
      "Epoch 51/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.9200 - accuracy: 0.5823\n",
      "Epoch 51: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.9193 - accuracy: 0.5822 - val_loss: 1.0911 - val_accuracy: 0.5077\n",
      "Epoch 52/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.8818 - accuracy: 0.6048\n",
      "Epoch 52: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8889 - accuracy: 0.6020 - val_loss: 1.0125 - val_accuracy: 0.5026\n",
      "Epoch 53/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.9030 - accuracy: 0.5946\n",
      "Epoch 53: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9030 - accuracy: 0.5946 - val_loss: 1.0320 - val_accuracy: 0.5090\n",
      "Epoch 54/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.8905 - accuracy: 0.5959\n",
      "Epoch 54: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8895 - accuracy: 0.5951 - val_loss: 1.0840 - val_accuracy: 0.4769\n",
      "Epoch 55/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.8940 - accuracy: 0.5964\n",
      "Epoch 55: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8931 - accuracy: 0.5966 - val_loss: 0.9531 - val_accuracy: 0.5359\n",
      "Epoch 56/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.8998 - accuracy: 0.5889\n",
      "Epoch 56: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8958 - accuracy: 0.5922 - val_loss: 1.0164 - val_accuracy: 0.5205\n",
      "Epoch 57/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.8790 - accuracy: 0.6008\n",
      "Epoch 57: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8785 - accuracy: 0.6015 - val_loss: 1.0380 - val_accuracy: 0.5090\n",
      "Epoch 58/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.8795 - accuracy: 0.6034\n",
      "Epoch 58: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8786 - accuracy: 0.6007 - val_loss: 0.9639 - val_accuracy: 0.5295\n",
      "Epoch 59/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.8874 - accuracy: 0.5965\n",
      "Epoch 59: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8894 - accuracy: 0.5971 - val_loss: 0.9551 - val_accuracy: 0.5474\n",
      "Epoch 60/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.8860 - accuracy: 0.5967\n",
      "Epoch 60: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8858 - accuracy: 0.5979 - val_loss: 0.9860 - val_accuracy: 0.5385\n",
      "Epoch 61/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.8673 - accuracy: 0.5979\n",
      "Epoch 61: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8723 - accuracy: 0.5979 - val_loss: 1.0554 - val_accuracy: 0.5141\n",
      "Epoch 62/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.8985 - accuracy: 0.5943\n",
      "Epoch 62: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8970 - accuracy: 0.5948 - val_loss: 0.9815 - val_accuracy: 0.5359\n",
      "Epoch 63/1000\n",
      "113/122 [==========================>...] - ETA: 0s - loss: 0.8713 - accuracy: 0.6139\n",
      "Epoch 63: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8770 - accuracy: 0.6102 - val_loss: 1.0166 - val_accuracy: 0.5090\n",
      "Epoch 64/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.8780 - accuracy: 0.6031\n",
      "Epoch 64: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8754 - accuracy: 0.6059 - val_loss: 1.0003 - val_accuracy: 0.5282\n",
      "Epoch 65/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.8837 - accuracy: 0.6098\n",
      "Epoch 65: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8828 - accuracy: 0.6102 - val_loss: 0.9722 - val_accuracy: 0.5282\n",
      "Epoch 66/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.8725 - accuracy: 0.6083\n",
      "Epoch 66: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8707 - accuracy: 0.6095 - val_loss: 1.0520 - val_accuracy: 0.4923\n",
      "Epoch 67/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.8719 - accuracy: 0.6055\n",
      "Epoch 67: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8702 - accuracy: 0.6066 - val_loss: 0.9862 - val_accuracy: 0.5449\n",
      "Epoch 68/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.8562 - accuracy: 0.6192\n",
      "Epoch 68: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8562 - accuracy: 0.6192 - val_loss: 1.0746 - val_accuracy: 0.4962\n",
      "Epoch 69/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.8735 - accuracy: 0.6123\n",
      "Epoch 69: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8735 - accuracy: 0.6123 - val_loss: 1.0010 - val_accuracy: 0.5231\n",
      "Epoch 70/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.8608 - accuracy: 0.6096\n",
      "Epoch 70: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8624 - accuracy: 0.6089 - val_loss: 0.9920 - val_accuracy: 0.5141\n",
      "Epoch 71/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.8756 - accuracy: 0.6001\n",
      "Epoch 71: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8759 - accuracy: 0.5979 - val_loss: 1.0007 - val_accuracy: 0.5256\n",
      "Epoch 72/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.8751 - accuracy: 0.6112\n",
      "Epoch 72: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8743 - accuracy: 0.6115 - val_loss: 1.1164 - val_accuracy: 0.4808\n",
      "Epoch 73/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.8529 - accuracy: 0.6165\n",
      "Epoch 73: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8547 - accuracy: 0.6172 - val_loss: 0.9903 - val_accuracy: 0.5308\n",
      "Epoch 74/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.8509 - accuracy: 0.6289\n",
      "Epoch 74: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8523 - accuracy: 0.6285 - val_loss: 0.9809 - val_accuracy: 0.5474\n",
      "Epoch 75/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.8772 - accuracy: 0.6010\n",
      "Epoch 75: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8739 - accuracy: 0.6025 - val_loss: 1.0332 - val_accuracy: 0.5026\n",
      "Epoch 76/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.8574 - accuracy: 0.6193\n",
      "Epoch 76: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8592 - accuracy: 0.6172 - val_loss: 0.9947 - val_accuracy: 0.5282\n",
      "Epoch 77/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.8644 - accuracy: 0.6143\n",
      "Epoch 77: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8633 - accuracy: 0.6149 - val_loss: 1.0373 - val_accuracy: 0.5179\n",
      "Epoch 78/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.8568 - accuracy: 0.6090\n",
      "Epoch 78: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8552 - accuracy: 0.6105 - val_loss: 1.0046 - val_accuracy: 0.5372\n",
      "Epoch 79/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.8430 - accuracy: 0.6218\n",
      "Epoch 79: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8437 - accuracy: 0.6223 - val_loss: 0.9779 - val_accuracy: 0.5282\n",
      "Epoch 80/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.8513 - accuracy: 0.6223\n",
      "Epoch 80: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8508 - accuracy: 0.6215 - val_loss: 1.0224 - val_accuracy: 0.4962\n",
      "Epoch 81/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.8590 - accuracy: 0.6150\n",
      "Epoch 81: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8574 - accuracy: 0.6156 - val_loss: 1.0411 - val_accuracy: 0.5269\n",
      "Epoch 82/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.8442 - accuracy: 0.6218\n",
      "Epoch 82: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8441 - accuracy: 0.6233 - val_loss: 1.0824 - val_accuracy: 0.5321\n",
      "Epoch 83/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.8455 - accuracy: 0.6197\n",
      "Epoch 83: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8495 - accuracy: 0.6187 - val_loss: 1.0371 - val_accuracy: 0.5064\n",
      "Epoch 84/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.8532 - accuracy: 0.6096\n",
      "Epoch 84: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8511 - accuracy: 0.6115 - val_loss: 1.0466 - val_accuracy: 0.5295\n",
      "Epoch 85/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.8565 - accuracy: 0.6117\n",
      "Epoch 85: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8597 - accuracy: 0.6100 - val_loss: 1.0040 - val_accuracy: 0.5308\n",
      "Epoch 86/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.8554 - accuracy: 0.6099\n",
      "Epoch 86: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8556 - accuracy: 0.6092 - val_loss: 1.0638 - val_accuracy: 0.5064\n",
      "Epoch 87/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.8403 - accuracy: 0.6166\n",
      "Epoch 87: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8482 - accuracy: 0.6141 - val_loss: 0.9635 - val_accuracy: 0.5372\n",
      "Epoch 88/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.8488 - accuracy: 0.6191\n",
      "Epoch 88: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8484 - accuracy: 0.6184 - val_loss: 1.0312 - val_accuracy: 0.5244\n",
      "Epoch 89/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.8426 - accuracy: 0.6245\n",
      "Epoch 89: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8432 - accuracy: 0.6238 - val_loss: 1.0498 - val_accuracy: 0.5256\n",
      "Epoch 90/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.8505 - accuracy: 0.6196\n",
      "Epoch 90: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8491 - accuracy: 0.6202 - val_loss: 0.9794 - val_accuracy: 0.5487\n",
      "Epoch 91/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.8563 - accuracy: 0.6150\n",
      "Epoch 91: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8576 - accuracy: 0.6136 - val_loss: 1.0176 - val_accuracy: 0.5256\n",
      "Epoch 92/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.8358 - accuracy: 0.6223\n",
      "Epoch 92: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.8358 - accuracy: 0.6223 - val_loss: 0.9597 - val_accuracy: 0.5564\n",
      "Epoch 93/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.8454 - accuracy: 0.6216\n",
      "Epoch 93: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8421 - accuracy: 0.6223 - val_loss: 0.9757 - val_accuracy: 0.5282\n",
      "Epoch 94/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.8464 - accuracy: 0.6260\n",
      "Epoch 94: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.8488 - accuracy: 0.6246 - val_loss: 1.0620 - val_accuracy: 0.5103\n",
      "Epoch 95/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.8412 - accuracy: 0.6271\n",
      "Epoch 95: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8411 - accuracy: 0.6269 - val_loss: 1.0586 - val_accuracy: 0.4859\n",
      "Epoch 96/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.8554 - accuracy: 0.6183\n",
      "Epoch 96: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.8537 - accuracy: 0.6197 - val_loss: 1.0392 - val_accuracy: 0.5423\n",
      "Epoch 97/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.8515 - accuracy: 0.6247\n",
      "Epoch 97: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.8481 - accuracy: 0.6259 - val_loss: 1.0267 - val_accuracy: 0.5282\n",
      "Epoch 98/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.8379 - accuracy: 0.6284\n",
      "Epoch 98: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.8389 - accuracy: 0.6280 - val_loss: 1.0961 - val_accuracy: 0.4910\n",
      "Epoch 99/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.8348 - accuracy: 0.6166\n",
      "Epoch 99: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 2s 14ms/step - loss: 0.8348 - accuracy: 0.6166 - val_loss: 1.0683 - val_accuracy: 0.5038\n",
      "Epoch 100/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.8353 - accuracy: 0.6274\n",
      "Epoch 100: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8336 - accuracy: 0.6280 - val_loss: 1.0045 - val_accuracy: 0.5128\n",
      "Epoch 101/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.8265 - accuracy: 0.6296\n",
      "Epoch 101: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8262 - accuracy: 0.6295 - val_loss: 0.9800 - val_accuracy: 0.5385\n",
      "Epoch 102/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.8271 - accuracy: 0.6375\n",
      "Epoch 102: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.8271 - accuracy: 0.6375 - val_loss: 0.9723 - val_accuracy: 0.5564\n",
      "Epoch 103/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.8340 - accuracy: 0.6301\n",
      "Epoch 103: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8376 - accuracy: 0.6287 - val_loss: 0.9959 - val_accuracy: 0.5372\n",
      "Epoch 104/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.8316 - accuracy: 0.6311\n",
      "Epoch 104: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8296 - accuracy: 0.6328 - val_loss: 0.9827 - val_accuracy: 0.5333\n",
      "Epoch 105/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.8418 - accuracy: 0.6250\n",
      "Epoch 105: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8371 - accuracy: 0.6285 - val_loss: 1.0280 - val_accuracy: 0.5346\n",
      "Epoch 106/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.8151 - accuracy: 0.6384\n",
      "Epoch 106: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8151 - accuracy: 0.6382 - val_loss: 1.0760 - val_accuracy: 0.5013\n",
      "Epoch 107/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.8300 - accuracy: 0.6258\n",
      "Epoch 107: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8309 - accuracy: 0.6256 - val_loss: 1.0219 - val_accuracy: 0.5321\n",
      "Epoch 108/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.8285 - accuracy: 0.6245\n",
      "Epoch 108: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8317 - accuracy: 0.6231 - val_loss: 0.9980 - val_accuracy: 0.5359\n",
      "Epoch 109/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.8230 - accuracy: 0.6316\n",
      "Epoch 109: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.8230 - accuracy: 0.6316 - val_loss: 1.0133 - val_accuracy: 0.5346\n",
      "Epoch 110/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.8238 - accuracy: 0.6295\n",
      "Epoch 110: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.8238 - accuracy: 0.6295 - val_loss: 1.0591 - val_accuracy: 0.5192\n",
      "Epoch 111/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.8429 - accuracy: 0.6255\n",
      "Epoch 111: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8434 - accuracy: 0.6249 - val_loss: 0.9736 - val_accuracy: 0.5474\n",
      "Epoch 112/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.8337 - accuracy: 0.6304\n",
      "Epoch 112: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8331 - accuracy: 0.6308 - val_loss: 1.0617 - val_accuracy: 0.5269\n",
      "Epoch 113/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.8283 - accuracy: 0.6358\n",
      "Epoch 113: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8250 - accuracy: 0.6377 - val_loss: 1.0637 - val_accuracy: 0.5115\n",
      "Epoch 114/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.8264 - accuracy: 0.6292\n",
      "Epoch 114: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8264 - accuracy: 0.6292 - val_loss: 1.1524 - val_accuracy: 0.4962\n",
      "Epoch 115/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.8478 - accuracy: 0.6186\n",
      "Epoch 115: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8482 - accuracy: 0.6177 - val_loss: 1.0258 - val_accuracy: 0.5372\n",
      "Epoch 116/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.8270 - accuracy: 0.6377\n",
      "Epoch 116: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8243 - accuracy: 0.6380 - val_loss: 1.0994 - val_accuracy: 0.5115\n",
      "Epoch 117/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.8175 - accuracy: 0.6413\n",
      "Epoch 117: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8169 - accuracy: 0.6416 - val_loss: 1.0186 - val_accuracy: 0.5090\n",
      "Epoch 118/1000\n",
      "113/122 [==========================>...] - ETA: 0s - loss: 0.8243 - accuracy: 0.6341\n",
      "Epoch 118: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8291 - accuracy: 0.6326 - val_loss: 0.9993 - val_accuracy: 0.5282\n",
      "Epoch 119/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.8241 - accuracy: 0.6349\n",
      "Epoch 119: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8260 - accuracy: 0.6362 - val_loss: 0.9850 - val_accuracy: 0.5513\n",
      "Epoch 120/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.8257 - accuracy: 0.6213\n",
      "Epoch 120: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8246 - accuracy: 0.6226 - val_loss: 1.0012 - val_accuracy: 0.5321\n",
      "Epoch 121/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.8300 - accuracy: 0.6302\n",
      "Epoch 121: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 2s 15ms/step - loss: 0.8308 - accuracy: 0.6298 - val_loss: 1.0388 - val_accuracy: 0.5026\n",
      "Epoch 122/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.8161 - accuracy: 0.6372\n",
      "Epoch 122: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.8146 - accuracy: 0.6380 - val_loss: 1.0258 - val_accuracy: 0.5077\n",
      "Epoch 123/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.8214 - accuracy: 0.6336\n",
      "Epoch 123: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8192 - accuracy: 0.6349 - val_loss: 1.0741 - val_accuracy: 0.5205\n",
      "Epoch 124/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.8287 - accuracy: 0.6253\n",
      "Epoch 124: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8258 - accuracy: 0.6274 - val_loss: 0.9985 - val_accuracy: 0.5321\n",
      "Epoch 125/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.8134 - accuracy: 0.6431\n",
      "Epoch 125: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8128 - accuracy: 0.6436 - val_loss: 1.0249 - val_accuracy: 0.5269\n",
      "Epoch 126/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.8053 - accuracy: 0.6402\n",
      "Epoch 126: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8042 - accuracy: 0.6411 - val_loss: 1.0202 - val_accuracy: 0.5179\n",
      "Epoch 127/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.8246 - accuracy: 0.6309\n",
      "Epoch 127: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8258 - accuracy: 0.6303 - val_loss: 1.0518 - val_accuracy: 0.5077\n",
      "Epoch 128/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.8114 - accuracy: 0.6406\n",
      "Epoch 128: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8088 - accuracy: 0.6426 - val_loss: 1.0466 - val_accuracy: 0.5205\n",
      "Epoch 129/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.8030 - accuracy: 0.6385\n",
      "Epoch 129: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8021 - accuracy: 0.6380 - val_loss: 0.9681 - val_accuracy: 0.5385\n",
      "Epoch 130/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.8174 - accuracy: 0.6340\n",
      "Epoch 130: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8173 - accuracy: 0.6336 - val_loss: 1.0788 - val_accuracy: 0.4897\n",
      "Epoch 131/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.8115 - accuracy: 0.6414\n",
      "Epoch 131: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8127 - accuracy: 0.6395 - val_loss: 1.0075 - val_accuracy: 0.5256\n",
      "Epoch 132/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.8200 - accuracy: 0.6360\n",
      "Epoch 132: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8202 - accuracy: 0.6362 - val_loss: 0.9957 - val_accuracy: 0.5577\n",
      "Epoch 133/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.8066 - accuracy: 0.6413\n",
      "Epoch 133: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8093 - accuracy: 0.6395 - val_loss: 0.9921 - val_accuracy: 0.5385\n",
      "Epoch 134/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.8231 - accuracy: 0.6392\n",
      "Epoch 134: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8237 - accuracy: 0.6390 - val_loss: 1.0588 - val_accuracy: 0.5372\n",
      "Epoch 135/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.8316 - accuracy: 0.6182\n",
      "Epoch 135: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8316 - accuracy: 0.6182 - val_loss: 1.0574 - val_accuracy: 0.5410\n",
      "Epoch 136/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.8189 - accuracy: 0.6395\n",
      "Epoch 136: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8187 - accuracy: 0.6395 - val_loss: 0.9575 - val_accuracy: 0.5603\n",
      "Epoch 137/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.8091 - accuracy: 0.6368\n",
      "Epoch 137: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8078 - accuracy: 0.6375 - val_loss: 1.0133 - val_accuracy: 0.5487\n",
      "Epoch 138/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7973 - accuracy: 0.6474\n",
      "Epoch 138: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8006 - accuracy: 0.6457 - val_loss: 1.0169 - val_accuracy: 0.5372\n",
      "Epoch 139/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.8144 - accuracy: 0.6426\n",
      "Epoch 139: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8088 - accuracy: 0.6434 - val_loss: 1.0116 - val_accuracy: 0.5269\n",
      "Epoch 140/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.8075 - accuracy: 0.6372\n",
      "Epoch 140: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8075 - accuracy: 0.6372 - val_loss: 1.0168 - val_accuracy: 0.5218\n",
      "Epoch 141/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.7953 - accuracy: 0.6493\n",
      "Epoch 141: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.7937 - accuracy: 0.6485 - val_loss: 1.0474 - val_accuracy: 0.5321\n",
      "Epoch 142/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.8005 - accuracy: 0.6359\n",
      "Epoch 142: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8005 - accuracy: 0.6359 - val_loss: 1.0453 - val_accuracy: 0.5282\n",
      "Epoch 143/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.8200 - accuracy: 0.6368\n",
      "Epoch 143: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8193 - accuracy: 0.6372 - val_loss: 1.0214 - val_accuracy: 0.5397\n",
      "Epoch 144/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.8047 - accuracy: 0.6530\n",
      "Epoch 144: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7982 - accuracy: 0.6547 - val_loss: 0.9595 - val_accuracy: 0.5372\n",
      "Epoch 145/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.8030 - accuracy: 0.6509\n",
      "Epoch 145: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8015 - accuracy: 0.6521 - val_loss: 1.0505 - val_accuracy: 0.5295\n",
      "Epoch 146/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7979 - accuracy: 0.6394\n",
      "Epoch 146: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7974 - accuracy: 0.6398 - val_loss: 1.0769 - val_accuracy: 0.5192\n",
      "Epoch 147/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.8006 - accuracy: 0.6532\n",
      "Epoch 147: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7997 - accuracy: 0.6526 - val_loss: 1.0358 - val_accuracy: 0.5385\n",
      "Epoch 148/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.8051 - accuracy: 0.6388\n",
      "Epoch 148: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8050 - accuracy: 0.6390 - val_loss: 1.0025 - val_accuracy: 0.5423\n",
      "Epoch 149/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.8096 - accuracy: 0.6361\n",
      "Epoch 149: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8088 - accuracy: 0.6364 - val_loss: 1.0174 - val_accuracy: 0.5231\n",
      "Epoch 150/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.8007 - accuracy: 0.6517\n",
      "Epoch 150: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8002 - accuracy: 0.6518 - val_loss: 1.0707 - val_accuracy: 0.5026\n",
      "Epoch 151/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7919 - accuracy: 0.6428\n",
      "Epoch 151: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7906 - accuracy: 0.6434 - val_loss: 1.0593 - val_accuracy: 0.5231\n",
      "Epoch 152/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7921 - accuracy: 0.6436\n",
      "Epoch 152: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.7983 - accuracy: 0.6398 - val_loss: 0.9971 - val_accuracy: 0.5423\n",
      "Epoch 153/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7906 - accuracy: 0.6505\n",
      "Epoch 153: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7890 - accuracy: 0.6511 - val_loss: 1.0711 - val_accuracy: 0.5154\n",
      "Epoch 154/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.8116 - accuracy: 0.6353\n",
      "Epoch 154: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8084 - accuracy: 0.6359 - val_loss: 1.0122 - val_accuracy: 0.5462\n",
      "Epoch 155/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.7975 - accuracy: 0.6436\n",
      "Epoch 155: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7937 - accuracy: 0.6457 - val_loss: 1.0556 - val_accuracy: 0.5154\n",
      "Epoch 156/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.7867 - accuracy: 0.6529\n",
      "Epoch 156: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7867 - accuracy: 0.6529 - val_loss: 1.0285 - val_accuracy: 0.5308\n",
      "Epoch 157/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.7858 - accuracy: 0.6525\n",
      "Epoch 157: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7880 - accuracy: 0.6518 - val_loss: 1.0707 - val_accuracy: 0.5141\n",
      "Epoch 158/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7963 - accuracy: 0.6467\n",
      "Epoch 158: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.7981 - accuracy: 0.6472 - val_loss: 1.0407 - val_accuracy: 0.5231\n",
      "Epoch 159/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.7890 - accuracy: 0.6466\n",
      "Epoch 159: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7899 - accuracy: 0.6488 - val_loss: 1.0391 - val_accuracy: 0.5269\n",
      "Epoch 160/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.8008 - accuracy: 0.6445\n",
      "Epoch 160: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7994 - accuracy: 0.6454 - val_loss: 1.0565 - val_accuracy: 0.5026\n",
      "Epoch 161/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.7806 - accuracy: 0.6585\n",
      "Epoch 161: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7806 - accuracy: 0.6585 - val_loss: 1.0481 - val_accuracy: 0.5295\n",
      "Epoch 162/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7829 - accuracy: 0.6526\n",
      "Epoch 162: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7834 - accuracy: 0.6524 - val_loss: 1.0828 - val_accuracy: 0.5231\n",
      "Epoch 163/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7822 - accuracy: 0.6575\n",
      "Epoch 163: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7826 - accuracy: 0.6570 - val_loss: 0.9943 - val_accuracy: 0.5269\n",
      "Epoch 164/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7836 - accuracy: 0.6549\n",
      "Epoch 164: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7839 - accuracy: 0.6552 - val_loss: 1.0617 - val_accuracy: 0.5205\n",
      "Epoch 165/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.7929 - accuracy: 0.6465\n",
      "Epoch 165: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.7929 - accuracy: 0.6465 - val_loss: 0.9721 - val_accuracy: 0.5577\n",
      "Epoch 166/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.7850 - accuracy: 0.6498\n",
      "Epoch 166: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.7850 - accuracy: 0.6498 - val_loss: 1.0328 - val_accuracy: 0.5179\n",
      "Epoch 167/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.7900 - accuracy: 0.6493\n",
      "Epoch 167: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7881 - accuracy: 0.6508 - val_loss: 1.0271 - val_accuracy: 0.5321\n",
      "Epoch 168/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7864 - accuracy: 0.6511\n",
      "Epoch 168: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.7864 - accuracy: 0.6511 - val_loss: 1.1696 - val_accuracy: 0.4731\n",
      "Epoch 169/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7886 - accuracy: 0.6463\n",
      "Epoch 169: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.7876 - accuracy: 0.6480 - val_loss: 1.0436 - val_accuracy: 0.5359\n",
      "Epoch 170/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7906 - accuracy: 0.6521\n",
      "Epoch 170: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.7905 - accuracy: 0.6524 - val_loss: 1.0320 - val_accuracy: 0.5308\n",
      "Epoch 171/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.7786 - accuracy: 0.6579\n",
      "Epoch 171: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7833 - accuracy: 0.6560 - val_loss: 1.0350 - val_accuracy: 0.5295\n",
      "Epoch 172/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.7830 - accuracy: 0.6599\n",
      "Epoch 172: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7858 - accuracy: 0.6590 - val_loss: 0.9818 - val_accuracy: 0.5615\n",
      "Epoch 173/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7777 - accuracy: 0.6524\n",
      "Epoch 173: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7778 - accuracy: 0.6526 - val_loss: 1.0509 - val_accuracy: 0.5359\n",
      "Epoch 174/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7757 - accuracy: 0.6632\n",
      "Epoch 174: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7771 - accuracy: 0.6626 - val_loss: 1.0215 - val_accuracy: 0.5346\n",
      "Epoch 175/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7920 - accuracy: 0.6552\n",
      "Epoch 175: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7933 - accuracy: 0.6542 - val_loss: 1.0821 - val_accuracy: 0.5179\n",
      "Epoch 176/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7740 - accuracy: 0.6544\n",
      "Epoch 176: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7749 - accuracy: 0.6547 - val_loss: 1.0251 - val_accuracy: 0.5333\n",
      "Epoch 177/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7818 - accuracy: 0.6562\n",
      "Epoch 177: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7814 - accuracy: 0.6552 - val_loss: 1.1291 - val_accuracy: 0.5231\n",
      "Epoch 178/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.7758 - accuracy: 0.6647\n",
      "Epoch 178: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7758 - accuracy: 0.6647 - val_loss: 1.0183 - val_accuracy: 0.5308\n",
      "Epoch 179/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.7879 - accuracy: 0.6512\n",
      "Epoch 179: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.7867 - accuracy: 0.6518 - val_loss: 1.0598 - val_accuracy: 0.5051\n",
      "Epoch 180/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7751 - accuracy: 0.6544\n",
      "Epoch 180: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7757 - accuracy: 0.6547 - val_loss: 1.0018 - val_accuracy: 0.5308\n",
      "Epoch 181/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7781 - accuracy: 0.6619\n",
      "Epoch 181: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7773 - accuracy: 0.6614 - val_loss: 1.0196 - val_accuracy: 0.5551\n",
      "Epoch 182/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7766 - accuracy: 0.6578\n",
      "Epoch 182: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7746 - accuracy: 0.6596 - val_loss: 1.0684 - val_accuracy: 0.5141\n",
      "Epoch 183/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.7686 - accuracy: 0.6656\n",
      "Epoch 183: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7703 - accuracy: 0.6650 - val_loss: 1.0948 - val_accuracy: 0.4949\n",
      "Epoch 184/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7777 - accuracy: 0.6636\n",
      "Epoch 184: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7808 - accuracy: 0.6626 - val_loss: 1.0704 - val_accuracy: 0.5090\n",
      "Epoch 185/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7771 - accuracy: 0.6560\n",
      "Epoch 185: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7797 - accuracy: 0.6531 - val_loss: 1.0122 - val_accuracy: 0.5282\n",
      "Epoch 186/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7526 - accuracy: 0.6758\n",
      "Epoch 186: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7542 - accuracy: 0.6745 - val_loss: 1.0468 - val_accuracy: 0.5308\n",
      "Epoch 187/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.7814 - accuracy: 0.6510\n",
      "Epoch 187: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7792 - accuracy: 0.6529 - val_loss: 1.0410 - val_accuracy: 0.5333\n",
      "Epoch 188/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7666 - accuracy: 0.6520\n",
      "Epoch 188: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7690 - accuracy: 0.6495 - val_loss: 1.0365 - val_accuracy: 0.5590\n",
      "Epoch 189/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7675 - accuracy: 0.6626\n",
      "Epoch 189: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7694 - accuracy: 0.6614 - val_loss: 1.0710 - val_accuracy: 0.5013\n",
      "Epoch 190/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.7787 - accuracy: 0.6586\n",
      "Epoch 190: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7776 - accuracy: 0.6593 - val_loss: 0.9794 - val_accuracy: 0.5564\n",
      "Epoch 191/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7627 - accuracy: 0.6643\n",
      "Epoch 191: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7653 - accuracy: 0.6629 - val_loss: 1.0222 - val_accuracy: 0.5295\n",
      "Epoch 192/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.7604 - accuracy: 0.6680\n",
      "Epoch 192: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7604 - accuracy: 0.6680 - val_loss: 1.0866 - val_accuracy: 0.5205\n",
      "Epoch 193/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7706 - accuracy: 0.6606\n",
      "Epoch 193: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7752 - accuracy: 0.6593 - val_loss: 1.0162 - val_accuracy: 0.5346\n",
      "Epoch 194/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7661 - accuracy: 0.6459\n",
      "Epoch 194: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7635 - accuracy: 0.6483 - val_loss: 1.0263 - val_accuracy: 0.5359\n",
      "Epoch 195/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7621 - accuracy: 0.6643\n",
      "Epoch 195: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7613 - accuracy: 0.6650 - val_loss: 1.0737 - val_accuracy: 0.5141\n",
      "Epoch 196/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7666 - accuracy: 0.6544\n",
      "Epoch 196: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7649 - accuracy: 0.6547 - val_loss: 1.0764 - val_accuracy: 0.5321\n",
      "Epoch 197/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7583 - accuracy: 0.6647\n",
      "Epoch 197: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7624 - accuracy: 0.6626 - val_loss: 1.0084 - val_accuracy: 0.5359\n",
      "Epoch 198/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.7603 - accuracy: 0.6648\n",
      "Epoch 198: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7615 - accuracy: 0.6639 - val_loss: 1.0214 - val_accuracy: 0.5359\n",
      "Epoch 199/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.7758 - accuracy: 0.6592\n",
      "Epoch 199: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.7805 - accuracy: 0.6578 - val_loss: 1.0341 - val_accuracy: 0.5333\n",
      "Epoch 200/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.7646 - accuracy: 0.6598\n",
      "Epoch 200: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7630 - accuracy: 0.6629 - val_loss: 1.0737 - val_accuracy: 0.5179\n",
      "Epoch 201/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7675 - accuracy: 0.6484\n",
      "Epoch 201: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7691 - accuracy: 0.6483 - val_loss: 1.0852 - val_accuracy: 0.5179\n",
      "Epoch 202/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7609 - accuracy: 0.6665\n",
      "Epoch 202: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7589 - accuracy: 0.6662 - val_loss: 1.0081 - val_accuracy: 0.5474\n",
      "Epoch 203/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7532 - accuracy: 0.6663\n",
      "Epoch 203: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7536 - accuracy: 0.6665 - val_loss: 1.0288 - val_accuracy: 0.5410\n",
      "Epoch 204/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7653 - accuracy: 0.6719\n",
      "Epoch 204: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7662 - accuracy: 0.6709 - val_loss: 1.0405 - val_accuracy: 0.5359\n",
      "Epoch 205/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.7483 - accuracy: 0.6733\n",
      "Epoch 205: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7476 - accuracy: 0.6739 - val_loss: 1.0133 - val_accuracy: 0.5526\n",
      "Epoch 206/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.7583 - accuracy: 0.6711\n",
      "Epoch 206: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7583 - accuracy: 0.6711 - val_loss: 1.0603 - val_accuracy: 0.5205\n",
      "Epoch 207/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.7690 - accuracy: 0.6641\n",
      "Epoch 207: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7694 - accuracy: 0.6642 - val_loss: 1.0878 - val_accuracy: 0.4974\n",
      "Epoch 208/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7705 - accuracy: 0.6622\n",
      "Epoch 208: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7697 - accuracy: 0.6621 - val_loss: 1.0498 - val_accuracy: 0.5372\n",
      "Epoch 209/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.7604 - accuracy: 0.6665\n",
      "Epoch 209: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7604 - accuracy: 0.6665 - val_loss: 1.0010 - val_accuracy: 0.5500\n",
      "Epoch 210/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7677 - accuracy: 0.6671\n",
      "Epoch 210: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7688 - accuracy: 0.6662 - val_loss: 1.0143 - val_accuracy: 0.5269\n",
      "Epoch 211/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7648 - accuracy: 0.6695\n",
      "Epoch 211: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7640 - accuracy: 0.6709 - val_loss: 0.9899 - val_accuracy: 0.5269\n",
      "Epoch 212/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7492 - accuracy: 0.6723\n",
      "Epoch 212: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7503 - accuracy: 0.6719 - val_loss: 1.0845 - val_accuracy: 0.5244\n",
      "Epoch 213/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7456 - accuracy: 0.6727\n",
      "Epoch 213: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7459 - accuracy: 0.6706 - val_loss: 1.0180 - val_accuracy: 0.5487\n",
      "Epoch 214/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7613 - accuracy: 0.6748\n",
      "Epoch 214: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7643 - accuracy: 0.6737 - val_loss: 1.1187 - val_accuracy: 0.4949\n",
      "Epoch 215/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7680 - accuracy: 0.6611\n",
      "Epoch 215: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7702 - accuracy: 0.6624 - val_loss: 1.0353 - val_accuracy: 0.5436\n",
      "Epoch 216/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7560 - accuracy: 0.6723\n",
      "Epoch 216: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7569 - accuracy: 0.6696 - val_loss: 0.9905 - val_accuracy: 0.5538\n",
      "Epoch 217/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.7497 - accuracy: 0.6685\n",
      "Epoch 217: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7476 - accuracy: 0.6691 - val_loss: 1.0276 - val_accuracy: 0.5205\n",
      "Epoch 218/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7455 - accuracy: 0.6780\n",
      "Epoch 218: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7452 - accuracy: 0.6781 - val_loss: 1.0091 - val_accuracy: 0.5513\n",
      "Epoch 219/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7508 - accuracy: 0.6698\n",
      "Epoch 219: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7521 - accuracy: 0.6683 - val_loss: 0.9796 - val_accuracy: 0.5385\n",
      "Epoch 220/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7454 - accuracy: 0.6785\n",
      "Epoch 220: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7454 - accuracy: 0.6786 - val_loss: 1.0483 - val_accuracy: 0.5308\n",
      "Epoch 221/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7549 - accuracy: 0.6723\n",
      "Epoch 221: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7491 - accuracy: 0.6752 - val_loss: 1.0470 - val_accuracy: 0.5449\n",
      "Epoch 222/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7471 - accuracy: 0.6705\n",
      "Epoch 222: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7475 - accuracy: 0.6698 - val_loss: 1.0643 - val_accuracy: 0.5231\n",
      "Epoch 223/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7474 - accuracy: 0.6649\n",
      "Epoch 223: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7516 - accuracy: 0.6626 - val_loss: 1.1116 - val_accuracy: 0.5192\n",
      "Epoch 224/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.7571 - accuracy: 0.6648\n",
      "Epoch 224: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7587 - accuracy: 0.6632 - val_loss: 1.0467 - val_accuracy: 0.5231\n",
      "Epoch 225/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7555 - accuracy: 0.6663\n",
      "Epoch 225: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7581 - accuracy: 0.6642 - val_loss: 1.0568 - val_accuracy: 0.5244\n",
      "Epoch 226/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7540 - accuracy: 0.6725\n",
      "Epoch 226: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7543 - accuracy: 0.6721 - val_loss: 1.1434 - val_accuracy: 0.5000\n",
      "Epoch 227/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7431 - accuracy: 0.6690\n",
      "Epoch 227: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7481 - accuracy: 0.6678 - val_loss: 1.0398 - val_accuracy: 0.5192\n",
      "Epoch 228/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.7395 - accuracy: 0.6731\n",
      "Epoch 228: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7393 - accuracy: 0.6724 - val_loss: 1.0443 - val_accuracy: 0.5218\n",
      "Epoch 229/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7418 - accuracy: 0.6724\n",
      "Epoch 229: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7422 - accuracy: 0.6729 - val_loss: 1.0537 - val_accuracy: 0.5128\n",
      "Epoch 230/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.7437 - accuracy: 0.6743\n",
      "Epoch 230: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7462 - accuracy: 0.6742 - val_loss: 1.0886 - val_accuracy: 0.5282\n",
      "Epoch 231/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7486 - accuracy: 0.6723\n",
      "Epoch 231: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7483 - accuracy: 0.6729 - val_loss: 1.1306 - val_accuracy: 0.5000\n",
      "Epoch 232/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.7436 - accuracy: 0.6771\n",
      "Epoch 232: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7452 - accuracy: 0.6757 - val_loss: 1.0386 - val_accuracy: 0.5179\n",
      "Epoch 233/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7413 - accuracy: 0.6875\n",
      "Epoch 233: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7414 - accuracy: 0.6868 - val_loss: 1.0402 - val_accuracy: 0.5436\n",
      "Epoch 234/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7254 - accuracy: 0.6837\n",
      "Epoch 234: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7273 - accuracy: 0.6835 - val_loss: 1.0479 - val_accuracy: 0.5359\n",
      "Epoch 235/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7376 - accuracy: 0.6826\n",
      "Epoch 235: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7427 - accuracy: 0.6778 - val_loss: 1.0301 - val_accuracy: 0.5526\n",
      "Epoch 236/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7366 - accuracy: 0.6724\n",
      "Epoch 236: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7390 - accuracy: 0.6727 - val_loss: 1.1272 - val_accuracy: 0.4923\n",
      "Epoch 237/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7400 - accuracy: 0.6721\n",
      "Epoch 237: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7372 - accuracy: 0.6739 - val_loss: 1.0357 - val_accuracy: 0.5487\n",
      "Epoch 238/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7325 - accuracy: 0.6867\n",
      "Epoch 238: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7335 - accuracy: 0.6850 - val_loss: 1.0311 - val_accuracy: 0.5397\n",
      "Epoch 239/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7427 - accuracy: 0.6730\n",
      "Epoch 239: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7385 - accuracy: 0.6755 - val_loss: 1.1055 - val_accuracy: 0.5141\n",
      "Epoch 240/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7361 - accuracy: 0.6821\n",
      "Epoch 240: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7362 - accuracy: 0.6822 - val_loss: 1.0718 - val_accuracy: 0.5346\n",
      "Epoch 241/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7435 - accuracy: 0.6717\n",
      "Epoch 241: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7406 - accuracy: 0.6732 - val_loss: 1.0340 - val_accuracy: 0.5462\n",
      "Epoch 242/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.7371 - accuracy: 0.6804\n",
      "Epoch 242: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7331 - accuracy: 0.6819 - val_loss: 1.0646 - val_accuracy: 0.5269\n",
      "Epoch 243/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.7367 - accuracy: 0.6763\n",
      "Epoch 243: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7360 - accuracy: 0.6768 - val_loss: 1.0762 - val_accuracy: 0.5192\n",
      "Epoch 244/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7235 - accuracy: 0.6842\n",
      "Epoch 244: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7217 - accuracy: 0.6855 - val_loss: 1.0190 - val_accuracy: 0.5449\n",
      "Epoch 245/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7305 - accuracy: 0.6859\n",
      "Epoch 245: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7324 - accuracy: 0.6850 - val_loss: 1.0916 - val_accuracy: 0.5115\n",
      "Epoch 246/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7436 - accuracy: 0.6726\n",
      "Epoch 246: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7452 - accuracy: 0.6724 - val_loss: 1.0756 - val_accuracy: 0.5244\n",
      "Epoch 247/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.7327 - accuracy: 0.6781\n",
      "Epoch 247: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7320 - accuracy: 0.6786 - val_loss: 1.0874 - val_accuracy: 0.5244\n",
      "Epoch 248/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.7209 - accuracy: 0.6859\n",
      "Epoch 248: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7231 - accuracy: 0.6842 - val_loss: 1.0856 - val_accuracy: 0.5333\n",
      "Epoch 249/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.7358 - accuracy: 0.6773\n",
      "Epoch 249: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7362 - accuracy: 0.6770 - val_loss: 1.1036 - val_accuracy: 0.5192\n",
      "Epoch 250/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7329 - accuracy: 0.6821\n",
      "Epoch 250: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7329 - accuracy: 0.6811 - val_loss: 1.0339 - val_accuracy: 0.5436\n",
      "Epoch 251/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.7245 - accuracy: 0.6819\n",
      "Epoch 251: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7245 - accuracy: 0.6819 - val_loss: 1.0291 - val_accuracy: 0.5500\n",
      "Epoch 252/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.7290 - accuracy: 0.6831\n",
      "Epoch 252: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7297 - accuracy: 0.6827 - val_loss: 1.1113 - val_accuracy: 0.5154\n",
      "Epoch 253/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.7281 - accuracy: 0.6811\n",
      "Epoch 253: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7240 - accuracy: 0.6819 - val_loss: 1.0801 - val_accuracy: 0.5282\n",
      "Epoch 254/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7269 - accuracy: 0.6773\n",
      "Epoch 254: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7296 - accuracy: 0.6765 - val_loss: 1.0546 - val_accuracy: 0.5244\n",
      "Epoch 255/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.7183 - accuracy: 0.6862\n",
      "Epoch 255: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7199 - accuracy: 0.6850 - val_loss: 1.0659 - val_accuracy: 0.5359\n",
      "Epoch 256/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7313 - accuracy: 0.6814\n",
      "Epoch 256: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7288 - accuracy: 0.6822 - val_loss: 1.0416 - val_accuracy: 0.5449\n",
      "Epoch 257/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7387 - accuracy: 0.6750\n",
      "Epoch 257: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7370 - accuracy: 0.6755 - val_loss: 1.2234 - val_accuracy: 0.4744\n",
      "Epoch 258/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.7299 - accuracy: 0.6819\n",
      "Epoch 258: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7298 - accuracy: 0.6811 - val_loss: 1.0938 - val_accuracy: 0.5038\n",
      "Epoch 259/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7210 - accuracy: 0.6775\n",
      "Epoch 259: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7218 - accuracy: 0.6765 - val_loss: 1.0346 - val_accuracy: 0.5372\n",
      "Epoch 260/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7207 - accuracy: 0.6903\n",
      "Epoch 260: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7208 - accuracy: 0.6904 - val_loss: 1.0979 - val_accuracy: 0.5192\n",
      "Epoch 261/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7220 - accuracy: 0.6817\n",
      "Epoch 261: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7208 - accuracy: 0.6817 - val_loss: 1.0393 - val_accuracy: 0.5423\n",
      "Epoch 262/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7267 - accuracy: 0.6912\n",
      "Epoch 262: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7289 - accuracy: 0.6899 - val_loss: 1.0741 - val_accuracy: 0.5359\n",
      "Epoch 263/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7092 - accuracy: 0.6951\n",
      "Epoch 263: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7081 - accuracy: 0.6950 - val_loss: 1.0863 - val_accuracy: 0.5410\n",
      "Epoch 264/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.7318 - accuracy: 0.6799\n",
      "Epoch 264: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7327 - accuracy: 0.6804 - val_loss: 1.0967 - val_accuracy: 0.5192\n",
      "Epoch 265/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.7178 - accuracy: 0.6862\n",
      "Epoch 265: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7188 - accuracy: 0.6863 - val_loss: 1.1217 - val_accuracy: 0.5154\n",
      "Epoch 266/1000\n",
      "113/122 [==========================>...] - ETA: 0s - loss: 0.7237 - accuracy: 0.6814\n",
      "Epoch 266: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7210 - accuracy: 0.6835 - val_loss: 1.0777 - val_accuracy: 0.5474\n",
      "Epoch 267/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.7222 - accuracy: 0.6795\n",
      "Epoch 267: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7198 - accuracy: 0.6796 - val_loss: 1.0756 - val_accuracy: 0.5179\n",
      "Epoch 268/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7076 - accuracy: 0.6932\n",
      "Epoch 268: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7128 - accuracy: 0.6917 - val_loss: 1.0465 - val_accuracy: 0.5513\n",
      "Epoch 269/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7132 - accuracy: 0.6917\n",
      "Epoch 269: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7157 - accuracy: 0.6909 - val_loss: 1.0372 - val_accuracy: 0.5154\n",
      "Epoch 270/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7176 - accuracy: 0.6854\n",
      "Epoch 270: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7205 - accuracy: 0.6845 - val_loss: 1.0352 - val_accuracy: 0.5308\n",
      "Epoch 271/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.7187 - accuracy: 0.6846\n",
      "Epoch 271: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7163 - accuracy: 0.6850 - val_loss: 1.1331 - val_accuracy: 0.5064\n",
      "Epoch 272/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7068 - accuracy: 0.6875\n",
      "Epoch 272: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7074 - accuracy: 0.6871 - val_loss: 1.1174 - val_accuracy: 0.5051\n",
      "Epoch 273/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.7025 - accuracy: 0.6927\n",
      "Epoch 273: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7054 - accuracy: 0.6906 - val_loss: 1.0546 - val_accuracy: 0.5436\n",
      "Epoch 274/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.7168 - accuracy: 0.6846\n",
      "Epoch 274: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7167 - accuracy: 0.6847 - val_loss: 1.0470 - val_accuracy: 0.5333\n",
      "Epoch 275/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7252 - accuracy: 0.6815\n",
      "Epoch 275: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7305 - accuracy: 0.6781 - val_loss: 1.0580 - val_accuracy: 0.5308\n",
      "Epoch 276/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7167 - accuracy: 0.6840\n",
      "Epoch 276: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7162 - accuracy: 0.6837 - val_loss: 1.0616 - val_accuracy: 0.5321\n",
      "Epoch 277/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.7020 - accuracy: 0.6917\n",
      "Epoch 277: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7020 - accuracy: 0.6917 - val_loss: 1.0830 - val_accuracy: 0.5462\n",
      "Epoch 278/1000\n",
      "113/122 [==========================>...] - ETA: 0s - loss: 0.7021 - accuracy: 0.6900\n",
      "Epoch 278: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7015 - accuracy: 0.6899 - val_loss: 1.0843 - val_accuracy: 0.5128\n",
      "Epoch 279/1000\n",
      "113/122 [==========================>...] - ETA: 0s - loss: 0.7134 - accuracy: 0.6803\n",
      "Epoch 279: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7129 - accuracy: 0.6801 - val_loss: 1.0440 - val_accuracy: 0.5346\n",
      "Epoch 280/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7122 - accuracy: 0.6910\n",
      "Epoch 280: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7141 - accuracy: 0.6881 - val_loss: 1.0309 - val_accuracy: 0.5397\n",
      "Epoch 281/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.7069 - accuracy: 0.6930\n",
      "Epoch 281: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7058 - accuracy: 0.6935 - val_loss: 1.0745 - val_accuracy: 0.5372\n",
      "Epoch 282/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.7036 - accuracy: 0.6901\n",
      "Epoch 282: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7022 - accuracy: 0.6912 - val_loss: 1.0977 - val_accuracy: 0.5295\n",
      "Epoch 283/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6992 - accuracy: 0.6860\n",
      "Epoch 283: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6992 - accuracy: 0.6860 - val_loss: 1.0730 - val_accuracy: 0.5308\n",
      "Epoch 284/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.7110 - accuracy: 0.6839\n",
      "Epoch 284: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7133 - accuracy: 0.6827 - val_loss: 1.0864 - val_accuracy: 0.5154\n",
      "Epoch 285/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6945 - accuracy: 0.6983\n",
      "Epoch 285: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6958 - accuracy: 0.6981 - val_loss: 1.0503 - val_accuracy: 0.5179\n",
      "Epoch 286/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7042 - accuracy: 0.6918\n",
      "Epoch 286: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7076 - accuracy: 0.6888 - val_loss: 1.0683 - val_accuracy: 0.5321\n",
      "Epoch 287/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7223 - accuracy: 0.6832\n",
      "Epoch 287: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7183 - accuracy: 0.6860 - val_loss: 1.0566 - val_accuracy: 0.5359\n",
      "Epoch 288/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7113 - accuracy: 0.6856\n",
      "Epoch 288: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7107 - accuracy: 0.6858 - val_loss: 1.0967 - val_accuracy: 0.5244\n",
      "Epoch 289/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6921 - accuracy: 0.6973\n",
      "Epoch 289: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6942 - accuracy: 0.6963 - val_loss: 1.0519 - val_accuracy: 0.5282\n",
      "Epoch 290/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7074 - accuracy: 0.6933\n",
      "Epoch 290: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7058 - accuracy: 0.6940 - val_loss: 1.0649 - val_accuracy: 0.5397\n",
      "Epoch 291/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7118 - accuracy: 0.6886\n",
      "Epoch 291: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7083 - accuracy: 0.6888 - val_loss: 1.0581 - val_accuracy: 0.5333\n",
      "Epoch 292/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6896 - accuracy: 0.7029\n",
      "Epoch 292: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6910 - accuracy: 0.7022 - val_loss: 1.0782 - val_accuracy: 0.5372\n",
      "Epoch 293/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7082 - accuracy: 0.6898\n",
      "Epoch 293: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7074 - accuracy: 0.6906 - val_loss: 1.0953 - val_accuracy: 0.5244\n",
      "Epoch 294/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.7092 - accuracy: 0.6878\n",
      "Epoch 294: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7072 - accuracy: 0.6878 - val_loss: 1.0925 - val_accuracy: 0.5115\n",
      "Epoch 295/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7052 - accuracy: 0.6912\n",
      "Epoch 295: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7059 - accuracy: 0.6906 - val_loss: 1.0615 - val_accuracy: 0.5385\n",
      "Epoch 296/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6992 - accuracy: 0.6893\n",
      "Epoch 296: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6988 - accuracy: 0.6894 - val_loss: 1.0483 - val_accuracy: 0.5308\n",
      "Epoch 297/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6961 - accuracy: 0.6894\n",
      "Epoch 297: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7003 - accuracy: 0.6886 - val_loss: 1.0769 - val_accuracy: 0.5231\n",
      "Epoch 298/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.7038 - accuracy: 0.6923\n",
      "Epoch 298: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7011 - accuracy: 0.6942 - val_loss: 1.0710 - val_accuracy: 0.5462\n",
      "Epoch 299/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6974 - accuracy: 0.7030\n",
      "Epoch 299: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6963 - accuracy: 0.7035 - val_loss: 1.1156 - val_accuracy: 0.5154\n",
      "Epoch 300/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.6988 - accuracy: 0.6933\n",
      "Epoch 300: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6964 - accuracy: 0.6963 - val_loss: 1.0670 - val_accuracy: 0.5423\n",
      "Epoch 301/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7008 - accuracy: 0.6906\n",
      "Epoch 301: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7003 - accuracy: 0.6909 - val_loss: 1.1517 - val_accuracy: 0.4987\n",
      "Epoch 302/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7012 - accuracy: 0.6940\n",
      "Epoch 302: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7039 - accuracy: 0.6937 - val_loss: 1.0670 - val_accuracy: 0.5500\n",
      "Epoch 303/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6942 - accuracy: 0.6891\n",
      "Epoch 303: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6994 - accuracy: 0.6876 - val_loss: 1.0847 - val_accuracy: 0.5372\n",
      "Epoch 304/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6901 - accuracy: 0.7071\n",
      "Epoch 304: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6901 - accuracy: 0.7071 - val_loss: 1.1035 - val_accuracy: 0.5385\n",
      "Epoch 305/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.6995 - accuracy: 0.7018\n",
      "Epoch 305: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6987 - accuracy: 0.7022 - val_loss: 1.0875 - val_accuracy: 0.5333\n",
      "Epoch 306/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.7056 - accuracy: 0.6894\n",
      "Epoch 306: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7056 - accuracy: 0.6904 - val_loss: 1.1245 - val_accuracy: 0.5038\n",
      "Epoch 307/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6957 - accuracy: 0.6946\n",
      "Epoch 307: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6988 - accuracy: 0.6942 - val_loss: 1.0816 - val_accuracy: 0.5295\n",
      "Epoch 308/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7069 - accuracy: 0.6903\n",
      "Epoch 308: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7058 - accuracy: 0.6909 - val_loss: 1.1001 - val_accuracy: 0.5282\n",
      "Epoch 309/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6942 - accuracy: 0.6891\n",
      "Epoch 309: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6978 - accuracy: 0.6881 - val_loss: 1.1066 - val_accuracy: 0.5244\n",
      "Epoch 310/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6886 - accuracy: 0.6959\n",
      "Epoch 310: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.6896 - accuracy: 0.6950 - val_loss: 1.1370 - val_accuracy: 0.5244\n",
      "Epoch 311/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7008 - accuracy: 0.6894\n",
      "Epoch 311: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7019 - accuracy: 0.6881 - val_loss: 1.0476 - val_accuracy: 0.5474\n",
      "Epoch 312/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.6931\n",
      "Epoch 312: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6899 - accuracy: 0.6937 - val_loss: 1.0598 - val_accuracy: 0.5372\n",
      "Epoch 313/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.7127 - accuracy: 0.6894\n",
      "Epoch 313: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7079 - accuracy: 0.6909 - val_loss: 1.2050 - val_accuracy: 0.5013\n",
      "Epoch 314/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7055 - accuracy: 0.6918\n",
      "Epoch 314: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7029 - accuracy: 0.6922 - val_loss: 1.1465 - val_accuracy: 0.5000\n",
      "Epoch 315/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6821 - accuracy: 0.7014\n",
      "Epoch 315: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6832 - accuracy: 0.7009 - val_loss: 1.0587 - val_accuracy: 0.5449\n",
      "Epoch 316/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6942 - accuracy: 0.6945\n",
      "Epoch 316: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6952 - accuracy: 0.6942 - val_loss: 1.0983 - val_accuracy: 0.5487\n",
      "Epoch 317/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6905 - accuracy: 0.7011\n",
      "Epoch 317: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6925 - accuracy: 0.6986 - val_loss: 1.0917 - val_accuracy: 0.5423\n",
      "Epoch 318/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6821 - accuracy: 0.6986\n",
      "Epoch 318: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6821 - accuracy: 0.6986 - val_loss: 1.0436 - val_accuracy: 0.5500\n",
      "Epoch 319/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6946 - accuracy: 0.6994\n",
      "Epoch 319: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6946 - accuracy: 0.6994 - val_loss: 1.1274 - val_accuracy: 0.5218\n",
      "Epoch 320/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6911 - accuracy: 0.7001\n",
      "Epoch 320: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6895 - accuracy: 0.7002 - val_loss: 1.0526 - val_accuracy: 0.5282\n",
      "Epoch 321/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6879 - accuracy: 0.7027\n",
      "Epoch 321: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6879 - accuracy: 0.7027 - val_loss: 1.1256 - val_accuracy: 0.5295\n",
      "Epoch 322/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.6900 - accuracy: 0.6891\n",
      "Epoch 322: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6854 - accuracy: 0.6935 - val_loss: 1.1061 - val_accuracy: 0.5372\n",
      "Epoch 323/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6859 - accuracy: 0.6958\n",
      "Epoch 323: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6863 - accuracy: 0.6955 - val_loss: 1.0917 - val_accuracy: 0.5474\n",
      "Epoch 324/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6722 - accuracy: 0.7090\n",
      "Epoch 324: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6732 - accuracy: 0.7094 - val_loss: 1.1293 - val_accuracy: 0.5218\n",
      "Epoch 325/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6826 - accuracy: 0.7094\n",
      "Epoch 325: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6819 - accuracy: 0.7102 - val_loss: 1.1114 - val_accuracy: 0.5256\n",
      "Epoch 326/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6830 - accuracy: 0.6980\n",
      "Epoch 326: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6832 - accuracy: 0.6978 - val_loss: 1.1420 - val_accuracy: 0.5423\n",
      "Epoch 327/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6954 - accuracy: 0.6923\n",
      "Epoch 327: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6943 - accuracy: 0.6942 - val_loss: 1.0759 - val_accuracy: 0.5577\n",
      "Epoch 328/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6738 - accuracy: 0.7050\n",
      "Epoch 328: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6738 - accuracy: 0.7050 - val_loss: 1.0852 - val_accuracy: 0.5397\n",
      "Epoch 329/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6845 - accuracy: 0.7012\n",
      "Epoch 329: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6850 - accuracy: 0.7012 - val_loss: 1.0854 - val_accuracy: 0.5333\n",
      "Epoch 330/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.7013 - accuracy: 0.6899\n",
      "Epoch 330: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7024 - accuracy: 0.6888 - val_loss: 1.0921 - val_accuracy: 0.5295\n",
      "Epoch 331/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6832 - accuracy: 0.7078\n",
      "Epoch 331: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6841 - accuracy: 0.7066 - val_loss: 1.0783 - val_accuracy: 0.5474\n",
      "Epoch 332/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6631 - accuracy: 0.7091\n",
      "Epoch 332: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6629 - accuracy: 0.7104 - val_loss: 1.1114 - val_accuracy: 0.5295\n",
      "Epoch 333/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6843 - accuracy: 0.6977\n",
      "Epoch 333: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6823 - accuracy: 0.6991 - val_loss: 1.1379 - val_accuracy: 0.5038\n",
      "Epoch 334/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6627 - accuracy: 0.7092\n",
      "Epoch 334: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6636 - accuracy: 0.7094 - val_loss: 1.0963 - val_accuracy: 0.5256\n",
      "Epoch 335/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6777 - accuracy: 0.7030\n",
      "Epoch 335: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.6777 - accuracy: 0.7030 - val_loss: 1.1067 - val_accuracy: 0.5321\n",
      "Epoch 336/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6770 - accuracy: 0.7015\n",
      "Epoch 336: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6752 - accuracy: 0.7027 - val_loss: 1.1313 - val_accuracy: 0.5179\n",
      "Epoch 337/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6836 - accuracy: 0.7021\n",
      "Epoch 337: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6826 - accuracy: 0.7022 - val_loss: 1.1134 - val_accuracy: 0.5462\n",
      "Epoch 338/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.7038 - accuracy: 0.6836\n",
      "Epoch 338: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7029 - accuracy: 0.6840 - val_loss: 1.0840 - val_accuracy: 0.5641\n",
      "Epoch 339/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6802 - accuracy: 0.7046\n",
      "Epoch 339: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6784 - accuracy: 0.7058 - val_loss: 1.1205 - val_accuracy: 0.5295\n",
      "Epoch 340/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6782 - accuracy: 0.7048\n",
      "Epoch 340: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6765 - accuracy: 0.7061 - val_loss: 1.1189 - val_accuracy: 0.5321\n",
      "Epoch 341/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6760 - accuracy: 0.7085\n",
      "Epoch 341: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6783 - accuracy: 0.7081 - val_loss: 1.1171 - val_accuracy: 0.5321\n",
      "Epoch 342/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6714 - accuracy: 0.7012\n",
      "Epoch 342: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6714 - accuracy: 0.7012 - val_loss: 1.1905 - val_accuracy: 0.4923\n",
      "Epoch 343/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6766 - accuracy: 0.7076\n",
      "Epoch 343: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6766 - accuracy: 0.7076 - val_loss: 1.0903 - val_accuracy: 0.5487\n",
      "Epoch 344/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6794 - accuracy: 0.7034\n",
      "Epoch 344: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6807 - accuracy: 0.7030 - val_loss: 1.1062 - val_accuracy: 0.5513\n",
      "Epoch 345/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6880 - accuracy: 0.6923\n",
      "Epoch 345: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6860 - accuracy: 0.6932 - val_loss: 1.0963 - val_accuracy: 0.5282\n",
      "Epoch 346/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6798 - accuracy: 0.6959\n",
      "Epoch 346: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6760 - accuracy: 0.6971 - val_loss: 1.1098 - val_accuracy: 0.5205\n",
      "Epoch 347/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6724 - accuracy: 0.7033\n",
      "Epoch 347: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6714 - accuracy: 0.7063 - val_loss: 1.1672 - val_accuracy: 0.5077\n",
      "Epoch 348/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6620 - accuracy: 0.7048\n",
      "Epoch 348: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6617 - accuracy: 0.7058 - val_loss: 1.0852 - val_accuracy: 0.5423\n",
      "Epoch 349/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6625 - accuracy: 0.7096\n",
      "Epoch 349: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6612 - accuracy: 0.7099 - val_loss: 1.0801 - val_accuracy: 0.5462\n",
      "Epoch 350/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6716 - accuracy: 0.7131\n",
      "Epoch 350: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6751 - accuracy: 0.7115 - val_loss: 1.0927 - val_accuracy: 0.5474\n",
      "Epoch 351/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6751 - accuracy: 0.7037\n",
      "Epoch 351: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6743 - accuracy: 0.7045 - val_loss: 1.1499 - val_accuracy: 0.5385\n",
      "Epoch 352/1000\n",
      "113/122 [==========================>...] - ETA: 0s - loss: 0.6736 - accuracy: 0.7118\n",
      "Epoch 352: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6741 - accuracy: 0.7104 - val_loss: 1.2230 - val_accuracy: 0.4962\n",
      "Epoch 353/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6726 - accuracy: 0.6991\n",
      "Epoch 353: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6726 - accuracy: 0.6991 - val_loss: 1.1308 - val_accuracy: 0.5218\n",
      "Epoch 354/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6689 - accuracy: 0.7097\n",
      "Epoch 354: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6682 - accuracy: 0.7102 - val_loss: 1.1561 - val_accuracy: 0.5141\n",
      "Epoch 355/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6630 - accuracy: 0.7115\n",
      "Epoch 355: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6631 - accuracy: 0.7122 - val_loss: 1.1204 - val_accuracy: 0.5321\n",
      "Epoch 356/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.6688 - accuracy: 0.7075\n",
      "Epoch 356: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6736 - accuracy: 0.7063 - val_loss: 1.1031 - val_accuracy: 0.5359\n",
      "Epoch 357/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6619 - accuracy: 0.7085\n",
      "Epoch 357: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6603 - accuracy: 0.7094 - val_loss: 1.1246 - val_accuracy: 0.5308\n",
      "Epoch 358/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6574 - accuracy: 0.7190\n",
      "Epoch 358: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6570 - accuracy: 0.7197 - val_loss: 1.1104 - val_accuracy: 0.5513\n",
      "Epoch 359/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6573 - accuracy: 0.7071\n",
      "Epoch 359: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6626 - accuracy: 0.7066 - val_loss: 1.1442 - val_accuracy: 0.5103\n",
      "Epoch 360/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6499 - accuracy: 0.7185\n",
      "Epoch 360: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6494 - accuracy: 0.7192 - val_loss: 1.1823 - val_accuracy: 0.5064\n",
      "Epoch 361/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6638 - accuracy: 0.7136\n",
      "Epoch 361: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6652 - accuracy: 0.7109 - val_loss: 1.1192 - val_accuracy: 0.5346\n",
      "Epoch 362/1000\n",
      "113/122 [==========================>...] - ETA: 0s - loss: 0.6566 - accuracy: 0.7152\n",
      "Epoch 362: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6605 - accuracy: 0.7127 - val_loss: 1.1681 - val_accuracy: 0.5115\n",
      "Epoch 363/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6709 - accuracy: 0.6983\n",
      "Epoch 363: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6710 - accuracy: 0.6986 - val_loss: 1.1539 - val_accuracy: 0.5423\n",
      "Epoch 364/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6699 - accuracy: 0.7076\n",
      "Epoch 364: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6684 - accuracy: 0.7081 - val_loss: 1.1232 - val_accuracy: 0.5372\n",
      "Epoch 365/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6602 - accuracy: 0.7102\n",
      "Epoch 365: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6581 - accuracy: 0.7109 - val_loss: 1.1279 - val_accuracy: 0.5333\n",
      "Epoch 366/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6703 - accuracy: 0.7097\n",
      "Epoch 366: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6710 - accuracy: 0.7097 - val_loss: 1.1217 - val_accuracy: 0.5295\n",
      "Epoch 367/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6552 - accuracy: 0.7185\n",
      "Epoch 367: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6570 - accuracy: 0.7174 - val_loss: 1.1006 - val_accuracy: 0.5333\n",
      "Epoch 368/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6532 - accuracy: 0.7215\n",
      "Epoch 368: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6532 - accuracy: 0.7215 - val_loss: 1.1631 - val_accuracy: 0.5346\n",
      "Epoch 369/1000\n",
      "113/122 [==========================>...] - ETA: 0s - loss: 0.6557 - accuracy: 0.7163\n",
      "Epoch 369: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6530 - accuracy: 0.7176 - val_loss: 1.1157 - val_accuracy: 0.5551\n",
      "Epoch 370/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6718 - accuracy: 0.7120\n",
      "Epoch 370: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6712 - accuracy: 0.7122 - val_loss: 1.1158 - val_accuracy: 0.5462\n",
      "Epoch 371/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.6545 - accuracy: 0.7113\n",
      "Epoch 371: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6570 - accuracy: 0.7097 - val_loss: 1.1118 - val_accuracy: 0.5449\n",
      "Epoch 372/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6499 - accuracy: 0.7162\n",
      "Epoch 372: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6499 - accuracy: 0.7163 - val_loss: 1.1383 - val_accuracy: 0.5397\n",
      "Epoch 373/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6581 - accuracy: 0.7203\n",
      "Epoch 373: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6604 - accuracy: 0.7192 - val_loss: 1.1174 - val_accuracy: 0.5321\n",
      "Epoch 374/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6632 - accuracy: 0.7049\n",
      "Epoch 374: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6651 - accuracy: 0.7048 - val_loss: 1.1225 - val_accuracy: 0.5154\n",
      "Epoch 375/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6559 - accuracy: 0.7133\n",
      "Epoch 375: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6559 - accuracy: 0.7133 - val_loss: 1.1587 - val_accuracy: 0.5359\n",
      "Epoch 376/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6711 - accuracy: 0.7050\n",
      "Epoch 376: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6695 - accuracy: 0.7079 - val_loss: 1.0914 - val_accuracy: 0.5397\n",
      "Epoch 377/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.6593 - accuracy: 0.7149\n",
      "Epoch 377: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6620 - accuracy: 0.7135 - val_loss: 1.1354 - val_accuracy: 0.5269\n",
      "Epoch 378/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6617 - accuracy: 0.7071\n",
      "Epoch 378: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6603 - accuracy: 0.7086 - val_loss: 1.1076 - val_accuracy: 0.5500\n",
      "Epoch 379/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6633 - accuracy: 0.7011\n",
      "Epoch 379: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6641 - accuracy: 0.7014 - val_loss: 1.1002 - val_accuracy: 0.5474\n",
      "Epoch 380/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6596 - accuracy: 0.7127\n",
      "Epoch 380: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6580 - accuracy: 0.7133 - val_loss: 1.0985 - val_accuracy: 0.5718\n",
      "Epoch 381/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6410 - accuracy: 0.7266\n",
      "Epoch 381: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6423 - accuracy: 0.7271 - val_loss: 1.1782 - val_accuracy: 0.5128\n",
      "Epoch 382/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6578 - accuracy: 0.7123\n",
      "Epoch 382: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6587 - accuracy: 0.7120 - val_loss: 1.1292 - val_accuracy: 0.5244\n",
      "Epoch 383/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6621 - accuracy: 0.7105\n",
      "Epoch 383: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6607 - accuracy: 0.7115 - val_loss: 1.1231 - val_accuracy: 0.5321\n",
      "Epoch 384/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6597 - accuracy: 0.7171\n",
      "Epoch 384: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6597 - accuracy: 0.7171 - val_loss: 1.1780 - val_accuracy: 0.5128\n",
      "Epoch 385/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6547 - accuracy: 0.7171\n",
      "Epoch 385: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6547 - accuracy: 0.7171 - val_loss: 1.1204 - val_accuracy: 0.5295\n",
      "Epoch 386/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.6361 - accuracy: 0.7231\n",
      "Epoch 386: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6376 - accuracy: 0.7220 - val_loss: 1.1109 - val_accuracy: 0.5538\n",
      "Epoch 387/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6503 - accuracy: 0.7180\n",
      "Epoch 387: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6503 - accuracy: 0.7184 - val_loss: 1.1087 - val_accuracy: 0.5359\n",
      "Epoch 388/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6580 - accuracy: 0.7110\n",
      "Epoch 388: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6576 - accuracy: 0.7112 - val_loss: 1.1927 - val_accuracy: 0.5051\n",
      "Epoch 389/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6483 - accuracy: 0.7169\n",
      "Epoch 389: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6474 - accuracy: 0.7194 - val_loss: 1.1240 - val_accuracy: 0.5449\n",
      "Epoch 390/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6403 - accuracy: 0.7172\n",
      "Epoch 390: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6395 - accuracy: 0.7174 - val_loss: 1.1548 - val_accuracy: 0.5423\n",
      "Epoch 391/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6453 - accuracy: 0.7218\n",
      "Epoch 391: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6453 - accuracy: 0.7215 - val_loss: 1.1346 - val_accuracy: 0.5474\n",
      "Epoch 392/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6542 - accuracy: 0.7167\n",
      "Epoch 392: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6537 - accuracy: 0.7166 - val_loss: 1.0839 - val_accuracy: 0.5538\n",
      "Epoch 393/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6477 - accuracy: 0.7255\n",
      "Epoch 393: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6478 - accuracy: 0.7251 - val_loss: 1.1229 - val_accuracy: 0.5359\n",
      "Epoch 394/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6504 - accuracy: 0.7147\n",
      "Epoch 394: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6437 - accuracy: 0.7179 - val_loss: 1.1511 - val_accuracy: 0.5462\n",
      "Epoch 395/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6373 - accuracy: 0.7231\n",
      "Epoch 395: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6395 - accuracy: 0.7220 - val_loss: 1.1890 - val_accuracy: 0.5026\n",
      "Epoch 396/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6438 - accuracy: 0.7185\n",
      "Epoch 396: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6441 - accuracy: 0.7187 - val_loss: 1.1369 - val_accuracy: 0.5436\n",
      "Epoch 397/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6550 - accuracy: 0.7138\n",
      "Epoch 397: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6514 - accuracy: 0.7148 - val_loss: 1.1488 - val_accuracy: 0.5346\n",
      "Epoch 398/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6457 - accuracy: 0.7209\n",
      "Epoch 398: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6452 - accuracy: 0.7207 - val_loss: 1.1896 - val_accuracy: 0.5167\n",
      "Epoch 399/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6333 - accuracy: 0.7255\n",
      "Epoch 399: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6343 - accuracy: 0.7253 - val_loss: 1.1262 - val_accuracy: 0.5449\n",
      "Epoch 400/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6674 - accuracy: 0.7099\n",
      "Epoch 400: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6647 - accuracy: 0.7117 - val_loss: 1.1572 - val_accuracy: 0.5231\n",
      "Epoch 401/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6402 - accuracy: 0.7256\n",
      "Epoch 401: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6434 - accuracy: 0.7246 - val_loss: 1.1612 - val_accuracy: 0.5179\n",
      "Epoch 402/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6309 - accuracy: 0.7304\n",
      "Epoch 402: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6328 - accuracy: 0.7297 - val_loss: 1.1512 - val_accuracy: 0.5423\n",
      "Epoch 403/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6403 - accuracy: 0.7258\n",
      "Epoch 403: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6403 - accuracy: 0.7258 - val_loss: 1.1195 - val_accuracy: 0.5372\n",
      "Epoch 404/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6410 - accuracy: 0.7248\n",
      "Epoch 404: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6425 - accuracy: 0.7240 - val_loss: 1.2200 - val_accuracy: 0.5256\n",
      "Epoch 405/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6489 - accuracy: 0.7240\n",
      "Epoch 405: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6489 - accuracy: 0.7240 - val_loss: 1.1638 - val_accuracy: 0.5179\n",
      "Epoch 406/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6502 - accuracy: 0.7142\n",
      "Epoch 406: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6517 - accuracy: 0.7120 - val_loss: 1.1087 - val_accuracy: 0.5628\n",
      "Epoch 407/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6431 - accuracy: 0.7203\n",
      "Epoch 407: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6427 - accuracy: 0.7207 - val_loss: 1.1657 - val_accuracy: 0.5256\n",
      "Epoch 408/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6348 - accuracy: 0.7219\n",
      "Epoch 408: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6359 - accuracy: 0.7212 - val_loss: 1.1739 - val_accuracy: 0.5218\n",
      "Epoch 409/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.6380 - accuracy: 0.7204\n",
      "Epoch 409: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6395 - accuracy: 0.7210 - val_loss: 1.1836 - val_accuracy: 0.5231\n",
      "Epoch 410/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6400 - accuracy: 0.7258\n",
      "Epoch 410: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6406 - accuracy: 0.7253 - val_loss: 1.1589 - val_accuracy: 0.5308\n",
      "Epoch 411/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.6441 - accuracy: 0.7182\n",
      "Epoch 411: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6460 - accuracy: 0.7184 - val_loss: 1.1330 - val_accuracy: 0.5436\n",
      "Epoch 412/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6492 - accuracy: 0.7161\n",
      "Epoch 412: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6481 - accuracy: 0.7163 - val_loss: 1.1701 - val_accuracy: 0.5192\n",
      "Epoch 413/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.6495 - accuracy: 0.7193\n",
      "Epoch 413: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6443 - accuracy: 0.7220 - val_loss: 1.1979 - val_accuracy: 0.5167\n",
      "Epoch 414/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6386 - accuracy: 0.7271\n",
      "Epoch 414: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6386 - accuracy: 0.7261 - val_loss: 1.1823 - val_accuracy: 0.5269\n",
      "Epoch 415/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6248 - accuracy: 0.7312\n",
      "Epoch 415: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6277 - accuracy: 0.7315 - val_loss: 1.1162 - val_accuracy: 0.5333\n",
      "Epoch 416/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6166 - accuracy: 0.7379\n",
      "Epoch 416: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6146 - accuracy: 0.7395 - val_loss: 1.1689 - val_accuracy: 0.5436\n",
      "Epoch 417/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6539 - accuracy: 0.7128\n",
      "Epoch 417: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6541 - accuracy: 0.7117 - val_loss: 1.1564 - val_accuracy: 0.5346\n",
      "Epoch 418/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6491 - accuracy: 0.7161\n",
      "Epoch 418: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6483 - accuracy: 0.7169 - val_loss: 1.2468 - val_accuracy: 0.4923\n",
      "Epoch 419/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6306 - accuracy: 0.7279\n",
      "Epoch 419: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6306 - accuracy: 0.7279 - val_loss: 1.1413 - val_accuracy: 0.5500\n",
      "Epoch 420/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6273 - accuracy: 0.7269\n",
      "Epoch 420: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6273 - accuracy: 0.7269 - val_loss: 1.1870 - val_accuracy: 0.5167\n",
      "Epoch 421/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6340 - accuracy: 0.7198\n",
      "Epoch 421: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6344 - accuracy: 0.7199 - val_loss: 1.1225 - val_accuracy: 0.5500\n",
      "Epoch 422/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6382 - accuracy: 0.7258\n",
      "Epoch 422: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6366 - accuracy: 0.7261 - val_loss: 1.1070 - val_accuracy: 0.5308\n",
      "Epoch 423/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6253 - accuracy: 0.7266\n",
      "Epoch 423: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6304 - accuracy: 0.7253 - val_loss: 1.1427 - val_accuracy: 0.5282\n",
      "Epoch 424/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6336 - accuracy: 0.7216\n",
      "Epoch 424: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6338 - accuracy: 0.7217 - val_loss: 1.1769 - val_accuracy: 0.5346\n",
      "Epoch 425/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6264 - accuracy: 0.7304\n",
      "Epoch 425: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6262 - accuracy: 0.7305 - val_loss: 1.1298 - val_accuracy: 0.5410\n",
      "Epoch 426/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6369 - accuracy: 0.7276\n",
      "Epoch 426: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6344 - accuracy: 0.7287 - val_loss: 1.1413 - val_accuracy: 0.5385\n",
      "Epoch 427/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6177 - accuracy: 0.7327\n",
      "Epoch 427: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6181 - accuracy: 0.7330 - val_loss: 1.1748 - val_accuracy: 0.5218\n",
      "Epoch 428/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6366 - accuracy: 0.7256\n",
      "Epoch 428: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6398 - accuracy: 0.7240 - val_loss: 1.1761 - val_accuracy: 0.5192\n",
      "Epoch 429/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6325 - accuracy: 0.7284\n",
      "Epoch 429: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6355 - accuracy: 0.7269 - val_loss: 1.1806 - val_accuracy: 0.5590\n",
      "Epoch 430/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6322 - accuracy: 0.7249\n",
      "Epoch 430: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6279 - accuracy: 0.7269 - val_loss: 1.1587 - val_accuracy: 0.5359\n",
      "Epoch 431/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6317 - accuracy: 0.7280\n",
      "Epoch 431: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6314 - accuracy: 0.7282 - val_loss: 1.1552 - val_accuracy: 0.5321\n",
      "Epoch 432/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6430 - accuracy: 0.7219\n",
      "Epoch 432: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6441 - accuracy: 0.7205 - val_loss: 1.1397 - val_accuracy: 0.5564\n",
      "Epoch 433/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6373 - accuracy: 0.7201\n",
      "Epoch 433: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6347 - accuracy: 0.7220 - val_loss: 1.1714 - val_accuracy: 0.5154\n",
      "Epoch 434/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6376 - accuracy: 0.7206\n",
      "Epoch 434: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6383 - accuracy: 0.7205 - val_loss: 1.1614 - val_accuracy: 0.5372\n",
      "Epoch 435/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6292 - accuracy: 0.7339\n",
      "Epoch 435: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6283 - accuracy: 0.7343 - val_loss: 1.1580 - val_accuracy: 0.5487\n",
      "Epoch 436/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6230 - accuracy: 0.7283\n",
      "Epoch 436: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6223 - accuracy: 0.7289 - val_loss: 1.1614 - val_accuracy: 0.5295\n",
      "Epoch 437/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6414 - accuracy: 0.7140\n",
      "Epoch 437: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6423 - accuracy: 0.7133 - val_loss: 1.1662 - val_accuracy: 0.5321\n",
      "Epoch 438/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6336 - accuracy: 0.7255\n",
      "Epoch 438: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6341 - accuracy: 0.7253 - val_loss: 1.1821 - val_accuracy: 0.5282\n",
      "Epoch 439/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6272 - accuracy: 0.7231\n",
      "Epoch 439: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6285 - accuracy: 0.7243 - val_loss: 1.1663 - val_accuracy: 0.5500\n",
      "Epoch 440/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6200 - accuracy: 0.7362\n",
      "Epoch 440: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6192 - accuracy: 0.7356 - val_loss: 1.2140 - val_accuracy: 0.5077\n",
      "Epoch 441/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6246 - accuracy: 0.7248\n",
      "Epoch 441: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6209 - accuracy: 0.7271 - val_loss: 1.2642 - val_accuracy: 0.5179\n",
      "Epoch 442/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6339 - accuracy: 0.7232\n",
      "Epoch 442: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6365 - accuracy: 0.7220 - val_loss: 1.1868 - val_accuracy: 0.5179\n",
      "Epoch 443/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6345 - accuracy: 0.7271\n",
      "Epoch 443: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6348 - accuracy: 0.7269 - val_loss: 1.1601 - val_accuracy: 0.5513\n",
      "Epoch 444/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6197 - accuracy: 0.7356\n",
      "Epoch 444: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6184 - accuracy: 0.7356 - val_loss: 1.2214 - val_accuracy: 0.5115\n",
      "Epoch 445/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6404 - accuracy: 0.7219\n",
      "Epoch 445: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6382 - accuracy: 0.7228 - val_loss: 1.1976 - val_accuracy: 0.5231\n",
      "Epoch 446/1000\n",
      "113/122 [==========================>...] - ETA: 0s - loss: 0.6397 - accuracy: 0.7185\n",
      "Epoch 446: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6409 - accuracy: 0.7194 - val_loss: 1.2317 - val_accuracy: 0.5205\n",
      "Epoch 447/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6272 - accuracy: 0.7279\n",
      "Epoch 447: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6308 - accuracy: 0.7248 - val_loss: 1.2445 - val_accuracy: 0.5192\n",
      "Epoch 448/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.6153 - accuracy: 0.7283\n",
      "Epoch 448: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6166 - accuracy: 0.7292 - val_loss: 1.2447 - val_accuracy: 0.5051\n",
      "Epoch 449/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6325 - accuracy: 0.7220\n",
      "Epoch 449: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6293 - accuracy: 0.7233 - val_loss: 1.1702 - val_accuracy: 0.5154\n",
      "Epoch 450/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6324 - accuracy: 0.7253\n",
      "Epoch 450: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6277 - accuracy: 0.7276 - val_loss: 1.1937 - val_accuracy: 0.5385\n",
      "Epoch 451/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6339 - accuracy: 0.7219\n",
      "Epoch 451: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6332 - accuracy: 0.7217 - val_loss: 1.1623 - val_accuracy: 0.5462\n",
      "Epoch 452/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6317 - accuracy: 0.7227\n",
      "Epoch 452: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6311 - accuracy: 0.7235 - val_loss: 1.1830 - val_accuracy: 0.5436\n",
      "Epoch 453/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6085 - accuracy: 0.7368\n",
      "Epoch 453: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6148 - accuracy: 0.7348 - val_loss: 1.1808 - val_accuracy: 0.5205\n",
      "Epoch 454/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6272 - accuracy: 0.7312\n",
      "Epoch 454: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6274 - accuracy: 0.7302 - val_loss: 1.1604 - val_accuracy: 0.5487\n",
      "Epoch 455/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6170 - accuracy: 0.7281\n",
      "Epoch 455: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6170 - accuracy: 0.7292 - val_loss: 1.2041 - val_accuracy: 0.5385\n",
      "Epoch 456/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6258 - accuracy: 0.7343\n",
      "Epoch 456: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6258 - accuracy: 0.7343 - val_loss: 1.2040 - val_accuracy: 0.5423\n",
      "Epoch 457/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6239 - accuracy: 0.7249\n",
      "Epoch 457: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6273 - accuracy: 0.7261 - val_loss: 1.1462 - val_accuracy: 0.5333\n",
      "Epoch 458/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6078 - accuracy: 0.7452\n",
      "Epoch 458: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6102 - accuracy: 0.7443 - val_loss: 1.1915 - val_accuracy: 0.5513\n",
      "Epoch 459/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6225 - accuracy: 0.7234\n",
      "Epoch 459: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6251 - accuracy: 0.7235 - val_loss: 1.1654 - val_accuracy: 0.5282\n",
      "Epoch 460/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6081 - accuracy: 0.7266\n",
      "Epoch 460: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6081 - accuracy: 0.7279 - val_loss: 1.1853 - val_accuracy: 0.5462\n",
      "Epoch 461/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6187 - accuracy: 0.7300\n",
      "Epoch 461: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6162 - accuracy: 0.7323 - val_loss: 1.2184 - val_accuracy: 0.5333\n",
      "Epoch 462/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.6212 - accuracy: 0.7327\n",
      "Epoch 462: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6188 - accuracy: 0.7333 - val_loss: 1.2290 - val_accuracy: 0.5423\n",
      "Epoch 463/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6160 - accuracy: 0.7337\n",
      "Epoch 463: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6149 - accuracy: 0.7348 - val_loss: 1.1962 - val_accuracy: 0.5410\n",
      "Epoch 464/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6114 - accuracy: 0.7348\n",
      "Epoch 464: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6112 - accuracy: 0.7351 - val_loss: 1.2333 - val_accuracy: 0.5282\n",
      "Epoch 465/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6211 - accuracy: 0.7352\n",
      "Epoch 465: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6175 - accuracy: 0.7374 - val_loss: 1.2023 - val_accuracy: 0.5397\n",
      "Epoch 466/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6297 - accuracy: 0.7231\n",
      "Epoch 466: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6257 - accuracy: 0.7243 - val_loss: 1.2631 - val_accuracy: 0.5167\n",
      "Epoch 467/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6068 - accuracy: 0.7329\n",
      "Epoch 467: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6098 - accuracy: 0.7325 - val_loss: 1.1822 - val_accuracy: 0.5397\n",
      "Epoch 468/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6177 - accuracy: 0.7300\n",
      "Epoch 468: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6181 - accuracy: 0.7312 - val_loss: 1.1758 - val_accuracy: 0.5308\n",
      "Epoch 469/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6189 - accuracy: 0.7313\n",
      "Epoch 469: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6172 - accuracy: 0.7318 - val_loss: 1.2960 - val_accuracy: 0.4974\n",
      "Epoch 470/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6137 - accuracy: 0.7374\n",
      "Epoch 470: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6110 - accuracy: 0.7390 - val_loss: 1.2012 - val_accuracy: 0.5295\n",
      "Epoch 471/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6143 - accuracy: 0.7275\n",
      "Epoch 471: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6136 - accuracy: 0.7279 - val_loss: 1.2221 - val_accuracy: 0.5269\n",
      "Epoch 472/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6030 - accuracy: 0.7433\n",
      "Epoch 472: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6052 - accuracy: 0.7420 - val_loss: 1.2035 - val_accuracy: 0.5359\n",
      "Epoch 473/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6032 - accuracy: 0.7407\n",
      "Epoch 473: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6079 - accuracy: 0.7395 - val_loss: 1.2213 - val_accuracy: 0.5385\n",
      "Epoch 474/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6034 - accuracy: 0.7354\n",
      "Epoch 474: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6040 - accuracy: 0.7343 - val_loss: 1.2092 - val_accuracy: 0.5385\n",
      "Epoch 475/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5995 - accuracy: 0.7404\n",
      "Epoch 475: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 2s 13ms/step - loss: 0.6012 - accuracy: 0.7387 - val_loss: 1.2379 - val_accuracy: 0.5077\n",
      "Epoch 476/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6174 - accuracy: 0.7346\n",
      "Epoch 476: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6200 - accuracy: 0.7328 - val_loss: 1.2256 - val_accuracy: 0.5295\n",
      "Epoch 477/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6059 - accuracy: 0.7388\n",
      "Epoch 477: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6045 - accuracy: 0.7382 - val_loss: 1.2054 - val_accuracy: 0.5538\n",
      "Epoch 478/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6029 - accuracy: 0.7342\n",
      "Epoch 478: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6023 - accuracy: 0.7359 - val_loss: 1.2714 - val_accuracy: 0.5282\n",
      "Epoch 479/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6192 - accuracy: 0.7226\n",
      "Epoch 479: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6196 - accuracy: 0.7225 - val_loss: 1.2613 - val_accuracy: 0.5154\n",
      "Epoch 480/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5983 - accuracy: 0.7374\n",
      "Epoch 480: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5997 - accuracy: 0.7359 - val_loss: 1.2105 - val_accuracy: 0.5192\n",
      "Epoch 481/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6146 - accuracy: 0.7301\n",
      "Epoch 481: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6155 - accuracy: 0.7297 - val_loss: 1.2021 - val_accuracy: 0.5551\n",
      "Epoch 482/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6158 - accuracy: 0.7287\n",
      "Epoch 482: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6125 - accuracy: 0.7305 - val_loss: 1.2490 - val_accuracy: 0.5321\n",
      "Epoch 483/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6148 - accuracy: 0.7363\n",
      "Epoch 483: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6172 - accuracy: 0.7348 - val_loss: 1.1504 - val_accuracy: 0.5462\n",
      "Epoch 484/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6225 - accuracy: 0.7274\n",
      "Epoch 484: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6225 - accuracy: 0.7274 - val_loss: 1.3255 - val_accuracy: 0.4859\n",
      "Epoch 485/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6218 - accuracy: 0.7299\n",
      "Epoch 485: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6253 - accuracy: 0.7292 - val_loss: 1.2320 - val_accuracy: 0.5179\n",
      "Epoch 486/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6166 - accuracy: 0.7368\n",
      "Epoch 486: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6150 - accuracy: 0.7374 - val_loss: 1.2844 - val_accuracy: 0.5231\n",
      "Epoch 487/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6060 - accuracy: 0.7399\n",
      "Epoch 487: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.6053 - accuracy: 0.7400 - val_loss: 1.2377 - val_accuracy: 0.5256\n",
      "Epoch 488/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6037 - accuracy: 0.7401\n",
      "Epoch 488: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.6033 - accuracy: 0.7405 - val_loss: 1.2176 - val_accuracy: 0.5244\n",
      "Epoch 489/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6203 - accuracy: 0.7345\n",
      "Epoch 489: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6194 - accuracy: 0.7351 - val_loss: 1.2828 - val_accuracy: 0.5154\n",
      "Epoch 490/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6085 - accuracy: 0.7386\n",
      "Epoch 490: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6129 - accuracy: 0.7364 - val_loss: 1.1858 - val_accuracy: 0.5526\n",
      "Epoch 491/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6103 - accuracy: 0.7330\n",
      "Epoch 491: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6102 - accuracy: 0.7341 - val_loss: 1.2348 - val_accuracy: 0.5244\n",
      "Epoch 492/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6256 - accuracy: 0.7279\n",
      "Epoch 492: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6257 - accuracy: 0.7282 - val_loss: 1.1893 - val_accuracy: 0.5462\n",
      "Epoch 493/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6167 - accuracy: 0.7252\n",
      "Epoch 493: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6157 - accuracy: 0.7261 - val_loss: 1.2042 - val_accuracy: 0.5269\n",
      "Epoch 494/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6159 - accuracy: 0.7411\n",
      "Epoch 494: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.6166 - accuracy: 0.7402 - val_loss: 1.2207 - val_accuracy: 0.5321\n",
      "Epoch 495/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5951 - accuracy: 0.7455\n",
      "Epoch 495: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5964 - accuracy: 0.7446 - val_loss: 1.2202 - val_accuracy: 0.5436\n",
      "Epoch 496/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6148 - accuracy: 0.7361\n",
      "Epoch 496: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6139 - accuracy: 0.7372 - val_loss: 1.1781 - val_accuracy: 0.5256\n",
      "Epoch 497/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6120 - accuracy: 0.7383\n",
      "Epoch 497: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6108 - accuracy: 0.7395 - val_loss: 1.2211 - val_accuracy: 0.5346\n",
      "Epoch 498/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6128 - accuracy: 0.7323\n",
      "Epoch 498: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6124 - accuracy: 0.7328 - val_loss: 1.2180 - val_accuracy: 0.5333\n",
      "Epoch 499/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5980 - accuracy: 0.7388\n",
      "Epoch 499: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5994 - accuracy: 0.7382 - val_loss: 1.1753 - val_accuracy: 0.5474\n",
      "Epoch 500/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6026 - accuracy: 0.7443\n",
      "Epoch 500: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6026 - accuracy: 0.7443 - val_loss: 1.2847 - val_accuracy: 0.5295\n",
      "Epoch 501/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6138 - accuracy: 0.7273\n",
      "Epoch 501: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6170 - accuracy: 0.7261 - val_loss: 1.2243 - val_accuracy: 0.5333\n",
      "Epoch 502/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5980 - accuracy: 0.7487\n",
      "Epoch 502: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5980 - accuracy: 0.7487 - val_loss: 1.2644 - val_accuracy: 0.5154\n",
      "Epoch 503/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6000 - accuracy: 0.7471\n",
      "Epoch 503: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6013 - accuracy: 0.7469 - val_loss: 1.2938 - val_accuracy: 0.5103\n",
      "Epoch 504/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6079 - accuracy: 0.7387\n",
      "Epoch 504: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6079 - accuracy: 0.7387 - val_loss: 1.1801 - val_accuracy: 0.5474\n",
      "Epoch 505/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5950 - accuracy: 0.7446\n",
      "Epoch 505: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5957 - accuracy: 0.7443 - val_loss: 1.2299 - val_accuracy: 0.5346\n",
      "Epoch 506/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6025 - accuracy: 0.7365\n",
      "Epoch 506: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6014 - accuracy: 0.7372 - val_loss: 1.2042 - val_accuracy: 0.5346\n",
      "Epoch 507/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6015 - accuracy: 0.7346\n",
      "Epoch 507: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5995 - accuracy: 0.7359 - val_loss: 1.1694 - val_accuracy: 0.5333\n",
      "Epoch 508/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5986 - accuracy: 0.7396\n",
      "Epoch 508: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6003 - accuracy: 0.7392 - val_loss: 1.2280 - val_accuracy: 0.5064\n",
      "Epoch 509/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6201 - accuracy: 0.7283\n",
      "Epoch 509: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6171 - accuracy: 0.7310 - val_loss: 1.2498 - val_accuracy: 0.5244\n",
      "Epoch 510/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6072 - accuracy: 0.7412\n",
      "Epoch 510: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6077 - accuracy: 0.7418 - val_loss: 1.2699 - val_accuracy: 0.5167\n",
      "Epoch 511/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6041 - accuracy: 0.7443\n",
      "Epoch 511: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6041 - accuracy: 0.7443 - val_loss: 1.2533 - val_accuracy: 0.5269\n",
      "Epoch 512/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5997 - accuracy: 0.7404\n",
      "Epoch 512: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5986 - accuracy: 0.7400 - val_loss: 1.1932 - val_accuracy: 0.5372\n",
      "Epoch 513/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.6020 - accuracy: 0.7357\n",
      "Epoch 513: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6039 - accuracy: 0.7364 - val_loss: 1.2012 - val_accuracy: 0.5346\n",
      "Epoch 514/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5927 - accuracy: 0.7449\n",
      "Epoch 514: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5908 - accuracy: 0.7449 - val_loss: 1.2138 - val_accuracy: 0.5500\n",
      "Epoch 515/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5989 - accuracy: 0.7380\n",
      "Epoch 515: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6000 - accuracy: 0.7369 - val_loss: 1.2496 - val_accuracy: 0.5295\n",
      "Epoch 516/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5929 - accuracy: 0.7465\n",
      "Epoch 516: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5994 - accuracy: 0.7415 - val_loss: 1.1825 - val_accuracy: 0.5436\n",
      "Epoch 517/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6118 - accuracy: 0.7329\n",
      "Epoch 517: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6162 - accuracy: 0.7294 - val_loss: 1.1827 - val_accuracy: 0.5308\n",
      "Epoch 518/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6014 - accuracy: 0.7408\n",
      "Epoch 518: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6066 - accuracy: 0.7402 - val_loss: 1.2527 - val_accuracy: 0.5308\n",
      "Epoch 519/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.6043 - accuracy: 0.7384\n",
      "Epoch 519: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6043 - accuracy: 0.7384 - val_loss: 1.1998 - val_accuracy: 0.5269\n",
      "Epoch 520/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5981 - accuracy: 0.7438\n",
      "Epoch 520: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5988 - accuracy: 0.7431 - val_loss: 1.1985 - val_accuracy: 0.5474\n",
      "Epoch 521/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5945 - accuracy: 0.7434\n",
      "Epoch 521: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5958 - accuracy: 0.7436 - val_loss: 1.2089 - val_accuracy: 0.5333\n",
      "Epoch 522/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5979 - accuracy: 0.7370\n",
      "Epoch 522: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6040 - accuracy: 0.7336 - val_loss: 1.2102 - val_accuracy: 0.5423\n",
      "Epoch 523/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5894 - accuracy: 0.7433\n",
      "Epoch 523: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5903 - accuracy: 0.7423 - val_loss: 1.2668 - val_accuracy: 0.5256\n",
      "Epoch 524/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5978 - accuracy: 0.7430\n",
      "Epoch 524: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5973 - accuracy: 0.7431 - val_loss: 1.3003 - val_accuracy: 0.5115\n",
      "Epoch 525/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5921 - accuracy: 0.7447\n",
      "Epoch 525: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5902 - accuracy: 0.7446 - val_loss: 1.2359 - val_accuracy: 0.5192\n",
      "Epoch 526/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6048 - accuracy: 0.7342\n",
      "Epoch 526: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6010 - accuracy: 0.7369 - val_loss: 1.2618 - val_accuracy: 0.4987\n",
      "Epoch 527/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5940 - accuracy: 0.7464\n",
      "Epoch 527: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5929 - accuracy: 0.7469 - val_loss: 1.2309 - val_accuracy: 0.5192\n",
      "Epoch 528/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.6014 - accuracy: 0.7411\n",
      "Epoch 528: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6016 - accuracy: 0.7405 - val_loss: 1.2126 - val_accuracy: 0.5410\n",
      "Epoch 529/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.6096 - accuracy: 0.7366\n",
      "Epoch 529: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6082 - accuracy: 0.7366 - val_loss: 1.2300 - val_accuracy: 0.5487\n",
      "Epoch 530/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.6003 - accuracy: 0.7425\n",
      "Epoch 530: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6001 - accuracy: 0.7418 - val_loss: 1.2593 - val_accuracy: 0.5218\n",
      "Epoch 531/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5961 - accuracy: 0.7469\n",
      "Epoch 531: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5976 - accuracy: 0.7461 - val_loss: 1.2474 - val_accuracy: 0.5295\n",
      "Epoch 532/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5941 - accuracy: 0.7407\n",
      "Epoch 532: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5928 - accuracy: 0.7420 - val_loss: 1.3701 - val_accuracy: 0.5026\n",
      "Epoch 533/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5885 - accuracy: 0.7433\n",
      "Epoch 533: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5873 - accuracy: 0.7438 - val_loss: 1.2583 - val_accuracy: 0.5397\n",
      "Epoch 534/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.6003 - accuracy: 0.7410\n",
      "Epoch 534: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5994 - accuracy: 0.7402 - val_loss: 1.2374 - val_accuracy: 0.5385\n",
      "Epoch 535/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5872 - accuracy: 0.7397\n",
      "Epoch 535: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5872 - accuracy: 0.7397 - val_loss: 1.2839 - val_accuracy: 0.5128\n",
      "Epoch 536/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5901 - accuracy: 0.7406\n",
      "Epoch 536: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5893 - accuracy: 0.7431 - val_loss: 1.2130 - val_accuracy: 0.5423\n",
      "Epoch 537/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5873 - accuracy: 0.7411\n",
      "Epoch 537: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5876 - accuracy: 0.7415 - val_loss: 1.2650 - val_accuracy: 0.5282\n",
      "Epoch 538/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.6015 - accuracy: 0.7421\n",
      "Epoch 538: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6018 - accuracy: 0.7423 - val_loss: 1.3150 - val_accuracy: 0.5077\n",
      "Epoch 539/1000\n",
      "113/122 [==========================>...] - ETA: 0s - loss: 0.6039 - accuracy: 0.7459\n",
      "Epoch 539: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6018 - accuracy: 0.7461 - val_loss: 1.2348 - val_accuracy: 0.5385\n",
      "Epoch 540/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5909 - accuracy: 0.7370\n",
      "Epoch 540: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5898 - accuracy: 0.7390 - val_loss: 1.2066 - val_accuracy: 0.5372\n",
      "Epoch 541/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5899 - accuracy: 0.7441\n",
      "Epoch 541: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5880 - accuracy: 0.7438 - val_loss: 1.2499 - val_accuracy: 0.5154\n",
      "Epoch 542/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5923 - accuracy: 0.7395\n",
      "Epoch 542: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5922 - accuracy: 0.7400 - val_loss: 1.2372 - val_accuracy: 0.5077\n",
      "Epoch 543/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5922 - accuracy: 0.7445\n",
      "Epoch 543: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5959 - accuracy: 0.7433 - val_loss: 1.2848 - val_accuracy: 0.5154\n",
      "Epoch 544/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5740 - accuracy: 0.7581\n",
      "Epoch 544: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5744 - accuracy: 0.7582 - val_loss: 1.2523 - val_accuracy: 0.5308\n",
      "Epoch 545/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5770 - accuracy: 0.7495\n",
      "Epoch 545: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5784 - accuracy: 0.7487 - val_loss: 1.2387 - val_accuracy: 0.5423\n",
      "Epoch 546/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5793 - accuracy: 0.7481\n",
      "Epoch 546: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5803 - accuracy: 0.7469 - val_loss: 1.2938 - val_accuracy: 0.5218\n",
      "Epoch 547/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5908 - accuracy: 0.7375\n",
      "Epoch 547: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5927 - accuracy: 0.7366 - val_loss: 1.2738 - val_accuracy: 0.5308\n",
      "Epoch 548/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5988 - accuracy: 0.7456\n",
      "Epoch 548: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5989 - accuracy: 0.7456 - val_loss: 1.2281 - val_accuracy: 0.5269\n",
      "Epoch 549/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5980 - accuracy: 0.7400\n",
      "Epoch 549: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5980 - accuracy: 0.7395 - val_loss: 1.2369 - val_accuracy: 0.5346\n",
      "Epoch 550/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.6029 - accuracy: 0.7406\n",
      "Epoch 550: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6016 - accuracy: 0.7413 - val_loss: 1.2526 - val_accuracy: 0.5346\n",
      "Epoch 551/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5879 - accuracy: 0.7461\n",
      "Epoch 551: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5883 - accuracy: 0.7464 - val_loss: 1.2469 - val_accuracy: 0.5321\n",
      "Epoch 552/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5726 - accuracy: 0.7545\n",
      "Epoch 552: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5741 - accuracy: 0.7536 - val_loss: 1.2949 - val_accuracy: 0.5103\n",
      "Epoch 553/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5808 - accuracy: 0.7438\n",
      "Epoch 553: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5812 - accuracy: 0.7441 - val_loss: 1.2599 - val_accuracy: 0.5436\n",
      "Epoch 554/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5994 - accuracy: 0.7430\n",
      "Epoch 554: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5954 - accuracy: 0.7467 - val_loss: 1.2235 - val_accuracy: 0.5513\n",
      "Epoch 555/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5746 - accuracy: 0.7543\n",
      "Epoch 555: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5779 - accuracy: 0.7528 - val_loss: 1.2869 - val_accuracy: 0.5256\n",
      "Epoch 556/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5830 - accuracy: 0.7513\n",
      "Epoch 556: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5842 - accuracy: 0.7495 - val_loss: 1.3101 - val_accuracy: 0.5038\n",
      "Epoch 557/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5986 - accuracy: 0.7458\n",
      "Epoch 557: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5980 - accuracy: 0.7467 - val_loss: 1.2804 - val_accuracy: 0.5141\n",
      "Epoch 558/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5989 - accuracy: 0.7436\n",
      "Epoch 558: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6004 - accuracy: 0.7423 - val_loss: 1.3217 - val_accuracy: 0.5026\n",
      "Epoch 559/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5959 - accuracy: 0.7405\n",
      "Epoch 559: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5971 - accuracy: 0.7408 - val_loss: 1.2993 - val_accuracy: 0.5218\n",
      "Epoch 560/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5894 - accuracy: 0.7449\n",
      "Epoch 560: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.5880 - accuracy: 0.7459 - val_loss: 1.2701 - val_accuracy: 0.5231\n",
      "Epoch 561/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5685 - accuracy: 0.7581\n",
      "Epoch 561: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.5707 - accuracy: 0.7575 - val_loss: 1.3269 - val_accuracy: 0.5090\n",
      "Epoch 562/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5837 - accuracy: 0.7419\n",
      "Epoch 562: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5832 - accuracy: 0.7425 - val_loss: 1.2069 - val_accuracy: 0.5385\n",
      "Epoch 563/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5921 - accuracy: 0.7423\n",
      "Epoch 563: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5921 - accuracy: 0.7423 - val_loss: 1.2352 - val_accuracy: 0.5192\n",
      "Epoch 564/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5771 - accuracy: 0.7474\n",
      "Epoch 564: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5781 - accuracy: 0.7472 - val_loss: 1.2899 - val_accuracy: 0.5256\n",
      "Epoch 565/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5913 - accuracy: 0.7476\n",
      "Epoch 565: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5953 - accuracy: 0.7436 - val_loss: 1.2138 - val_accuracy: 0.5333\n",
      "Epoch 566/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5782 - accuracy: 0.7478\n",
      "Epoch 566: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5817 - accuracy: 0.7474 - val_loss: 1.2524 - val_accuracy: 0.5295\n",
      "Epoch 567/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5822 - accuracy: 0.7435\n",
      "Epoch 567: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5804 - accuracy: 0.7464 - val_loss: 1.3035 - val_accuracy: 0.5013\n",
      "Epoch 568/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5836 - accuracy: 0.7451\n",
      "Epoch 568: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5832 - accuracy: 0.7449 - val_loss: 1.2456 - val_accuracy: 0.5538\n",
      "Epoch 569/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5916 - accuracy: 0.7438\n",
      "Epoch 569: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5916 - accuracy: 0.7438 - val_loss: 1.2460 - val_accuracy: 0.5333\n",
      "Epoch 570/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5706 - accuracy: 0.7559\n",
      "Epoch 570: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5708 - accuracy: 0.7559 - val_loss: 1.2841 - val_accuracy: 0.5179\n",
      "Epoch 571/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5936 - accuracy: 0.7411\n",
      "Epoch 571: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5939 - accuracy: 0.7418 - val_loss: 1.2777 - val_accuracy: 0.5154\n",
      "Epoch 572/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5780 - accuracy: 0.7421\n",
      "Epoch 572: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5769 - accuracy: 0.7428 - val_loss: 1.2492 - val_accuracy: 0.5397\n",
      "Epoch 573/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5762 - accuracy: 0.7484\n",
      "Epoch 573: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5765 - accuracy: 0.7482 - val_loss: 1.2911 - val_accuracy: 0.5282\n",
      "Epoch 574/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5917 - accuracy: 0.7430\n",
      "Epoch 574: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5895 - accuracy: 0.7441 - val_loss: 1.2912 - val_accuracy: 0.5295\n",
      "Epoch 575/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5734 - accuracy: 0.7524\n",
      "Epoch 575: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5714 - accuracy: 0.7528 - val_loss: 1.3144 - val_accuracy: 0.4974\n",
      "Epoch 576/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5741 - accuracy: 0.7500\n",
      "Epoch 576: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5721 - accuracy: 0.7515 - val_loss: 1.3042 - val_accuracy: 0.5141\n",
      "Epoch 577/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5785 - accuracy: 0.7544\n",
      "Epoch 577: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5785 - accuracy: 0.7544 - val_loss: 1.2734 - val_accuracy: 0.5295\n",
      "Epoch 578/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5996 - accuracy: 0.7333\n",
      "Epoch 578: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6000 - accuracy: 0.7333 - val_loss: 1.3406 - val_accuracy: 0.4923\n",
      "Epoch 579/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5789 - accuracy: 0.7531\n",
      "Epoch 579: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5789 - accuracy: 0.7531 - val_loss: 1.4033 - val_accuracy: 0.5295\n",
      "Epoch 580/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5774 - accuracy: 0.7477\n",
      "Epoch 580: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5773 - accuracy: 0.7479 - val_loss: 1.2692 - val_accuracy: 0.5385\n",
      "Epoch 581/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5776 - accuracy: 0.7489\n",
      "Epoch 581: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5803 - accuracy: 0.7485 - val_loss: 1.2671 - val_accuracy: 0.5372\n",
      "Epoch 582/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5687 - accuracy: 0.7603\n",
      "Epoch 582: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5702 - accuracy: 0.7598 - val_loss: 1.2865 - val_accuracy: 0.5179\n",
      "Epoch 583/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5735 - accuracy: 0.7538\n",
      "Epoch 583: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5737 - accuracy: 0.7539 - val_loss: 1.2644 - val_accuracy: 0.5192\n",
      "Epoch 584/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5793 - accuracy: 0.7474\n",
      "Epoch 584: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5789 - accuracy: 0.7472 - val_loss: 1.3241 - val_accuracy: 0.5038\n",
      "Epoch 585/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5828 - accuracy: 0.7466\n",
      "Epoch 585: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5816 - accuracy: 0.7469 - val_loss: 1.3028 - val_accuracy: 0.5308\n",
      "Epoch 586/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5958 - accuracy: 0.7422\n",
      "Epoch 586: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5954 - accuracy: 0.7415 - val_loss: 1.2939 - val_accuracy: 0.5282\n",
      "Epoch 587/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5813 - accuracy: 0.7473\n",
      "Epoch 587: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5806 - accuracy: 0.7490 - val_loss: 1.2691 - val_accuracy: 0.5269\n",
      "Epoch 588/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5729 - accuracy: 0.7508\n",
      "Epoch 588: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5800 - accuracy: 0.7474 - val_loss: 1.2215 - val_accuracy: 0.5410\n",
      "Epoch 589/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.6008 - accuracy: 0.7448\n",
      "Epoch 589: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5991 - accuracy: 0.7456 - val_loss: 1.2710 - val_accuracy: 0.5205\n",
      "Epoch 590/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7521\n",
      "Epoch 590: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5699 - accuracy: 0.7528 - val_loss: 1.3220 - val_accuracy: 0.5231\n",
      "Epoch 591/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5811 - accuracy: 0.7481\n",
      "Epoch 591: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5773 - accuracy: 0.7495 - val_loss: 1.2715 - val_accuracy: 0.5474\n",
      "Epoch 592/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5805 - accuracy: 0.7470\n",
      "Epoch 592: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5760 - accuracy: 0.7492 - val_loss: 1.2971 - val_accuracy: 0.5321\n",
      "Epoch 593/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5670 - accuracy: 0.7558\n",
      "Epoch 593: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5656 - accuracy: 0.7564 - val_loss: 1.2707 - val_accuracy: 0.5269\n",
      "Epoch 594/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5606 - accuracy: 0.7508\n",
      "Epoch 594: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5643 - accuracy: 0.7487 - val_loss: 1.2711 - val_accuracy: 0.5372\n",
      "Epoch 595/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5716 - accuracy: 0.7532\n",
      "Epoch 595: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5657 - accuracy: 0.7549 - val_loss: 1.2876 - val_accuracy: 0.5282\n",
      "Epoch 596/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5809 - accuracy: 0.7492\n",
      "Epoch 596: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5809 - accuracy: 0.7492 - val_loss: 1.2875 - val_accuracy: 0.5103\n",
      "Epoch 597/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5739 - accuracy: 0.7535\n",
      "Epoch 597: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5729 - accuracy: 0.7544 - val_loss: 1.3128 - val_accuracy: 0.5231\n",
      "Epoch 598/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5835 - accuracy: 0.7466\n",
      "Epoch 598: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5823 - accuracy: 0.7472 - val_loss: 1.2905 - val_accuracy: 0.5179\n",
      "Epoch 599/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5708 - accuracy: 0.7585\n",
      "Epoch 599: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5708 - accuracy: 0.7585 - val_loss: 1.3390 - val_accuracy: 0.5115\n",
      "Epoch 600/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5810 - accuracy: 0.7481\n",
      "Epoch 600: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5811 - accuracy: 0.7479 - val_loss: 1.3445 - val_accuracy: 0.5128\n",
      "Epoch 601/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5715 - accuracy: 0.7518\n",
      "Epoch 601: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5726 - accuracy: 0.7518 - val_loss: 1.2925 - val_accuracy: 0.5218\n",
      "Epoch 602/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5915 - accuracy: 0.7487\n",
      "Epoch 602: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5902 - accuracy: 0.7495 - val_loss: 1.2570 - val_accuracy: 0.5308\n",
      "Epoch 603/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5808 - accuracy: 0.7515\n",
      "Epoch 603: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5808 - accuracy: 0.7515 - val_loss: 1.3860 - val_accuracy: 0.4962\n",
      "Epoch 604/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5799 - accuracy: 0.7495\n",
      "Epoch 604: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5805 - accuracy: 0.7490 - val_loss: 1.2243 - val_accuracy: 0.5449\n",
      "Epoch 605/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5667 - accuracy: 0.7532\n",
      "Epoch 605: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5657 - accuracy: 0.7541 - val_loss: 1.2991 - val_accuracy: 0.5244\n",
      "Epoch 606/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7586\n",
      "Epoch 606: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5659 - accuracy: 0.7582 - val_loss: 1.3250 - val_accuracy: 0.5115\n",
      "Epoch 607/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5811 - accuracy: 0.7439\n",
      "Epoch 607: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5814 - accuracy: 0.7438 - val_loss: 1.3535 - val_accuracy: 0.5038\n",
      "Epoch 608/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5750 - accuracy: 0.7481\n",
      "Epoch 608: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5715 - accuracy: 0.7497 - val_loss: 1.3147 - val_accuracy: 0.5154\n",
      "Epoch 609/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5845 - accuracy: 0.7459\n",
      "Epoch 609: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5845 - accuracy: 0.7459 - val_loss: 1.2894 - val_accuracy: 0.5256\n",
      "Epoch 610/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5601 - accuracy: 0.7631\n",
      "Epoch 610: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5601 - accuracy: 0.7631 - val_loss: 1.3272 - val_accuracy: 0.5205\n",
      "Epoch 611/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5717 - accuracy: 0.7547\n",
      "Epoch 611: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5731 - accuracy: 0.7544 - val_loss: 1.3274 - val_accuracy: 0.5500\n",
      "Epoch 612/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5699 - accuracy: 0.7532\n",
      "Epoch 612: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5693 - accuracy: 0.7533 - val_loss: 1.3344 - val_accuracy: 0.5256\n",
      "Epoch 613/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5665 - accuracy: 0.7535\n",
      "Epoch 613: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5646 - accuracy: 0.7551 - val_loss: 1.3228 - val_accuracy: 0.5256\n",
      "Epoch 614/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5671 - accuracy: 0.7570\n",
      "Epoch 614: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5668 - accuracy: 0.7569 - val_loss: 1.3111 - val_accuracy: 0.5282\n",
      "Epoch 615/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5714 - accuracy: 0.7518\n",
      "Epoch 615: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5698 - accuracy: 0.7533 - val_loss: 1.3270 - val_accuracy: 0.5244\n",
      "Epoch 616/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5556 - accuracy: 0.7611\n",
      "Epoch 616: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5557 - accuracy: 0.7621 - val_loss: 1.3115 - val_accuracy: 0.5308\n",
      "Epoch 617/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5624 - accuracy: 0.7557\n",
      "Epoch 617: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5645 - accuracy: 0.7544 - val_loss: 1.2712 - val_accuracy: 0.5231\n",
      "Epoch 618/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5572 - accuracy: 0.7579\n",
      "Epoch 618: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5579 - accuracy: 0.7577 - val_loss: 1.3125 - val_accuracy: 0.5141\n",
      "Epoch 619/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5675 - accuracy: 0.7527\n",
      "Epoch 619: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5688 - accuracy: 0.7521 - val_loss: 1.3304 - val_accuracy: 0.5103\n",
      "Epoch 620/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5769 - accuracy: 0.7531\n",
      "Epoch 620: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5773 - accuracy: 0.7526 - val_loss: 1.2769 - val_accuracy: 0.5436\n",
      "Epoch 621/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5713 - accuracy: 0.7508\n",
      "Epoch 621: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5648 - accuracy: 0.7544 - val_loss: 1.3265 - val_accuracy: 0.5256\n",
      "Epoch 622/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5676 - accuracy: 0.7484\n",
      "Epoch 622: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5668 - accuracy: 0.7490 - val_loss: 1.3467 - val_accuracy: 0.5128\n",
      "Epoch 623/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5739 - accuracy: 0.7497\n",
      "Epoch 623: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5739 - accuracy: 0.7497 - val_loss: 1.3176 - val_accuracy: 0.5333\n",
      "Epoch 624/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5707 - accuracy: 0.7534\n",
      "Epoch 624: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5764 - accuracy: 0.7518 - val_loss: 1.2561 - val_accuracy: 0.5256\n",
      "Epoch 625/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5692 - accuracy: 0.7603\n",
      "Epoch 625: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5698 - accuracy: 0.7600 - val_loss: 1.2857 - val_accuracy: 0.5487\n",
      "Epoch 626/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5559 - accuracy: 0.7665\n",
      "Epoch 626: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5584 - accuracy: 0.7657 - val_loss: 1.2908 - val_accuracy: 0.5295\n",
      "Epoch 627/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5636 - accuracy: 0.7531\n",
      "Epoch 627: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5691 - accuracy: 0.7497 - val_loss: 1.2723 - val_accuracy: 0.5410\n",
      "Epoch 628/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5567 - accuracy: 0.7645\n",
      "Epoch 628: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5566 - accuracy: 0.7649 - val_loss: 1.3185 - val_accuracy: 0.5179\n",
      "Epoch 629/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5599 - accuracy: 0.7565\n",
      "Epoch 629: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5655 - accuracy: 0.7539 - val_loss: 1.3277 - val_accuracy: 0.5397\n",
      "Epoch 630/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5566 - accuracy: 0.7561\n",
      "Epoch 630: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5587 - accuracy: 0.7559 - val_loss: 1.3576 - val_accuracy: 0.5282\n",
      "Epoch 631/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5675 - accuracy: 0.7489\n",
      "Epoch 631: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5677 - accuracy: 0.7477 - val_loss: 1.2841 - val_accuracy: 0.5321\n",
      "Epoch 632/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5568 - accuracy: 0.7633\n",
      "Epoch 632: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5593 - accuracy: 0.7626 - val_loss: 1.2996 - val_accuracy: 0.5321\n",
      "Epoch 633/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5770 - accuracy: 0.7508\n",
      "Epoch 633: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5770 - accuracy: 0.7508 - val_loss: 1.2762 - val_accuracy: 0.5269\n",
      "Epoch 634/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5673 - accuracy: 0.7555\n",
      "Epoch 634: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5670 - accuracy: 0.7559 - val_loss: 1.2826 - val_accuracy: 0.5231\n",
      "Epoch 635/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5450 - accuracy: 0.7723\n",
      "Epoch 635: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5432 - accuracy: 0.7724 - val_loss: 1.3640 - val_accuracy: 0.5269\n",
      "Epoch 636/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5625 - accuracy: 0.7587\n",
      "Epoch 636: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5621 - accuracy: 0.7580 - val_loss: 1.3074 - val_accuracy: 0.5449\n",
      "Epoch 637/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5625 - accuracy: 0.7534\n",
      "Epoch 637: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5623 - accuracy: 0.7533 - val_loss: 1.2938 - val_accuracy: 0.5269\n",
      "Epoch 638/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5710 - accuracy: 0.7497\n",
      "Epoch 638: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5708 - accuracy: 0.7497 - val_loss: 1.3375 - val_accuracy: 0.5167\n",
      "Epoch 639/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5594 - accuracy: 0.7589\n",
      "Epoch 639: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5604 - accuracy: 0.7587 - val_loss: 1.3281 - val_accuracy: 0.5333\n",
      "Epoch 640/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5684 - accuracy: 0.7546\n",
      "Epoch 640: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5668 - accuracy: 0.7544 - val_loss: 1.2951 - val_accuracy: 0.5321\n",
      "Epoch 641/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5652 - accuracy: 0.7576\n",
      "Epoch 641: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5645 - accuracy: 0.7577 - val_loss: 1.3297 - val_accuracy: 0.5244\n",
      "Epoch 642/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5599 - accuracy: 0.7530\n",
      "Epoch 642: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5606 - accuracy: 0.7539 - val_loss: 1.3088 - val_accuracy: 0.5397\n",
      "Epoch 643/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5745 - accuracy: 0.7487\n",
      "Epoch 643: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5708 - accuracy: 0.7503 - val_loss: 1.2969 - val_accuracy: 0.5218\n",
      "Epoch 644/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5531 - accuracy: 0.7666\n",
      "Epoch 644: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5567 - accuracy: 0.7639 - val_loss: 1.3349 - val_accuracy: 0.5115\n",
      "Epoch 645/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5644 - accuracy: 0.7536\n",
      "Epoch 645: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5644 - accuracy: 0.7536 - val_loss: 1.2376 - val_accuracy: 0.5372\n",
      "Epoch 646/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5645 - accuracy: 0.7658\n",
      "Epoch 646: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5644 - accuracy: 0.7654 - val_loss: 1.3429 - val_accuracy: 0.5128\n",
      "Epoch 647/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5746 - accuracy: 0.7553\n",
      "Epoch 647: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5794 - accuracy: 0.7526 - val_loss: 1.2905 - val_accuracy: 0.4987\n",
      "Epoch 648/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5712 - accuracy: 0.7534\n",
      "Epoch 648: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5689 - accuracy: 0.7549 - val_loss: 1.3242 - val_accuracy: 0.5346\n",
      "Epoch 649/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5695 - accuracy: 0.7529\n",
      "Epoch 649: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5703 - accuracy: 0.7536 - val_loss: 1.3915 - val_accuracy: 0.4962\n",
      "Epoch 650/1000\n",
      "113/122 [==========================>...] - ETA: 0s - loss: 0.5557 - accuracy: 0.7586\n",
      "Epoch 650: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5517 - accuracy: 0.7590 - val_loss: 1.3339 - val_accuracy: 0.5192\n",
      "Epoch 651/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5771 - accuracy: 0.7481\n",
      "Epoch 651: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5721 - accuracy: 0.7500 - val_loss: 1.3503 - val_accuracy: 0.5205\n",
      "Epoch 652/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5659 - accuracy: 0.7543\n",
      "Epoch 652: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5664 - accuracy: 0.7515 - val_loss: 1.2953 - val_accuracy: 0.5423\n",
      "Epoch 653/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5564 - accuracy: 0.7609\n",
      "Epoch 653: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5563 - accuracy: 0.7608 - val_loss: 1.3176 - val_accuracy: 0.5244\n",
      "Epoch 654/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5777 - accuracy: 0.7489\n",
      "Epoch 654: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5787 - accuracy: 0.7485 - val_loss: 1.3132 - val_accuracy: 0.5308\n",
      "Epoch 655/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5615 - accuracy: 0.7619\n",
      "Epoch 655: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5634 - accuracy: 0.7608 - val_loss: 1.3438 - val_accuracy: 0.5282\n",
      "Epoch 656/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5554 - accuracy: 0.7557\n",
      "Epoch 656: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5567 - accuracy: 0.7562 - val_loss: 1.2741 - val_accuracy: 0.5436\n",
      "Epoch 657/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5523 - accuracy: 0.7557\n",
      "Epoch 657: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5519 - accuracy: 0.7562 - val_loss: 1.2661 - val_accuracy: 0.5538\n",
      "Epoch 658/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7539\n",
      "Epoch 658: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5629 - accuracy: 0.7546 - val_loss: 1.3741 - val_accuracy: 0.5346\n",
      "Epoch 659/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5811 - accuracy: 0.7500\n",
      "Epoch 659: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5770 - accuracy: 0.7521 - val_loss: 1.2832 - val_accuracy: 0.5474\n",
      "Epoch 660/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5408 - accuracy: 0.7563\n",
      "Epoch 660: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5440 - accuracy: 0.7562 - val_loss: 1.3275 - val_accuracy: 0.5436\n",
      "Epoch 661/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7585\n",
      "Epoch 661: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5635 - accuracy: 0.7590 - val_loss: 1.3623 - val_accuracy: 0.4936\n",
      "Epoch 662/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7513\n",
      "Epoch 662: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5668 - accuracy: 0.7513 - val_loss: 1.2941 - val_accuracy: 0.5321\n",
      "Epoch 663/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5691 - accuracy: 0.7565\n",
      "Epoch 663: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5689 - accuracy: 0.7569 - val_loss: 1.3107 - val_accuracy: 0.5436\n",
      "Epoch 664/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5638 - accuracy: 0.7545\n",
      "Epoch 664: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5660 - accuracy: 0.7541 - val_loss: 1.3156 - val_accuracy: 0.5436\n",
      "Epoch 665/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5594 - accuracy: 0.7575\n",
      "Epoch 665: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5597 - accuracy: 0.7572 - val_loss: 1.4015 - val_accuracy: 0.5179\n",
      "Epoch 666/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5509 - accuracy: 0.7664\n",
      "Epoch 666: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5499 - accuracy: 0.7662 - val_loss: 1.3243 - val_accuracy: 0.5321\n",
      "Epoch 667/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5588 - accuracy: 0.7521\n",
      "Epoch 667: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5574 - accuracy: 0.7539 - val_loss: 1.3731 - val_accuracy: 0.5103\n",
      "Epoch 668/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7642\n",
      "Epoch 668: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5613 - accuracy: 0.7649 - val_loss: 1.3439 - val_accuracy: 0.5333\n",
      "Epoch 669/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5540 - accuracy: 0.7645\n",
      "Epoch 669: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5529 - accuracy: 0.7649 - val_loss: 1.3552 - val_accuracy: 0.5090\n",
      "Epoch 670/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5502 - accuracy: 0.7658\n",
      "Epoch 670: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5539 - accuracy: 0.7628 - val_loss: 1.4177 - val_accuracy: 0.5103\n",
      "Epoch 671/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5554 - accuracy: 0.7664\n",
      "Epoch 671: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5499 - accuracy: 0.7685 - val_loss: 1.3800 - val_accuracy: 0.5128\n",
      "Epoch 672/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5744 - accuracy: 0.7453\n",
      "Epoch 672: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5712 - accuracy: 0.7479 - val_loss: 1.2982 - val_accuracy: 0.5359\n",
      "Epoch 673/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5827 - accuracy: 0.7532\n",
      "Epoch 673: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5821 - accuracy: 0.7528 - val_loss: 1.3442 - val_accuracy: 0.5256\n",
      "Epoch 674/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5585 - accuracy: 0.7623\n",
      "Epoch 674: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5601 - accuracy: 0.7582 - val_loss: 1.3343 - val_accuracy: 0.5321\n",
      "Epoch 675/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5573 - accuracy: 0.7610\n",
      "Epoch 675: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5573 - accuracy: 0.7610 - val_loss: 1.3009 - val_accuracy: 0.5321\n",
      "Epoch 676/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5644 - accuracy: 0.7558\n",
      "Epoch 676: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5630 - accuracy: 0.7569 - val_loss: 1.2916 - val_accuracy: 0.5449\n",
      "Epoch 677/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5552 - accuracy: 0.7514\n",
      "Epoch 677: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5505 - accuracy: 0.7539 - val_loss: 1.3579 - val_accuracy: 0.5244\n",
      "Epoch 678/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5458 - accuracy: 0.7632\n",
      "Epoch 678: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5399 - accuracy: 0.7667 - val_loss: 1.3690 - val_accuracy: 0.5231\n",
      "Epoch 679/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5565 - accuracy: 0.7592\n",
      "Epoch 679: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5546 - accuracy: 0.7600 - val_loss: 1.3308 - val_accuracy: 0.5231\n",
      "Epoch 680/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5446 - accuracy: 0.7571\n",
      "Epoch 680: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5512 - accuracy: 0.7536 - val_loss: 1.2996 - val_accuracy: 0.5308\n",
      "Epoch 681/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7634\n",
      "Epoch 681: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5618 - accuracy: 0.7634 - val_loss: 1.3223 - val_accuracy: 0.5346\n",
      "Epoch 682/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5465 - accuracy: 0.7652\n",
      "Epoch 682: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5472 - accuracy: 0.7644 - val_loss: 1.3726 - val_accuracy: 0.5154\n",
      "Epoch 683/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5420 - accuracy: 0.7684\n",
      "Epoch 683: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5392 - accuracy: 0.7690 - val_loss: 1.4022 - val_accuracy: 0.5141\n",
      "Epoch 684/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5472 - accuracy: 0.7719\n",
      "Epoch 684: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5452 - accuracy: 0.7713 - val_loss: 1.3443 - val_accuracy: 0.5244\n",
      "Epoch 685/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5715 - accuracy: 0.7588\n",
      "Epoch 685: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5747 - accuracy: 0.7551 - val_loss: 1.3127 - val_accuracy: 0.5474\n",
      "Epoch 686/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5601 - accuracy: 0.7529\n",
      "Epoch 686: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5601 - accuracy: 0.7533 - val_loss: 1.3682 - val_accuracy: 0.5141\n",
      "Epoch 687/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5639 - accuracy: 0.7565\n",
      "Epoch 687: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5620 - accuracy: 0.7575 - val_loss: 1.3399 - val_accuracy: 0.5077\n",
      "Epoch 688/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5422 - accuracy: 0.7682\n",
      "Epoch 688: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5422 - accuracy: 0.7682 - val_loss: 1.3509 - val_accuracy: 0.5346\n",
      "Epoch 689/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5458 - accuracy: 0.7600\n",
      "Epoch 689: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5458 - accuracy: 0.7600 - val_loss: 1.3002 - val_accuracy: 0.5462\n",
      "Epoch 690/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5462 - accuracy: 0.7654\n",
      "Epoch 690: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5472 - accuracy: 0.7654 - val_loss: 1.3815 - val_accuracy: 0.5064\n",
      "Epoch 691/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5632 - accuracy: 0.7529\n",
      "Epoch 691: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5648 - accuracy: 0.7528 - val_loss: 1.3495 - val_accuracy: 0.5090\n",
      "Epoch 692/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5584 - accuracy: 0.7522\n",
      "Epoch 692: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5577 - accuracy: 0.7541 - val_loss: 1.3318 - val_accuracy: 0.5321\n",
      "Epoch 693/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5558 - accuracy: 0.7603\n",
      "Epoch 693: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5551 - accuracy: 0.7610 - val_loss: 1.3403 - val_accuracy: 0.5231\n",
      "Epoch 694/1000\n",
      "113/122 [==========================>...] - ETA: 0s - loss: 0.5435 - accuracy: 0.7616\n",
      "Epoch 694: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5425 - accuracy: 0.7636 - val_loss: 1.3370 - val_accuracy: 0.5346\n",
      "Epoch 695/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5498 - accuracy: 0.7599\n",
      "Epoch 695: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5502 - accuracy: 0.7610 - val_loss: 1.3694 - val_accuracy: 0.5167\n",
      "Epoch 696/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5656 - accuracy: 0.7530\n",
      "Epoch 696: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5619 - accuracy: 0.7567 - val_loss: 1.3833 - val_accuracy: 0.5115\n",
      "Epoch 697/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5500 - accuracy: 0.7552\n",
      "Epoch 697: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5485 - accuracy: 0.7569 - val_loss: 1.3410 - val_accuracy: 0.5346\n",
      "Epoch 698/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5673 - accuracy: 0.7500\n",
      "Epoch 698: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5691 - accuracy: 0.7490 - val_loss: 1.3334 - val_accuracy: 0.5333\n",
      "Epoch 699/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5552 - accuracy: 0.7592\n",
      "Epoch 699: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5518 - accuracy: 0.7605 - val_loss: 1.3796 - val_accuracy: 0.5077\n",
      "Epoch 700/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5610 - accuracy: 0.7563\n",
      "Epoch 700: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5616 - accuracy: 0.7554 - val_loss: 1.3504 - val_accuracy: 0.5321\n",
      "Epoch 701/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5512 - accuracy: 0.7632\n",
      "Epoch 701: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5480 - accuracy: 0.7641 - val_loss: 1.4253 - val_accuracy: 0.5115\n",
      "Epoch 702/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5377 - accuracy: 0.7629\n",
      "Epoch 702: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5416 - accuracy: 0.7631 - val_loss: 1.3120 - val_accuracy: 0.5474\n",
      "Epoch 703/1000\n",
      "113/122 [==========================>...] - ETA: 0s - loss: 0.5380 - accuracy: 0.7716\n",
      "Epoch 703: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5376 - accuracy: 0.7713 - val_loss: 1.3501 - val_accuracy: 0.5397\n",
      "Epoch 704/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5647 - accuracy: 0.7532\n",
      "Epoch 704: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5642 - accuracy: 0.7536 - val_loss: 1.3410 - val_accuracy: 0.5167\n",
      "Epoch 705/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5390 - accuracy: 0.7717\n",
      "Epoch 705: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5381 - accuracy: 0.7724 - val_loss: 1.3785 - val_accuracy: 0.5359\n",
      "Epoch 706/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5546 - accuracy: 0.7608\n",
      "Epoch 706: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5553 - accuracy: 0.7605 - val_loss: 1.3421 - val_accuracy: 0.5449\n",
      "Epoch 707/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5449 - accuracy: 0.7561\n",
      "Epoch 707: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5475 - accuracy: 0.7546 - val_loss: 1.3297 - val_accuracy: 0.5449\n",
      "Epoch 708/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5472 - accuracy: 0.7600\n",
      "Epoch 708: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5485 - accuracy: 0.7590 - val_loss: 1.3343 - val_accuracy: 0.5256\n",
      "Epoch 709/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5554 - accuracy: 0.7551\n",
      "Epoch 709: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5583 - accuracy: 0.7536 - val_loss: 1.3635 - val_accuracy: 0.5244\n",
      "Epoch 710/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5581 - accuracy: 0.7545\n",
      "Epoch 710: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5583 - accuracy: 0.7541 - val_loss: 1.3465 - val_accuracy: 0.5244\n",
      "Epoch 711/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5473 - accuracy: 0.7652\n",
      "Epoch 711: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5449 - accuracy: 0.7657 - val_loss: 1.4081 - val_accuracy: 0.5167\n",
      "Epoch 712/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5408 - accuracy: 0.7694\n",
      "Epoch 712: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5450 - accuracy: 0.7664 - val_loss: 1.3230 - val_accuracy: 0.5090\n",
      "Epoch 713/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5355 - accuracy: 0.7740\n",
      "Epoch 713: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 2s 12ms/step - loss: 0.5346 - accuracy: 0.7747 - val_loss: 1.3747 - val_accuracy: 0.5205\n",
      "Epoch 714/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5447 - accuracy: 0.7634\n",
      "Epoch 714: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5451 - accuracy: 0.7639 - val_loss: 1.3230 - val_accuracy: 0.5436\n",
      "Epoch 715/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5325 - accuracy: 0.7729\n",
      "Epoch 715: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5336 - accuracy: 0.7724 - val_loss: 1.3888 - val_accuracy: 0.5397\n",
      "Epoch 716/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5387 - accuracy: 0.7688\n",
      "Epoch 716: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5375 - accuracy: 0.7703 - val_loss: 1.3754 - val_accuracy: 0.5154\n",
      "Epoch 717/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5420 - accuracy: 0.7607\n",
      "Epoch 717: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5410 - accuracy: 0.7608 - val_loss: 1.3517 - val_accuracy: 0.5449\n",
      "Epoch 718/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5407 - accuracy: 0.7681\n",
      "Epoch 718: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5391 - accuracy: 0.7688 - val_loss: 1.3261 - val_accuracy: 0.5192\n",
      "Epoch 719/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5495 - accuracy: 0.7636\n",
      "Epoch 719: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5480 - accuracy: 0.7636 - val_loss: 1.3420 - val_accuracy: 0.5436\n",
      "Epoch 720/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5490 - accuracy: 0.7565\n",
      "Epoch 720: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5484 - accuracy: 0.7575 - val_loss: 1.3758 - val_accuracy: 0.5385\n",
      "Epoch 721/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5609 - accuracy: 0.7500\n",
      "Epoch 721: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5613 - accuracy: 0.7497 - val_loss: 1.3506 - val_accuracy: 0.5397\n",
      "Epoch 722/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5391 - accuracy: 0.7696\n",
      "Epoch 722: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5444 - accuracy: 0.7685 - val_loss: 1.3760 - val_accuracy: 0.5333\n",
      "Epoch 723/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5566 - accuracy: 0.7572\n",
      "Epoch 723: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5568 - accuracy: 0.7572 - val_loss: 1.3384 - val_accuracy: 0.5333\n",
      "Epoch 724/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5405 - accuracy: 0.7646\n",
      "Epoch 724: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5405 - accuracy: 0.7646 - val_loss: 1.3443 - val_accuracy: 0.5179\n",
      "Epoch 725/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5403 - accuracy: 0.7753\n",
      "Epoch 725: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5369 - accuracy: 0.7760 - val_loss: 1.3544 - val_accuracy: 0.5333\n",
      "Epoch 726/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5439 - accuracy: 0.7601\n",
      "Epoch 726: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5452 - accuracy: 0.7603 - val_loss: 1.4046 - val_accuracy: 0.5256\n",
      "Epoch 727/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5360 - accuracy: 0.7746\n",
      "Epoch 727: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5362 - accuracy: 0.7760 - val_loss: 1.3978 - val_accuracy: 0.5077\n",
      "Epoch 728/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5472 - accuracy: 0.7654\n",
      "Epoch 728: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5445 - accuracy: 0.7659 - val_loss: 1.4196 - val_accuracy: 0.5218\n",
      "Epoch 729/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5545 - accuracy: 0.7602\n",
      "Epoch 729: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5545 - accuracy: 0.7598 - val_loss: 1.4515 - val_accuracy: 0.5038\n",
      "Epoch 730/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5506 - accuracy: 0.7585\n",
      "Epoch 730: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5524 - accuracy: 0.7580 - val_loss: 1.3827 - val_accuracy: 0.5244\n",
      "Epoch 731/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5293 - accuracy: 0.7707\n",
      "Epoch 731: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5285 - accuracy: 0.7716 - val_loss: 1.3611 - val_accuracy: 0.5282\n",
      "Epoch 732/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5408 - accuracy: 0.7646\n",
      "Epoch 732: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 12ms/step - loss: 0.5413 - accuracy: 0.7636 - val_loss: 1.3128 - val_accuracy: 0.5513\n",
      "Epoch 733/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7621\n",
      "Epoch 733: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5384 - accuracy: 0.7628 - val_loss: 1.3433 - val_accuracy: 0.5410\n",
      "Epoch 734/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5434 - accuracy: 0.7685\n",
      "Epoch 734: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5491 - accuracy: 0.7659 - val_loss: 1.3880 - val_accuracy: 0.5244\n",
      "Epoch 735/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5326 - accuracy: 0.7711\n",
      "Epoch 735: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5304 - accuracy: 0.7724 - val_loss: 1.4122 - val_accuracy: 0.5167\n",
      "Epoch 736/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5273 - accuracy: 0.7721\n",
      "Epoch 736: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5294 - accuracy: 0.7718 - val_loss: 1.3304 - val_accuracy: 0.5372\n",
      "Epoch 737/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5501 - accuracy: 0.7605\n",
      "Epoch 737: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5494 - accuracy: 0.7608 - val_loss: 1.3605 - val_accuracy: 0.5295\n",
      "Epoch 738/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5524 - accuracy: 0.7646\n",
      "Epoch 738: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5524 - accuracy: 0.7646 - val_loss: 1.4151 - val_accuracy: 0.5179\n",
      "Epoch 739/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5347 - accuracy: 0.7676\n",
      "Epoch 739: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5367 - accuracy: 0.7675 - val_loss: 1.3682 - val_accuracy: 0.5282\n",
      "Epoch 740/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5311 - accuracy: 0.7731\n",
      "Epoch 740: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5290 - accuracy: 0.7736 - val_loss: 1.3937 - val_accuracy: 0.5321\n",
      "Epoch 741/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5435 - accuracy: 0.7696\n",
      "Epoch 741: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5423 - accuracy: 0.7706 - val_loss: 1.3970 - val_accuracy: 0.5231\n",
      "Epoch 742/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5241 - accuracy: 0.7740\n",
      "Epoch 742: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5240 - accuracy: 0.7742 - val_loss: 1.3913 - val_accuracy: 0.5154\n",
      "Epoch 743/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5549 - accuracy: 0.7601\n",
      "Epoch 743: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5551 - accuracy: 0.7608 - val_loss: 1.3438 - val_accuracy: 0.5308\n",
      "Epoch 744/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5460 - accuracy: 0.7679\n",
      "Epoch 744: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5436 - accuracy: 0.7680 - val_loss: 1.3621 - val_accuracy: 0.5346\n",
      "Epoch 745/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5334 - accuracy: 0.7680\n",
      "Epoch 745: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 11ms/step - loss: 0.5334 - accuracy: 0.7680 - val_loss: 1.3548 - val_accuracy: 0.5205\n",
      "Epoch 746/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5436 - accuracy: 0.7690\n",
      "Epoch 746: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5441 - accuracy: 0.7695 - val_loss: 1.3889 - val_accuracy: 0.5385\n",
      "Epoch 747/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5394 - accuracy: 0.7663\n",
      "Epoch 747: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5425 - accuracy: 0.7641 - val_loss: 1.3980 - val_accuracy: 0.5231\n",
      "Epoch 748/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5282 - accuracy: 0.7722\n",
      "Epoch 748: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5290 - accuracy: 0.7713 - val_loss: 1.3584 - val_accuracy: 0.5192\n",
      "Epoch 749/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5401 - accuracy: 0.7639\n",
      "Epoch 749: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5410 - accuracy: 0.7628 - val_loss: 1.3964 - val_accuracy: 0.5256\n",
      "Epoch 750/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5453 - accuracy: 0.7628\n",
      "Epoch 750: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5424 - accuracy: 0.7646 - val_loss: 1.3980 - val_accuracy: 0.5244\n",
      "Epoch 751/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5459 - accuracy: 0.7614\n",
      "Epoch 751: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5450 - accuracy: 0.7618 - val_loss: 1.3811 - val_accuracy: 0.5269\n",
      "Epoch 752/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5417 - accuracy: 0.7621\n",
      "Epoch 752: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5371 - accuracy: 0.7646 - val_loss: 1.4014 - val_accuracy: 0.5103\n",
      "Epoch 753/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5412 - accuracy: 0.7610\n",
      "Epoch 753: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5413 - accuracy: 0.7613 - val_loss: 1.4287 - val_accuracy: 0.5128\n",
      "Epoch 754/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5204 - accuracy: 0.7742\n",
      "Epoch 754: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5223 - accuracy: 0.7747 - val_loss: 1.3846 - val_accuracy: 0.5308\n",
      "Epoch 755/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5400 - accuracy: 0.7751\n",
      "Epoch 755: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5399 - accuracy: 0.7752 - val_loss: 1.3681 - val_accuracy: 0.5154\n",
      "Epoch 756/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5351 - accuracy: 0.7709\n",
      "Epoch 756: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5344 - accuracy: 0.7711 - val_loss: 1.4001 - val_accuracy: 0.5256\n",
      "Epoch 757/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5436 - accuracy: 0.7644\n",
      "Epoch 757: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5401 - accuracy: 0.7662 - val_loss: 1.3627 - val_accuracy: 0.5359\n",
      "Epoch 758/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5392 - accuracy: 0.7696\n",
      "Epoch 758: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5394 - accuracy: 0.7698 - val_loss: 1.3804 - val_accuracy: 0.5192\n",
      "Epoch 759/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5237 - accuracy: 0.7810\n",
      "Epoch 759: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5287 - accuracy: 0.7777 - val_loss: 1.4184 - val_accuracy: 0.5179\n",
      "Epoch 760/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5364 - accuracy: 0.7685\n",
      "Epoch 760: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 11ms/step - loss: 0.5368 - accuracy: 0.7682 - val_loss: 1.3939 - val_accuracy: 0.5167\n",
      "Epoch 761/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5336 - accuracy: 0.7740\n",
      "Epoch 761: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5338 - accuracy: 0.7739 - val_loss: 1.4030 - val_accuracy: 0.5128\n",
      "Epoch 762/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5286 - accuracy: 0.7745\n",
      "Epoch 762: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5293 - accuracy: 0.7754 - val_loss: 1.4502 - val_accuracy: 0.5115\n",
      "Epoch 763/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5334 - accuracy: 0.7737\n",
      "Epoch 763: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5329 - accuracy: 0.7739 - val_loss: 1.4283 - val_accuracy: 0.5295\n",
      "Epoch 764/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5292 - accuracy: 0.7688\n",
      "Epoch 764: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5288 - accuracy: 0.7690 - val_loss: 1.3842 - val_accuracy: 0.5231\n",
      "Epoch 765/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5462 - accuracy: 0.7579\n",
      "Epoch 765: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5456 - accuracy: 0.7585 - val_loss: 1.4072 - val_accuracy: 0.5205\n",
      "Epoch 766/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5266 - accuracy: 0.7804\n",
      "Epoch 766: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5295 - accuracy: 0.7783 - val_loss: 1.4225 - val_accuracy: 0.5256\n",
      "Epoch 767/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5271 - accuracy: 0.7688\n",
      "Epoch 767: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5326 - accuracy: 0.7672 - val_loss: 1.3677 - val_accuracy: 0.5397\n",
      "Epoch 768/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5386 - accuracy: 0.7641\n",
      "Epoch 768: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5386 - accuracy: 0.7641 - val_loss: 1.4506 - val_accuracy: 0.5333\n",
      "Epoch 769/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5336 - accuracy: 0.7714\n",
      "Epoch 769: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5320 - accuracy: 0.7721 - val_loss: 1.3846 - val_accuracy: 0.5282\n",
      "Epoch 770/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5335 - accuracy: 0.7702\n",
      "Epoch 770: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5309 - accuracy: 0.7718 - val_loss: 1.4062 - val_accuracy: 0.5167\n",
      "Epoch 771/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5202 - accuracy: 0.7745\n",
      "Epoch 771: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5171 - accuracy: 0.7780 - val_loss: 1.3692 - val_accuracy: 0.5308\n",
      "Epoch 772/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5380 - accuracy: 0.7682\n",
      "Epoch 772: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5380 - accuracy: 0.7682 - val_loss: 1.4714 - val_accuracy: 0.4987\n",
      "Epoch 773/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5405 - accuracy: 0.7678\n",
      "Epoch 773: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5411 - accuracy: 0.7670 - val_loss: 1.4383 - val_accuracy: 0.5256\n",
      "Epoch 774/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5366 - accuracy: 0.7749\n",
      "Epoch 774: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5366 - accuracy: 0.7749 - val_loss: 1.3918 - val_accuracy: 0.5282\n",
      "Epoch 775/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5394 - accuracy: 0.7636\n",
      "Epoch 775: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5400 - accuracy: 0.7644 - val_loss: 1.4186 - val_accuracy: 0.5359\n",
      "Epoch 776/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5375 - accuracy: 0.7634\n",
      "Epoch 776: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5382 - accuracy: 0.7634 - val_loss: 1.3853 - val_accuracy: 0.5346\n",
      "Epoch 777/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5154 - accuracy: 0.7768\n",
      "Epoch 777: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5153 - accuracy: 0.7767 - val_loss: 1.4475 - val_accuracy: 0.5308\n",
      "Epoch 778/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5269 - accuracy: 0.7785\n",
      "Epoch 778: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5292 - accuracy: 0.7780 - val_loss: 1.3919 - val_accuracy: 0.5154\n",
      "Epoch 779/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5217 - accuracy: 0.7761\n",
      "Epoch 779: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5226 - accuracy: 0.7752 - val_loss: 1.4134 - val_accuracy: 0.5141\n",
      "Epoch 780/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5319 - accuracy: 0.7676\n",
      "Epoch 780: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5346 - accuracy: 0.7654 - val_loss: 1.3681 - val_accuracy: 0.5141\n",
      "Epoch 781/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5356 - accuracy: 0.7622\n",
      "Epoch 781: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5354 - accuracy: 0.7636 - val_loss: 1.3796 - val_accuracy: 0.5244\n",
      "Epoch 782/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5300 - accuracy: 0.7713\n",
      "Epoch 782: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5303 - accuracy: 0.7718 - val_loss: 1.4032 - val_accuracy: 0.5372\n",
      "Epoch 783/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5183 - accuracy: 0.7747\n",
      "Epoch 783: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5226 - accuracy: 0.7736 - val_loss: 1.3892 - val_accuracy: 0.5231\n",
      "Epoch 784/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5280 - accuracy: 0.7744\n",
      "Epoch 784: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5277 - accuracy: 0.7749 - val_loss: 1.3993 - val_accuracy: 0.5333\n",
      "Epoch 785/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5378 - accuracy: 0.7631\n",
      "Epoch 785: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5388 - accuracy: 0.7628 - val_loss: 1.4522 - val_accuracy: 0.5308\n",
      "Epoch 786/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5269 - accuracy: 0.7753\n",
      "Epoch 786: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5304 - accuracy: 0.7711 - val_loss: 1.3624 - val_accuracy: 0.5397\n",
      "Epoch 787/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5472 - accuracy: 0.7642\n",
      "Epoch 787: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5459 - accuracy: 0.7654 - val_loss: 1.3785 - val_accuracy: 0.5077\n",
      "Epoch 788/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5222 - accuracy: 0.7811\n",
      "Epoch 788: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5222 - accuracy: 0.7811 - val_loss: 1.4535 - val_accuracy: 0.5013\n",
      "Epoch 789/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5393 - accuracy: 0.7659\n",
      "Epoch 789: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5376 - accuracy: 0.7662 - val_loss: 1.3802 - val_accuracy: 0.5410\n",
      "Epoch 790/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5404 - accuracy: 0.7693\n",
      "Epoch 790: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5416 - accuracy: 0.7682 - val_loss: 1.4444 - val_accuracy: 0.5179\n",
      "Epoch 791/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5430 - accuracy: 0.7638\n",
      "Epoch 791: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5442 - accuracy: 0.7641 - val_loss: 1.4011 - val_accuracy: 0.5205\n",
      "Epoch 792/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5189 - accuracy: 0.7769\n",
      "Epoch 792: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5199 - accuracy: 0.7783 - val_loss: 1.3720 - val_accuracy: 0.5205\n",
      "Epoch 793/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5139 - accuracy: 0.7856\n",
      "Epoch 793: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5131 - accuracy: 0.7865 - val_loss: 1.4215 - val_accuracy: 0.5282\n",
      "Epoch 794/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5230 - accuracy: 0.7676\n",
      "Epoch 794: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5231 - accuracy: 0.7677 - val_loss: 1.4428 - val_accuracy: 0.5013\n",
      "Epoch 795/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5294 - accuracy: 0.7742\n",
      "Epoch 795: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5294 - accuracy: 0.7742 - val_loss: 1.4157 - val_accuracy: 0.5269\n",
      "Epoch 796/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5248 - accuracy: 0.7810\n",
      "Epoch 796: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5233 - accuracy: 0.7808 - val_loss: 1.4824 - val_accuracy: 0.5128\n",
      "Epoch 797/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5307 - accuracy: 0.7740\n",
      "Epoch 797: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5312 - accuracy: 0.7736 - val_loss: 1.4517 - val_accuracy: 0.5346\n",
      "Epoch 798/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5397 - accuracy: 0.7690\n",
      "Epoch 798: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5433 - accuracy: 0.7672 - val_loss: 1.3794 - val_accuracy: 0.5359\n",
      "Epoch 799/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5266 - accuracy: 0.7744\n",
      "Epoch 799: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 11ms/step - loss: 0.5266 - accuracy: 0.7744 - val_loss: 1.4141 - val_accuracy: 0.5103\n",
      "Epoch 800/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5255 - accuracy: 0.7752\n",
      "Epoch 800: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5255 - accuracy: 0.7752 - val_loss: 1.4343 - val_accuracy: 0.5064\n",
      "Epoch 801/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5274 - accuracy: 0.7654\n",
      "Epoch 801: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5278 - accuracy: 0.7654 - val_loss: 1.4072 - val_accuracy: 0.5256\n",
      "Epoch 802/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5138 - accuracy: 0.7918\n",
      "Epoch 802: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5117 - accuracy: 0.7927 - val_loss: 1.4543 - val_accuracy: 0.5128\n",
      "Epoch 803/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5169 - accuracy: 0.7807\n",
      "Epoch 803: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5176 - accuracy: 0.7808 - val_loss: 1.3863 - val_accuracy: 0.5487\n",
      "Epoch 804/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5132 - accuracy: 0.7751\n",
      "Epoch 804: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5130 - accuracy: 0.7762 - val_loss: 1.4032 - val_accuracy: 0.5282\n",
      "Epoch 805/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5392 - accuracy: 0.7674\n",
      "Epoch 805: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5352 - accuracy: 0.7690 - val_loss: 1.3977 - val_accuracy: 0.5269\n",
      "Epoch 806/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5331 - accuracy: 0.7771\n",
      "Epoch 806: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5318 - accuracy: 0.7770 - val_loss: 1.4302 - val_accuracy: 0.5154\n",
      "Epoch 807/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5419 - accuracy: 0.7664\n",
      "Epoch 807: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5436 - accuracy: 0.7654 - val_loss: 1.3827 - val_accuracy: 0.5436\n",
      "Epoch 808/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5274 - accuracy: 0.7692\n",
      "Epoch 808: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5303 - accuracy: 0.7682 - val_loss: 1.4180 - val_accuracy: 0.5321\n",
      "Epoch 809/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5211 - accuracy: 0.7778\n",
      "Epoch 809: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5204 - accuracy: 0.7788 - val_loss: 1.4446 - val_accuracy: 0.5167\n",
      "Epoch 810/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5197 - accuracy: 0.7718\n",
      "Epoch 810: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5205 - accuracy: 0.7706 - val_loss: 1.4461 - val_accuracy: 0.5218\n",
      "Epoch 811/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5412 - accuracy: 0.7650\n",
      "Epoch 811: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5435 - accuracy: 0.7641 - val_loss: 1.4535 - val_accuracy: 0.5115\n",
      "Epoch 812/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5259 - accuracy: 0.7721\n",
      "Epoch 812: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5281 - accuracy: 0.7708 - val_loss: 1.3904 - val_accuracy: 0.5282\n",
      "Epoch 813/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5441 - accuracy: 0.7695\n",
      "Epoch 813: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5470 - accuracy: 0.7685 - val_loss: 1.3422 - val_accuracy: 0.5372\n",
      "Epoch 814/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5195 - accuracy: 0.7714\n",
      "Epoch 814: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5189 - accuracy: 0.7716 - val_loss: 1.4183 - val_accuracy: 0.5282\n",
      "Epoch 815/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5384 - accuracy: 0.7709\n",
      "Epoch 815: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5376 - accuracy: 0.7713 - val_loss: 1.4144 - val_accuracy: 0.5205\n",
      "Epoch 816/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5399 - accuracy: 0.7670\n",
      "Epoch 816: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5393 - accuracy: 0.7667 - val_loss: 1.4467 - val_accuracy: 0.5256\n",
      "Epoch 817/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5274 - accuracy: 0.7717\n",
      "Epoch 817: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5283 - accuracy: 0.7711 - val_loss: 1.4195 - val_accuracy: 0.5244\n",
      "Epoch 818/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5314 - accuracy: 0.7765\n",
      "Epoch 818: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5314 - accuracy: 0.7765 - val_loss: 1.4187 - val_accuracy: 0.5372\n",
      "Epoch 819/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5280 - accuracy: 0.7729\n",
      "Epoch 819: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5273 - accuracy: 0.7726 - val_loss: 1.4003 - val_accuracy: 0.5244\n",
      "Epoch 820/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5219 - accuracy: 0.7744\n",
      "Epoch 820: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5202 - accuracy: 0.7754 - val_loss: 1.4507 - val_accuracy: 0.5179\n",
      "Epoch 821/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5225 - accuracy: 0.7766\n",
      "Epoch 821: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5222 - accuracy: 0.7770 - val_loss: 1.4193 - val_accuracy: 0.5205\n",
      "Epoch 822/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5304 - accuracy: 0.7833\n",
      "Epoch 822: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5292 - accuracy: 0.7834 - val_loss: 1.4059 - val_accuracy: 0.5218\n",
      "Epoch 823/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5194 - accuracy: 0.7738\n",
      "Epoch 823: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5205 - accuracy: 0.7731 - val_loss: 1.4522 - val_accuracy: 0.5192\n",
      "Epoch 824/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5350 - accuracy: 0.7767\n",
      "Epoch 824: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5357 - accuracy: 0.7760 - val_loss: 1.4377 - val_accuracy: 0.5321\n",
      "Epoch 825/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5223 - accuracy: 0.7707\n",
      "Epoch 825: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5218 - accuracy: 0.7711 - val_loss: 1.4253 - val_accuracy: 0.5333\n",
      "Epoch 826/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5196 - accuracy: 0.7774\n",
      "Epoch 826: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5197 - accuracy: 0.7770 - val_loss: 1.4795 - val_accuracy: 0.5179\n",
      "Epoch 827/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5122 - accuracy: 0.7812\n",
      "Epoch 827: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.5126 - accuracy: 0.7808 - val_loss: 1.4438 - val_accuracy: 0.5269\n",
      "Epoch 828/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5278 - accuracy: 0.7824\n",
      "Epoch 828: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 11ms/step - loss: 0.5278 - accuracy: 0.7824 - val_loss: 1.4440 - val_accuracy: 0.5295\n",
      "Epoch 829/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5121 - accuracy: 0.7823\n",
      "Epoch 829: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5126 - accuracy: 0.7816 - val_loss: 1.3958 - val_accuracy: 0.5449\n",
      "Epoch 830/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5097 - accuracy: 0.7794\n",
      "Epoch 830: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5103 - accuracy: 0.7788 - val_loss: 1.3894 - val_accuracy: 0.5410\n",
      "Epoch 831/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5168 - accuracy: 0.7769\n",
      "Epoch 831: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 2s 15ms/step - loss: 0.5163 - accuracy: 0.7772 - val_loss: 1.3787 - val_accuracy: 0.5423\n",
      "Epoch 832/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5228 - accuracy: 0.7724\n",
      "Epoch 832: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 12ms/step - loss: 0.5228 - accuracy: 0.7724 - val_loss: 1.4574 - val_accuracy: 0.5192\n",
      "Epoch 833/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5512 - accuracy: 0.7617\n",
      "Epoch 833: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.5513 - accuracy: 0.7623 - val_loss: 1.4069 - val_accuracy: 0.5231\n",
      "Epoch 834/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5408 - accuracy: 0.7694\n",
      "Epoch 834: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5399 - accuracy: 0.7703 - val_loss: 1.4260 - val_accuracy: 0.5244\n",
      "Epoch 835/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5265 - accuracy: 0.7716\n",
      "Epoch 835: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 2s 14ms/step - loss: 0.5289 - accuracy: 0.7713 - val_loss: 1.4247 - val_accuracy: 0.5128\n",
      "Epoch 836/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5198 - accuracy: 0.7754\n",
      "Epoch 836: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 11ms/step - loss: 0.5198 - accuracy: 0.7754 - val_loss: 1.4240 - val_accuracy: 0.5321\n",
      "Epoch 837/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5429 - accuracy: 0.7696\n",
      "Epoch 837: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5425 - accuracy: 0.7693 - val_loss: 1.4555 - val_accuracy: 0.5154\n",
      "Epoch 838/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5241 - accuracy: 0.7759\n",
      "Epoch 838: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.5229 - accuracy: 0.7770 - val_loss: 1.4392 - val_accuracy: 0.5128\n",
      "Epoch 839/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5081 - accuracy: 0.7748\n",
      "Epoch 839: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 2s 17ms/step - loss: 0.5111 - accuracy: 0.7744 - val_loss: 1.4920 - val_accuracy: 0.5064\n",
      "Epoch 840/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5401 - accuracy: 0.7637\n",
      "Epoch 840: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 3s 21ms/step - loss: 0.5398 - accuracy: 0.7639 - val_loss: 1.4624 - val_accuracy: 0.5500\n",
      "Epoch 841/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5117 - accuracy: 0.7798\n",
      "Epoch 841: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 12ms/step - loss: 0.5117 - accuracy: 0.7798 - val_loss: 1.4223 - val_accuracy: 0.5538\n",
      "Epoch 842/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5083 - accuracy: 0.7776\n",
      "Epoch 842: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5124 - accuracy: 0.7767 - val_loss: 1.4419 - val_accuracy: 0.5090\n",
      "Epoch 843/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5308 - accuracy: 0.7662\n",
      "Epoch 843: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5295 - accuracy: 0.7659 - val_loss: 1.4166 - val_accuracy: 0.5282\n",
      "Epoch 844/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5099 - accuracy: 0.7842\n",
      "Epoch 844: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5167 - accuracy: 0.7831 - val_loss: 1.4318 - val_accuracy: 0.5128\n",
      "Epoch 845/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5126 - accuracy: 0.7826\n",
      "Epoch 845: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5118 - accuracy: 0.7834 - val_loss: 1.4386 - val_accuracy: 0.5423\n",
      "Epoch 846/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5109 - accuracy: 0.7856\n",
      "Epoch 846: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5137 - accuracy: 0.7844 - val_loss: 1.4030 - val_accuracy: 0.5359\n",
      "Epoch 847/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5190 - accuracy: 0.7720\n",
      "Epoch 847: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5186 - accuracy: 0.7726 - val_loss: 1.4006 - val_accuracy: 0.5231\n",
      "Epoch 848/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5223 - accuracy: 0.7775\n",
      "Epoch 848: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.5197 - accuracy: 0.7798 - val_loss: 1.4730 - val_accuracy: 0.5321\n",
      "Epoch 849/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5287 - accuracy: 0.7721\n",
      "Epoch 849: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5299 - accuracy: 0.7706 - val_loss: 1.3624 - val_accuracy: 0.5500\n",
      "Epoch 850/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5341 - accuracy: 0.7659\n",
      "Epoch 850: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.5324 - accuracy: 0.7659 - val_loss: 1.4334 - val_accuracy: 0.5295\n",
      "Epoch 851/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5414 - accuracy: 0.7654\n",
      "Epoch 851: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5388 - accuracy: 0.7672 - val_loss: 1.4530 - val_accuracy: 0.5269\n",
      "Epoch 852/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5223 - accuracy: 0.7740\n",
      "Epoch 852: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5211 - accuracy: 0.7739 - val_loss: 1.4526 - val_accuracy: 0.5333\n",
      "Epoch 853/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5058 - accuracy: 0.7810\n",
      "Epoch 853: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5046 - accuracy: 0.7816 - val_loss: 1.3947 - val_accuracy: 0.5410\n",
      "Epoch 854/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5096 - accuracy: 0.7781\n",
      "Epoch 854: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 11ms/step - loss: 0.5101 - accuracy: 0.7798 - val_loss: 1.4605 - val_accuracy: 0.5167\n",
      "Epoch 855/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5259 - accuracy: 0.7713\n",
      "Epoch 855: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 2s 14ms/step - loss: 0.5259 - accuracy: 0.7713 - val_loss: 1.4952 - val_accuracy: 0.5205\n",
      "Epoch 856/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5221 - accuracy: 0.7759\n",
      "Epoch 856: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5196 - accuracy: 0.7760 - val_loss: 1.4711 - val_accuracy: 0.5192\n",
      "Epoch 857/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5042 - accuracy: 0.7810\n",
      "Epoch 857: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5081 - accuracy: 0.7793 - val_loss: 1.4782 - val_accuracy: 0.5103\n",
      "Epoch 858/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5326 - accuracy: 0.7695\n",
      "Epoch 858: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 0.5326 - accuracy: 0.7695 - val_loss: 1.4361 - val_accuracy: 0.5423\n",
      "Epoch 859/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5340 - accuracy: 0.7680\n",
      "Epoch 859: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 2s 13ms/step - loss: 0.5369 - accuracy: 0.7677 - val_loss: 1.4299 - val_accuracy: 0.5231\n",
      "Epoch 860/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5109 - accuracy: 0.7818\n",
      "Epoch 860: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5093 - accuracy: 0.7831 - val_loss: 1.4692 - val_accuracy: 0.5205\n",
      "Epoch 861/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5078 - accuracy: 0.7825\n",
      "Epoch 861: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5070 - accuracy: 0.7829 - val_loss: 1.3793 - val_accuracy: 0.5346\n",
      "Epoch 862/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5187 - accuracy: 0.7770\n",
      "Epoch 862: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5187 - accuracy: 0.7770 - val_loss: 1.5006 - val_accuracy: 0.5231\n",
      "Epoch 863/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5209 - accuracy: 0.7703\n",
      "Epoch 863: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5211 - accuracy: 0.7708 - val_loss: 1.3975 - val_accuracy: 0.5205\n",
      "Epoch 864/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5233 - accuracy: 0.7788\n",
      "Epoch 864: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5232 - accuracy: 0.7795 - val_loss: 1.4647 - val_accuracy: 0.5423\n",
      "Epoch 865/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5269 - accuracy: 0.7728\n",
      "Epoch 865: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5283 - accuracy: 0.7724 - val_loss: 1.4656 - val_accuracy: 0.5218\n",
      "Epoch 866/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5274 - accuracy: 0.7785\n",
      "Epoch 866: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5281 - accuracy: 0.7775 - val_loss: 1.4275 - val_accuracy: 0.5359\n",
      "Epoch 867/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.4979 - accuracy: 0.7855\n",
      "Epoch 867: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4999 - accuracy: 0.7844 - val_loss: 1.4547 - val_accuracy: 0.5295\n",
      "Epoch 868/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5247 - accuracy: 0.7716\n",
      "Epoch 868: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5230 - accuracy: 0.7736 - val_loss: 1.4333 - val_accuracy: 0.5359\n",
      "Epoch 869/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5170 - accuracy: 0.7815\n",
      "Epoch 869: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5212 - accuracy: 0.7795 - val_loss: 1.4237 - val_accuracy: 0.5346\n",
      "Epoch 870/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5042 - accuracy: 0.7805\n",
      "Epoch 870: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5091 - accuracy: 0.7785 - val_loss: 1.4613 - val_accuracy: 0.5372\n",
      "Epoch 871/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5121 - accuracy: 0.7760\n",
      "Epoch 871: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5124 - accuracy: 0.7757 - val_loss: 1.4219 - val_accuracy: 0.5269\n",
      "Epoch 872/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5247 - accuracy: 0.7815\n",
      "Epoch 872: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5288 - accuracy: 0.7780 - val_loss: 1.3937 - val_accuracy: 0.5513\n",
      "Epoch 873/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5147 - accuracy: 0.7806\n",
      "Epoch 873: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 12ms/step - loss: 0.5147 - accuracy: 0.7806 - val_loss: 1.4207 - val_accuracy: 0.5244\n",
      "Epoch 874/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5106 - accuracy: 0.7802\n",
      "Epoch 874: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5123 - accuracy: 0.7790 - val_loss: 1.4320 - val_accuracy: 0.5256\n",
      "Epoch 875/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5311 - accuracy: 0.7719\n",
      "Epoch 875: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5300 - accuracy: 0.7716 - val_loss: 1.4443 - val_accuracy: 0.5308\n",
      "Epoch 876/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5050 - accuracy: 0.7799\n",
      "Epoch 876: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5061 - accuracy: 0.7795 - val_loss: 1.4487 - val_accuracy: 0.5346\n",
      "Epoch 877/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5101 - accuracy: 0.7755\n",
      "Epoch 877: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5128 - accuracy: 0.7749 - val_loss: 1.3813 - val_accuracy: 0.5615\n",
      "Epoch 878/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5077 - accuracy: 0.7796\n",
      "Epoch 878: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5052 - accuracy: 0.7788 - val_loss: 1.4059 - val_accuracy: 0.5359\n",
      "Epoch 879/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5168 - accuracy: 0.7767\n",
      "Epoch 879: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5168 - accuracy: 0.7767 - val_loss: 1.4558 - val_accuracy: 0.5269\n",
      "Epoch 880/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.4986 - accuracy: 0.7828\n",
      "Epoch 880: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5012 - accuracy: 0.7819 - val_loss: 1.4354 - val_accuracy: 0.5436\n",
      "Epoch 881/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5155 - accuracy: 0.7783\n",
      "Epoch 881: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5155 - accuracy: 0.7783 - val_loss: 1.4715 - val_accuracy: 0.5282\n",
      "Epoch 882/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5305 - accuracy: 0.7691\n",
      "Epoch 882: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5319 - accuracy: 0.7680 - val_loss: 1.4180 - val_accuracy: 0.5308\n",
      "Epoch 883/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5276 - accuracy: 0.7774\n",
      "Epoch 883: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5306 - accuracy: 0.7765 - val_loss: 1.4643 - val_accuracy: 0.5269\n",
      "Epoch 884/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5211 - accuracy: 0.7679\n",
      "Epoch 884: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5212 - accuracy: 0.7667 - val_loss: 1.4394 - val_accuracy: 0.5346\n",
      "Epoch 885/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5180 - accuracy: 0.7686\n",
      "Epoch 885: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5165 - accuracy: 0.7695 - val_loss: 1.4933 - val_accuracy: 0.5179\n",
      "Epoch 886/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5076 - accuracy: 0.7849\n",
      "Epoch 886: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5093 - accuracy: 0.7844 - val_loss: 1.4590 - val_accuracy: 0.5244\n",
      "Epoch 887/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5189 - accuracy: 0.7741\n",
      "Epoch 887: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5198 - accuracy: 0.7747 - val_loss: 1.4252 - val_accuracy: 0.5256\n",
      "Epoch 888/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5065 - accuracy: 0.7844\n",
      "Epoch 888: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5085 - accuracy: 0.7837 - val_loss: 1.4802 - val_accuracy: 0.5141\n",
      "Epoch 889/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5133 - accuracy: 0.7818\n",
      "Epoch 889: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5107 - accuracy: 0.7816 - val_loss: 1.4149 - val_accuracy: 0.5256\n",
      "Epoch 890/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5078 - accuracy: 0.7828\n",
      "Epoch 890: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5087 - accuracy: 0.7819 - val_loss: 1.4029 - val_accuracy: 0.5397\n",
      "Epoch 891/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5064 - accuracy: 0.7856\n",
      "Epoch 891: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5104 - accuracy: 0.7824 - val_loss: 1.4266 - val_accuracy: 0.5192\n",
      "Epoch 892/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5161 - accuracy: 0.7784\n",
      "Epoch 892: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5169 - accuracy: 0.7780 - val_loss: 1.4857 - val_accuracy: 0.5103\n",
      "Epoch 893/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5175 - accuracy: 0.7731\n",
      "Epoch 893: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5165 - accuracy: 0.7744 - val_loss: 1.4641 - val_accuracy: 0.5205\n",
      "Epoch 894/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5045 - accuracy: 0.7812\n",
      "Epoch 894: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5083 - accuracy: 0.7777 - val_loss: 1.4373 - val_accuracy: 0.5372\n",
      "Epoch 895/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5192 - accuracy: 0.7724\n",
      "Epoch 895: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5197 - accuracy: 0.7724 - val_loss: 1.4148 - val_accuracy: 0.5154\n",
      "Epoch 896/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5008 - accuracy: 0.7868\n",
      "Epoch 896: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5004 - accuracy: 0.7870 - val_loss: 1.4346 - val_accuracy: 0.5321\n",
      "Epoch 897/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5052 - accuracy: 0.7805\n",
      "Epoch 897: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5042 - accuracy: 0.7808 - val_loss: 1.4256 - val_accuracy: 0.5359\n",
      "Epoch 898/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5183 - accuracy: 0.7764\n",
      "Epoch 898: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5208 - accuracy: 0.7752 - val_loss: 1.5186 - val_accuracy: 0.5269\n",
      "Epoch 899/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5242 - accuracy: 0.7760\n",
      "Epoch 899: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5246 - accuracy: 0.7742 - val_loss: 1.5415 - val_accuracy: 0.5115\n",
      "Epoch 900/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5074 - accuracy: 0.7846\n",
      "Epoch 900: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5074 - accuracy: 0.7847 - val_loss: 1.4984 - val_accuracy: 0.5167\n",
      "Epoch 901/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5147 - accuracy: 0.7794\n",
      "Epoch 901: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5138 - accuracy: 0.7795 - val_loss: 1.4219 - val_accuracy: 0.5295\n",
      "Epoch 902/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5139 - accuracy: 0.7702\n",
      "Epoch 902: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5148 - accuracy: 0.7713 - val_loss: 1.4762 - val_accuracy: 0.5231\n",
      "Epoch 903/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5086 - accuracy: 0.7804\n",
      "Epoch 903: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5077 - accuracy: 0.7806 - val_loss: 1.4424 - val_accuracy: 0.5346\n",
      "Epoch 904/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5224 - accuracy: 0.7709\n",
      "Epoch 904: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5198 - accuracy: 0.7724 - val_loss: 1.4434 - val_accuracy: 0.5397\n",
      "Epoch 905/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5157 - accuracy: 0.7680\n",
      "Epoch 905: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5165 - accuracy: 0.7688 - val_loss: 1.4965 - val_accuracy: 0.5282\n",
      "Epoch 906/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5109 - accuracy: 0.7834\n",
      "Epoch 906: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5099 - accuracy: 0.7839 - val_loss: 1.5402 - val_accuracy: 0.5346\n",
      "Epoch 907/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5071 - accuracy: 0.7770\n",
      "Epoch 907: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5079 - accuracy: 0.7760 - val_loss: 1.4694 - val_accuracy: 0.5462\n",
      "Epoch 908/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5212 - accuracy: 0.7753\n",
      "Epoch 908: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5208 - accuracy: 0.7775 - val_loss: 1.4570 - val_accuracy: 0.5282\n",
      "Epoch 909/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.4954 - accuracy: 0.7876\n",
      "Epoch 909: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4968 - accuracy: 0.7873 - val_loss: 1.4491 - val_accuracy: 0.5295\n",
      "Epoch 910/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5189 - accuracy: 0.7701\n",
      "Epoch 910: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5177 - accuracy: 0.7706 - val_loss: 1.4759 - val_accuracy: 0.5218\n",
      "Epoch 911/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5067 - accuracy: 0.7746\n",
      "Epoch 911: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5078 - accuracy: 0.7739 - val_loss: 1.5007 - val_accuracy: 0.5256\n",
      "Epoch 912/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5036 - accuracy: 0.7868\n",
      "Epoch 912: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5059 - accuracy: 0.7862 - val_loss: 1.4570 - val_accuracy: 0.5500\n",
      "Epoch 913/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5116 - accuracy: 0.7777\n",
      "Epoch 913: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5116 - accuracy: 0.7777 - val_loss: 1.4949 - val_accuracy: 0.5346\n",
      "Epoch 914/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5136 - accuracy: 0.7791\n",
      "Epoch 914: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5116 - accuracy: 0.7793 - val_loss: 1.4748 - val_accuracy: 0.5038\n",
      "Epoch 915/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5076 - accuracy: 0.7833\n",
      "Epoch 915: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5077 - accuracy: 0.7834 - val_loss: 1.5094 - val_accuracy: 0.4974\n",
      "Epoch 916/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5108 - accuracy: 0.7783\n",
      "Epoch 916: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5194 - accuracy: 0.7757 - val_loss: 1.4384 - val_accuracy: 0.5218\n",
      "Epoch 917/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5175 - accuracy: 0.7772\n",
      "Epoch 917: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5186 - accuracy: 0.7765 - val_loss: 1.4470 - val_accuracy: 0.5179\n",
      "Epoch 918/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5143 - accuracy: 0.7862\n",
      "Epoch 918: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5143 - accuracy: 0.7857 - val_loss: 1.4427 - val_accuracy: 0.5308\n",
      "Epoch 919/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5106 - accuracy: 0.7718\n",
      "Epoch 919: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5100 - accuracy: 0.7726 - val_loss: 1.4607 - val_accuracy: 0.5167\n",
      "Epoch 920/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5182 - accuracy: 0.7740\n",
      "Epoch 920: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5173 - accuracy: 0.7747 - val_loss: 1.4644 - val_accuracy: 0.5397\n",
      "Epoch 921/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5024 - accuracy: 0.7770\n",
      "Epoch 921: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5026 - accuracy: 0.7785 - val_loss: 1.5242 - val_accuracy: 0.5103\n",
      "Epoch 922/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5116 - accuracy: 0.7788\n",
      "Epoch 922: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5143 - accuracy: 0.7770 - val_loss: 1.4606 - val_accuracy: 0.5192\n",
      "Epoch 923/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.4964 - accuracy: 0.7843\n",
      "Epoch 923: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.4984 - accuracy: 0.7837 - val_loss: 1.5014 - val_accuracy: 0.5295\n",
      "Epoch 924/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5056 - accuracy: 0.7893\n",
      "Epoch 924: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5033 - accuracy: 0.7891 - val_loss: 1.4612 - val_accuracy: 0.5218\n",
      "Epoch 925/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5050 - accuracy: 0.7818\n",
      "Epoch 925: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5042 - accuracy: 0.7819 - val_loss: 1.4236 - val_accuracy: 0.5423\n",
      "Epoch 926/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5044 - accuracy: 0.7766\n",
      "Epoch 926: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5046 - accuracy: 0.7767 - val_loss: 1.4630 - val_accuracy: 0.5346\n",
      "Epoch 927/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5134 - accuracy: 0.7802\n",
      "Epoch 927: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5120 - accuracy: 0.7813 - val_loss: 1.3964 - val_accuracy: 0.5269\n",
      "Epoch 928/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5102 - accuracy: 0.7732\n",
      "Epoch 928: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5093 - accuracy: 0.7736 - val_loss: 1.4740 - val_accuracy: 0.5308\n",
      "Epoch 929/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5205 - accuracy: 0.7775\n",
      "Epoch 929: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5221 - accuracy: 0.7765 - val_loss: 1.4845 - val_accuracy: 0.5218\n",
      "Epoch 930/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5132 - accuracy: 0.7760\n",
      "Epoch 930: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5128 - accuracy: 0.7780 - val_loss: 1.4722 - val_accuracy: 0.5038\n",
      "Epoch 931/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.4975 - accuracy: 0.7832\n",
      "Epoch 931: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5002 - accuracy: 0.7839 - val_loss: 1.4901 - val_accuracy: 0.5231\n",
      "Epoch 932/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5161 - accuracy: 0.7769\n",
      "Epoch 932: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5157 - accuracy: 0.7770 - val_loss: 1.5276 - val_accuracy: 0.5064\n",
      "Epoch 933/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5133 - accuracy: 0.7766\n",
      "Epoch 933: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5144 - accuracy: 0.7762 - val_loss: 1.4853 - val_accuracy: 0.5244\n",
      "Epoch 934/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5021 - accuracy: 0.7904\n",
      "Epoch 934: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5018 - accuracy: 0.7909 - val_loss: 1.5160 - val_accuracy: 0.5128\n",
      "Epoch 935/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5060 - accuracy: 0.7895\n",
      "Epoch 935: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5046 - accuracy: 0.7893 - val_loss: 1.4669 - val_accuracy: 0.5282\n",
      "Epoch 936/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.4979 - accuracy: 0.7911\n",
      "Epoch 936: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4963 - accuracy: 0.7921 - val_loss: 1.5178 - val_accuracy: 0.5051\n",
      "Epoch 937/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5029 - accuracy: 0.7804\n",
      "Epoch 937: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5042 - accuracy: 0.7788 - val_loss: 1.4913 - val_accuracy: 0.5115\n",
      "Epoch 938/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5105 - accuracy: 0.7872\n",
      "Epoch 938: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5097 - accuracy: 0.7870 - val_loss: 1.4862 - val_accuracy: 0.5205\n",
      "Epoch 939/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5079 - accuracy: 0.7862\n",
      "Epoch 939: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5079 - accuracy: 0.7862 - val_loss: 1.4524 - val_accuracy: 0.5346\n",
      "Epoch 940/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5181 - accuracy: 0.7714\n",
      "Epoch 940: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5120 - accuracy: 0.7749 - val_loss: 1.4827 - val_accuracy: 0.5295\n",
      "Epoch 941/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5042 - accuracy: 0.7818\n",
      "Epoch 941: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5033 - accuracy: 0.7821 - val_loss: 1.4734 - val_accuracy: 0.5256\n",
      "Epoch 942/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5084 - accuracy: 0.7794\n",
      "Epoch 942: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5089 - accuracy: 0.7795 - val_loss: 1.4340 - val_accuracy: 0.5410\n",
      "Epoch 943/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5123 - accuracy: 0.7831\n",
      "Epoch 943: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5143 - accuracy: 0.7826 - val_loss: 1.4277 - val_accuracy: 0.5244\n",
      "Epoch 944/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5160 - accuracy: 0.7812\n",
      "Epoch 944: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5156 - accuracy: 0.7811 - val_loss: 1.4897 - val_accuracy: 0.5256\n",
      "Epoch 945/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5143 - accuracy: 0.7810\n",
      "Epoch 945: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5139 - accuracy: 0.7813 - val_loss: 1.4654 - val_accuracy: 0.5077\n",
      "Epoch 946/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5039 - accuracy: 0.7801\n",
      "Epoch 946: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5039 - accuracy: 0.7801 - val_loss: 1.4290 - val_accuracy: 0.5538\n",
      "Epoch 947/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5071 - accuracy: 0.7826\n",
      "Epoch 947: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 11ms/step - loss: 0.5071 - accuracy: 0.7826 - val_loss: 1.4753 - val_accuracy: 0.5359\n",
      "Epoch 948/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.4882 - accuracy: 0.7890\n",
      "Epoch 948: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.4875 - accuracy: 0.7893 - val_loss: 1.4260 - val_accuracy: 0.5321\n",
      "Epoch 949/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.4978 - accuracy: 0.7839\n",
      "Epoch 949: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4955 - accuracy: 0.7860 - val_loss: 1.4660 - val_accuracy: 0.5090\n",
      "Epoch 950/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.4955 - accuracy: 0.7828\n",
      "Epoch 950: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5002 - accuracy: 0.7808 - val_loss: 1.4526 - val_accuracy: 0.5231\n",
      "Epoch 951/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5145 - accuracy: 0.7826\n",
      "Epoch 951: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.5150 - accuracy: 0.7811 - val_loss: 1.5054 - val_accuracy: 0.5269\n",
      "Epoch 952/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5042 - accuracy: 0.7925\n",
      "Epoch 952: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5055 - accuracy: 0.7921 - val_loss: 1.4315 - val_accuracy: 0.5205\n",
      "Epoch 953/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.4925 - accuracy: 0.7964\n",
      "Epoch 953: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.4938 - accuracy: 0.7952 - val_loss: 1.4670 - val_accuracy: 0.5346\n",
      "Epoch 954/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.4910 - accuracy: 0.7895\n",
      "Epoch 954: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.4920 - accuracy: 0.7893 - val_loss: 1.4631 - val_accuracy: 0.5269\n",
      "Epoch 955/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.4978 - accuracy: 0.7786\n",
      "Epoch 955: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4969 - accuracy: 0.7798 - val_loss: 1.5120 - val_accuracy: 0.5244\n",
      "Epoch 956/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5193 - accuracy: 0.7780\n",
      "Epoch 956: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5210 - accuracy: 0.7772 - val_loss: 1.4684 - val_accuracy: 0.5128\n",
      "Epoch 957/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5037 - accuracy: 0.7813\n",
      "Epoch 957: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5037 - accuracy: 0.7813 - val_loss: 1.5514 - val_accuracy: 0.5103\n",
      "Epoch 958/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5094 - accuracy: 0.7807\n",
      "Epoch 958: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5086 - accuracy: 0.7808 - val_loss: 1.5010 - val_accuracy: 0.5115\n",
      "Epoch 959/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.4989 - accuracy: 0.7855\n",
      "Epoch 959: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4999 - accuracy: 0.7852 - val_loss: 1.4810 - val_accuracy: 0.5346\n",
      "Epoch 960/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.4991 - accuracy: 0.7842\n",
      "Epoch 960: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4997 - accuracy: 0.7852 - val_loss: 1.4713 - val_accuracy: 0.5205\n",
      "Epoch 961/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5030 - accuracy: 0.7847\n",
      "Epoch 961: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5031 - accuracy: 0.7847 - val_loss: 1.4789 - val_accuracy: 0.5026\n",
      "Epoch 962/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.4902 - accuracy: 0.7903\n",
      "Epoch 962: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4907 - accuracy: 0.7888 - val_loss: 1.4492 - val_accuracy: 0.5500\n",
      "Epoch 963/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.4938 - accuracy: 0.7920\n",
      "Epoch 963: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4970 - accuracy: 0.7909 - val_loss: 1.4601 - val_accuracy: 0.5346\n",
      "Epoch 964/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5034 - accuracy: 0.7795\n",
      "Epoch 964: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5034 - accuracy: 0.7795 - val_loss: 1.4684 - val_accuracy: 0.5244\n",
      "Epoch 965/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5068 - accuracy: 0.7820\n",
      "Epoch 965: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5060 - accuracy: 0.7826 - val_loss: 1.5069 - val_accuracy: 0.5346\n",
      "Epoch 966/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.5040 - accuracy: 0.7833\n",
      "Epoch 966: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5049 - accuracy: 0.7824 - val_loss: 1.4394 - val_accuracy: 0.5474\n",
      "Epoch 967/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5075 - accuracy: 0.7818\n",
      "Epoch 967: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5076 - accuracy: 0.7811 - val_loss: 1.4858 - val_accuracy: 0.5115\n",
      "Epoch 968/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5049 - accuracy: 0.7760\n",
      "Epoch 968: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5058 - accuracy: 0.7752 - val_loss: 1.5723 - val_accuracy: 0.5167\n",
      "Epoch 969/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5095 - accuracy: 0.7845\n",
      "Epoch 969: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5107 - accuracy: 0.7844 - val_loss: 1.5265 - val_accuracy: 0.4949\n",
      "Epoch 970/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.4969 - accuracy: 0.7839\n",
      "Epoch 970: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4987 - accuracy: 0.7837 - val_loss: 1.4748 - val_accuracy: 0.5218\n",
      "Epoch 971/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.4995 - accuracy: 0.7826\n",
      "Epoch 971: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5014 - accuracy: 0.7831 - val_loss: 1.5177 - val_accuracy: 0.5167\n",
      "Epoch 972/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.4902 - accuracy: 0.7879\n",
      "Epoch 972: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4928 - accuracy: 0.7867 - val_loss: 1.5377 - val_accuracy: 0.5205\n",
      "Epoch 973/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.5049 - accuracy: 0.7839\n",
      "Epoch 973: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5032 - accuracy: 0.7852 - val_loss: 1.4952 - val_accuracy: 0.5179\n",
      "Epoch 974/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5139 - accuracy: 0.7782\n",
      "Epoch 974: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5146 - accuracy: 0.7785 - val_loss: 1.4761 - val_accuracy: 0.5269\n",
      "Epoch 975/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.4982 - accuracy: 0.7856\n",
      "Epoch 975: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5008 - accuracy: 0.7847 - val_loss: 1.5520 - val_accuracy: 0.5128\n",
      "Epoch 976/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5143 - accuracy: 0.7810\n",
      "Epoch 976: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5161 - accuracy: 0.7793 - val_loss: 1.5026 - val_accuracy: 0.5115\n",
      "Epoch 977/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.4929 - accuracy: 0.7914\n",
      "Epoch 977: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4910 - accuracy: 0.7906 - val_loss: 1.5082 - val_accuracy: 0.5449\n",
      "Epoch 978/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.4880 - accuracy: 0.7907\n",
      "Epoch 978: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4877 - accuracy: 0.7901 - val_loss: 1.5076 - val_accuracy: 0.5192\n",
      "Epoch 979/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5039 - accuracy: 0.7815\n",
      "Epoch 979: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5021 - accuracy: 0.7826 - val_loss: 1.4526 - val_accuracy: 0.5333\n",
      "Epoch 980/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.5231 - accuracy: 0.7789\n",
      "Epoch 980: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5232 - accuracy: 0.7785 - val_loss: 1.4455 - val_accuracy: 0.5410\n",
      "Epoch 981/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.5095 - accuracy: 0.7820\n",
      "Epoch 981: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5079 - accuracy: 0.7834 - val_loss: 1.4566 - val_accuracy: 0.5410\n",
      "Epoch 982/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.4987 - accuracy: 0.7867\n",
      "Epoch 982: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.4987 - accuracy: 0.7867 - val_loss: 1.4682 - val_accuracy: 0.5269\n",
      "Epoch 983/1000\n",
      "118/122 [============================>.] - ETA: 0s - loss: 0.5071 - accuracy: 0.7826\n",
      "Epoch 983: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5133 - accuracy: 0.7801 - val_loss: 1.4927 - val_accuracy: 0.5282\n",
      "Epoch 984/1000\n",
      "121/122 [============================>.] - ETA: 0s - loss: 0.4901 - accuracy: 0.7893\n",
      "Epoch 984: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4919 - accuracy: 0.7888 - val_loss: 1.5276 - val_accuracy: 0.5205\n",
      "Epoch 985/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.4938 - accuracy: 0.7911\n",
      "Epoch 985: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4967 - accuracy: 0.7873 - val_loss: 1.5451 - val_accuracy: 0.5205\n",
      "Epoch 986/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.4988 - accuracy: 0.7891\n",
      "Epoch 986: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4988 - accuracy: 0.7891 - val_loss: 1.4252 - val_accuracy: 0.5359\n",
      "Epoch 987/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.4908 - accuracy: 0.7860\n",
      "Epoch 987: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4898 - accuracy: 0.7865 - val_loss: 1.5193 - val_accuracy: 0.5333\n",
      "Epoch 988/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.5186 - accuracy: 0.7770\n",
      "Epoch 988: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5186 - accuracy: 0.7770 - val_loss: 1.4533 - val_accuracy: 0.5423\n",
      "Epoch 989/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5070 - accuracy: 0.7837\n",
      "Epoch 989: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5037 - accuracy: 0.7860 - val_loss: 1.5044 - val_accuracy: 0.5308\n",
      "Epoch 990/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5171 - accuracy: 0.7755\n",
      "Epoch 990: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.5151 - accuracy: 0.7772 - val_loss: 1.4994 - val_accuracy: 0.5436\n",
      "Epoch 991/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.4957 - accuracy: 0.7844\n",
      "Epoch 991: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4957 - accuracy: 0.7844 - val_loss: 1.4939 - val_accuracy: 0.5333\n",
      "Epoch 992/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.4950 - accuracy: 0.7856\n",
      "Epoch 992: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.4929 - accuracy: 0.7862 - val_loss: 1.4976 - val_accuracy: 0.5410\n",
      "Epoch 993/1000\n",
      "116/122 [===========================>..] - ETA: 0s - loss: 0.4942 - accuracy: 0.7853\n",
      "Epoch 993: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4923 - accuracy: 0.7847 - val_loss: 1.4924 - val_accuracy: 0.5295\n",
      "Epoch 994/1000\n",
      "117/122 [===========================>..] - ETA: 0s - loss: 0.5039 - accuracy: 0.7914\n",
      "Epoch 994: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5056 - accuracy: 0.7898 - val_loss: 1.4737 - val_accuracy: 0.5282\n",
      "Epoch 995/1000\n",
      "115/122 [===========================>..] - ETA: 0s - loss: 0.5030 - accuracy: 0.7870\n",
      "Epoch 995: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5020 - accuracy: 0.7865 - val_loss: 1.5141 - val_accuracy: 0.5269\n",
      "Epoch 996/1000\n",
      "119/122 [============================>.] - ETA: 0s - loss: 0.4768 - accuracy: 0.7994\n",
      "Epoch 996: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4779 - accuracy: 0.7998 - val_loss: 1.4792 - val_accuracy: 0.5128\n",
      "Epoch 997/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.4995 - accuracy: 0.7834\n",
      "Epoch 997: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.4996 - accuracy: 0.7829 - val_loss: 1.5395 - val_accuracy: 0.5269\n",
      "Epoch 998/1000\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.4939 - accuracy: 0.7837\n",
      "Epoch 998: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4939 - accuracy: 0.7837 - val_loss: 1.4944 - val_accuracy: 0.5244\n",
      "Epoch 999/1000\n",
      "120/122 [============================>.] - ETA: 0s - loss: 0.4982 - accuracy: 0.7854\n",
      "Epoch 999: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.5006 - accuracy: 0.7849 - val_loss: 1.5402 - val_accuracy: 0.5051\n",
      "Epoch 1000/1000\n",
      "114/122 [===========================>..] - ETA: 0s - loss: 0.5176 - accuracy: 0.7796\n",
      "Epoch 1000: val_loss did not improve from 0.94758\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5167 - accuracy: 0.7803 - val_loss: 1.5372 - val_accuracy: 0.5423\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:58:24.328320Z",
     "start_time": "2025-05-22T17:58:24.174779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(15, 8))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ],
   "id": "78d4cfb01af72746",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:58:24.604011Z",
     "start_time": "2025-05-22T17:58:24.383254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = y_pred.argmax(axis=1)\n",
    "\n",
    "\n",
    "# y_test: true labels, y_pred_labels: predicted labels\n",
    "print(classification_report(y_test, y_pred_labels, target_names=le.classes_))\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(y_test, y_pred_labels))\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_score(y_test, y_pred_labels))"
   ],
   "id": "91df9585112fd117",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          QB       0.50      0.56      0.53       117\n",
      "          RB       0.50      0.59      0.54       285\n",
      "          TE       0.35      0.42      0.38        85\n",
      "          WR       0.61      0.44      0.51       293\n",
      "\n",
      "    accuracy                           0.51       780\n",
      "   macro avg       0.49      0.50      0.49       780\n",
      "weighted avg       0.52      0.51      0.51       780\n",
      "\n",
      "Cohen's Kappa: 0.30098368634351325\n",
      "Balanced Accuracy: 0.5026142391018393\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:58:24.647588Z",
     "start_time": "2025-05-22T17:58:24.638538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(pd.Series(draft_data['position_drafted_encoded']).value_counts())\n",
    "for idx, label in enumerate(le.classes_):\n",
    "    print(f\"{label}: {idx}\")"
   ],
   "id": "3f60aea9ba11d782",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position_drafted_encoded\n",
      "3    1581\n",
      "1    1285\n",
      "0     584\n",
      "2     450\n",
      "Name: count, dtype: int64\n",
      "QB: 0\n",
      "RB: 1\n",
      "TE: 2\n",
      "WR: 3\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:58:24.975006Z",
     "start_time": "2025-05-22T17:58:24.676649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xgb_model = xgb.XGBClassifier(eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(\"Validation accuracy:\", xgb_model.score(X_val, y_val))"
   ],
   "id": "d31c044bb1f2ddf7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5435897435897435\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:58:25.442552Z",
     "start_time": "2025-05-22T17:58:25.016464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"Validation accuracy:\", rf.score(X_val, y_val))"
   ],
   "id": "caf68091902e2431",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5435897435897435\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:58:25.597715Z",
     "start_time": "2025-05-22T17:58:25.471788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get feature importances from the trained model\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame for easy plotting\n",
    "feat_imp = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "feat_imp = feat_imp.sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feat_imp)\n",
    "plt.title('Random Forest Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "3c4bbd67178c2112",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 114
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
